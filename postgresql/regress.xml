<?xml version="1.0" encoding="UTF-8"?>
<!-- Dernière modification
     le       $Date$
     par      $Author$
     révision $Revision$ -->

<chapter id="regress">
 <title>Tests de régression</title>

 <indexterm zone="regress">
  <primary>tests de régression</primary>
 </indexterm>

 <indexterm zone="regress">
  <primary>test</primary>
 </indexterm>

 <para>
  Les tests de régression composent un ensemble exhaustif de tests pour
  l'implémentation SQL dans <productname>PostgreSQL</productname>. Ils testent
  les opérations SQL standards ainsi que les fonctionnalités étendues de
  <productname>PostgreSQL</productname>.
 </para>

 <sect1 id="regress-run">
  <title>Lancer les tests</title>

  <para>
   Les tests de régression peuvent être lancés sur un serveur déjà installé et
   fonctionnel ou en utilisant une installation temporaire à l'intérieur du
   répertoire de construction. De plus, ils peuvent être lancés en mode
   <quote>parallèle</quote> ou en mode <quote>séquentiel</quote>. Le mode
   séquentiel lance les scripts de test en série, alors que le mode
   parallèle lance plusieurs processus serveur pour paralléliser l'exécution des
   groupes de tests. Les tests parallèles permettent de s'assurer du
   bon fonctionnement des communications interprocessus et du verrouillage.
  </para>

  <sect2>
   <title>Exécuter les tests sur une installation temporaire</title>

   <para>
    Pour lancer les tests de régression en parallèle après la construction, mais avant l'installation,
    il suffit de saisir
    <screen>make check</screen>
   dans le répertoire de premier niveau (on peut aussi se placer dans le
   répertoire <filename>src/test/regress</filename> et y lancer la commande).
   Au final, la sortie devrait ressembler à quelque chose comme
<screen><computeroutput>======================
 All 193 tests passed.
======================</computeroutput></screen>
   ou une note indiquant l'échec des tests.  Voir la <xref
   linkend="regress-evaluation"/> avant de supposer qu'un <quote>échec</quote>
   représente un problème sérieux.
  </para>

    <para>
    Comme cette méthode de tests exécute un serveur temporaire, cela ne
    fonctionnera pas si vous avez construit le serveur en tant que root, étant
    donné que le serveur ne démarre pas en tant que root. La procédure
    recommandée est de ne pas construire en tant que root ou de réaliser les
    tests après avoir terminé l'installation.
   </para>

   <para>
    Si vous avez configuré <productname>PostgreSQL</productname> pour qu'il
    s'installe dans un emplacement où existe déjà une ancienne installation de
    <productname>PostgreSQL</productname> et que vous lancez <literal>make
    check</literal> avant d'installer la nouvelle version, vous pourriez trouver que
    les tests échouent parce que les nouveaux programmes essaient d'utiliser les
    bibliothèques partagées déjà installées (les symptômes typiques sont des
    plaintes concernant des symboles non définis). Si vous souhaitez lancer les
    tests avant d'écraser l'ancienne installation, vous devrez construire avec
    <literal>configure --disable-rpath</literal>. Néanmoins, il n'est pas recommandé
    d'utiliser cette option pour l'installation finale.
   </para>

   <para>
    Les tests de régression en parallèle lancent quelques processus avec
    votre utilisateur. Actuellement, le nombre maximum est de vingt scripts de
    tests en parallèle, ce qui signifie 40 processus&nbsp;: il existe un
    processus serveur, un <application>psql</application> et habituellement un processus
    parent pour le <application>psql</application> de chaque script de tests. Si votre
    système force une limite par utilisateur sur le nombre de processus,
    assurez-vous que cette limite est d'au moins 50, sinon vous pourriez obtenir
    des échecs hasardeux dans les tests en parallèle. Si vous ne pouvez pas
    augmenter cette limite, vous pouvez diminuer le degré de parallélisme en
    initialisant le paramètre <literal>MAX_CONNECTIONS</literal>. Par
exemple,
<screen>make MAX_CONNECTIONS=10 check</screen>
    ne lance pas plus de dix tests en même temps.
   </para>
  </sect2>

  <sect2>
   <title>Exécuter les tests sur une installation existante</title>

  <para>
   Pour lancer les tests après l'installation<phrase> (voir le <xref
   linkend="installation"/>)</phrase>, initialisez un répertoire de données et lancez le
   serveur<phrase> comme expliqué dans le <xref linkend="runtime"/></phrase>,
   puis lancez
<screen>make installcheck</screen>
ou pour un test parallèle
<screen>make installcheck-parallel</screen>
   Les tests s'attendront à contacter le serveur sur l'hôte local et avec le
   numéro de port par défaut, sauf en cas d'indication contraire avec les
   variables d'environnement <envar>PGHOST</envar> et <envar>PGPORT</envar>.
   Les tests seront exécutés dans une base de données nommée
   <literal>regression</literal>&nbsp;; toute base de données existante de
   même nom sera supprimée.
  </para>

  <para>
   Les tests créent aussi de façon temporaire des objets globaux, comme les
   rôles, les tablespaces et les abonnements. Ces objets auront des noms commençant avec
   <literal>regress_</literal>. Attention à l'utilisation du mode
   <literal>installcheck</literal> dans une installation qui a de vrais
   objets globaux nommés de cette manière.
  </para>
  </sect2>

  <sect2>
   <title>Suites supplémentaires de tests</title>

   <para>
   Les commandes <literal>make check</literal> et <literal>make
   installcheck</literal> exécutent seulement les tests de régression
   internes qui testent des fonctionnalités internes du serveur
   <productname>PostgreSQL</productname>. Les sources contiennent aussi
   des suites supplémentaires de tests, la plupart ayant à voir avec
   des fonctionnalités supplémentaires comme les langages optionnels de
   procédures.
   </para>

   <para>
   Pour exécuter toutes les suites de tests applicables aux modules qui ont
   été sélectionnés à la construction, en incluant les tests internes, tapez
   une des commandes suivantes dans le répertoire principal de
   construction&nbsp;:
 <screen>
make check-world
make installcheck-world
</screen>
   Ces commandes exécutent les tests en utilisant, respectivement, un serveur
   temporaire ou un serveur déjà installé, comme expliqué précédemment pour
   <literal>make check</literal> et <literal>make installcheck</literal>.
   Les autres considérations sont identiques à celles expliquées précédemment
   pour chaque méthode. Notez que <literal>make check-world</literal> construit
   une instance séparée (répertoire de données temporaire) pour chaque module
   testé, donc cela réclame plus de temps et d'espace disque que
   <literal>make installcheck-world</literal>.
  </para>

  <para>
   Sur une machine moderne avec plusieurs CPU et sans limites au niveau du
   système d'exploitation, il est possible d'aller bien plus vite avec le
   parallélisme. La solution utilisée par de nombreux développeurs de
   PostgreSQL revient en fait à exécuter tous les tests ainsi&nbsp;:
<screen>
make check-world -j8 >/dev/null
</screen>
   avec une limite au niveau de la valeur de l'option <option>-j</option> proche
   du nombre de CPU disponibles. Supprimer <systemitem>stdout</systemitem>
   élimine une sortie verbeuse qui n'est pas particulièrement intéressante
   quand on veut simplement vérifier le succès de l'opération. (En cas d'échec,
   les messages sur <systemitem>stderr</systemitem> sont généralement
   suffisant pour déterminer ce qu'il faut vérifier avec précision.)
   </para>

   <para>
    Autrement, vous pouvez exécuter les suites individuelles de tests en tapant
    <literal>make check</literal> ou <literal>make installcheck</literal> dans
    le sous-répertoire approprié du répertoire de construction. Gardez en tête
    que <literal>make installcheck</literal> suppose que vous avez installé les
    modules adéquats, pas seulement le serveur de base.
   </para>

   <para>
    Les tests supplémentaires pouvant être demandés de cette façon incluent&nbsp;:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      Les tests de régression pour les langages optionnels de procédures
      stockées. Ils sont situés dans <filename>src/pl</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      Les tests de régression pour les modules <filename>contrib</filename>,
      situés dans <filename>contrib</filename>. Tous les modules
      <filename>contrib</filename> n'ont pas forcément des suites de tests.
     </para>
    </listitem>
    <listitem>
     <para>
      Les tests de régression pour l'interface ECPG, situés dans
      <filename>src/interfaces/ecpg/test</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      Les tests pour les méthodes d'authentification supportés
      nativemement, situés dans <filename>src/test/authentication</filename>.
      (Voir ci-dessous pour des tests supplémentaires relatifs à
      l'authentification.)
     </para>
    </listitem>
    <listitem>
     <para>
      Les tests simulant le comportement de sessions concurrentes, situés dans
      <filename>src/test/isolation</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      Les tests de la restauration après crash et de la réplication
      physique, situés dans <filename>src/test/recovery</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      Les tests pour la réplication logique, situés dans
      <filename>src/test/subscription</filename>.
     </para>
    </listitem>
    <listitem>
     <para>
      Les tests des programmes clients, situés dans <filename>src/bin</filename>.
     </para>
    </listitem>
   </itemizedlist>

   <para>
    Lors de l'utilisation du mode <literal>installcheck</literal>, ces tests
    créeront et supprimeront des bases de données de test dont les noms
    contiennent <literal>regression</literal>, par exemple
    <literal>pl_regression</literal> ou <literal>contrib_regression</literal>.
    Il faut faire bien attention à ne pas utiliser le mode
    <literal>installcheck</literal> avec une installation qui contient des bases
    de données importantes nommées de cette façon.
   </para>

   <para>
    Certains des suites de tests supplémentaires utilisent l'infrastructure TAP
    expliquée dans <xref linkend="regress-tap"/>.
    Les tests TAP sont seulement exécutés si PostgreSQL a été configuré
    avec l'option <option>--enable-tap-tests</option>. Cela est recommandé
    pour le développement, mais peut être omis s'il n'y a pas d'installation
    Perl appropriée.
   </para>

   <para>
    Certains tests ne sont pas exécutés par défaut, soit parce qu'ils ne sont
    pas sécurisés pour fonctionner sur un système multi-utilisateurs, soit
    parce qu'ils nécessitent un logiciel spécifique. Vous pouvez décider quels
    seront les suites de test à exécuter lors de l'execution de
    <command>make</command> par son paramètrage ou par l'affectation d'une
    configuration à la variable d'environnement <varname>PG_TEST_EXTRA</varname>
    dans une liste séparée par des espaces, par exemple&nbsp;:
<programlisting>
make check-world PG_TEST_EXTRA='kerberos ldap ssl'
</programlisting>
    Les valeurs suivantes sont actuellement prises en charge&nbsp;:
    <variablelist>
     <varlistentry>
      <term><literal>kerberos</literal></term>
      <listitem>
       <para>
        Exécute la suite de tests présent dans
        <filename>src/test/kerberos</filename>. Cela nécessite une installation
        de MIT Kerberos et ouvre les sockets d'écoute TCP/IP.
       </para>
      </listitem>
     </varlistentry>

     <varlistentry>
      <term><literal>ldap</literal></term>
      <listitem>
       <para>
        Exécute la suite de tests présent dans
	    <filename>src/test/ldap</filename>. Cela nécessite une installation de
	    <productname>OpenLDAP</productname> et ouvre les sockets d'écoute TCP/IP.
       </para>
      </listitem>
     </varlistentry>

     <varlistentry>
      <term><literal>ssl</literal></term>
      <listitem>
       <para>
	    Exécute la suite de tests présent dans
	    <filename>src/test/ssl</filename>. Ceci ouvre les sockets d'écoute
	    TCP/IP.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>

    Les tests pour les fonctionnalités qui ne sont pas prises en charge par la
    configuration de construction actuelle ne sont pas exécutés même si elles
    sont mentionnées dans <varname>PG_TEST_EXTRA</varname>.
   </para>

   <para>
    De plus, il existe des tests dans <filename>src/test/modules</filename>
    qui seront exécutés par <literal>make check-world</literal> mais pas par
    <literal>make installcheck-world</literal>. Ceci est dû au fait qu'ils
    installent des extensions qui ne sont pas de production ou qui ont des
    effets de bord considérés non désirables pour une installation en
    production. Vous pouvez utiliser <literal>make install</literal> et
    <literal>make installcheck</literal> dans un de ces sous-répertoires si
    vous le souhaitez mais il n'est pas recommandé de la faire sur un serveur
    autre qu'un serveur de tests.
   </para>
  </sect2>

  <sect2>
   <title>Locale et encodage</title>

   <para>
    Par défaut, les tests sur une installation temporaire utilisent la locale
    définie dans l'environnement et l'encodage de la base de données
    correspondante est déterminé par <command>initdb</command>. Il peut être
    utile de tester différentes locales en configurant les variables
    d'environnement appropriées. Par exemple&nbsp;:
    <screen>
make check LANG=C
make check LC_COLLATE=en_US.utf8 LC_CTYPE=fr_CA.utf8
    </screen>
    Pour des raisons d'implémentation, configurer <envar>LC_ALL</envar> ne
    fonctionne pas dans ce cas. Toutes les autres variables d'environnement
    liées à la locale fonctionnent.
   </para>

   <para>
    Lors d'un test sur une installation existante, la locale est déterminée
    par l'instance existante et ne peut pas être configurée séparément pour un
    test.
   </para>

   <para>
    Vous pouvez aussi choisir l'encodage de la base explicitement en
    configurant la variable <envar>ENCODING</envar>. Par exemple&nbsp;:
    <screen>
make check LANG=C ENCODING=EUC_JP
    </screen>
    Configurer l'encodage de la base de cette façon n'a un sens que si la
    locale est C. Dans les autres cas, l'encodage est choisi automatiquement à
    partir de la locale. Spécifier un encodage qui ne correspond pas à la
    locale donnera une erreur.
   </para>

   <para>
    L'encodage de la base de données peut être configuré pour des tests sur
    une installation temporaire ou existante, bien que, dans ce dernier cas,
    il doit être compatible avec la locale d'installation.
   </para>
  </sect2>

  <sect2>
   <title>Tests supplémentaires</title>

   <para>
    La suite interne de tests de régression contient quelques fichiers de tests qui ne
    sont pas exécutés par défaut, car ils pourraient dépendre de la plateforme
    ou prendre trop de temps pour s'exécuter. Vous pouvez les exécuter ou en
    exécuter d'autres en configurant la variable <envar>EXTRA_TESTS</envar>.
    Par exemple, pour exécuter le test <literal>numeric_big</literal>&nbsp;:
    <screen>
make check EXTRA_TESTS=numeric_big
    </screen>
    Pour exécuter les tests sur le collationnement&nbsp;:
    <screen>
make check EXTRA_TESTS='collate.linux.utf8 collate.icu.utf8' LANG=en_US.utf8
    </screen>
    Le test <literal>collate.linux.utf8</literal> fonctionne seulement sur
    les plateformes Linux/glibc. Le test <literal>collate.icu.utf8</literal>
    fonctionne seulement si le support pour ICU est présent. Les deux tests
    ne pourront réussir que s'ils sont effectués sur une base utilisant 
    un encodage UTF-8. 
   </para>
  </sect2>

  <sect2>
   <title>Tests du Hot Standby</title>

   <para>
    La distribution des sources contient aussi des tests de régression du
    comportement statique du Hot Standby. Ces tests requièrent un serveur
    primaire et un serveur en attente, les deux en cours d'exécution, le dernier
    acceptant les modifications des journaux de transactions du primaire en
    utilisant soit l'envoi des fichiers soit la réplication en flux. Ces
    serveurs ne sont pas automatiquement créés pour vous, pas plus que la
    configuration n'est documentée ici. Merci de vérifier les différentes
    sections de la documentation qui sont déjà dévolues aux commandes requises
    et aux problèmes associés.
   </para>

   <para>
    Pour exécuter les tests Hot Standby, créez une base de données appelée
    « regression » sur le primaire.
    <screen>psql -h primary -c "CREATE DATABASE regression"
    </screen>
    Ensuite, exécutez le script préparatoire
    <filename>src/test/regress/sql/hs_primary_setup.sql</filename> sur le
    primaire dans la base de données de régression. Par exemple&nbsp;:
    <screen>psql -h primary -f src/test/regress/sql/hs_primary_setup.sql regression
    </screen>
    Attendez la propagation des modifications vers le serveur en standby.
   </para>

   <para>
    Maintenant, arrangez-vous pour que la connexion par défaut à la base de
    données soit sur le serveur en standby sous test (par exemple en configurant
    les variables d'environnement <envar>PGHOST</envar> et <envar>PGPORT</envar>).
    Enfin, lancez l'action <literal>standbycheck</literal> à partir du
    répertoire de la suite de tests de régression.
    <screen>
cd src/test/regress
make standbycheck
    </screen>
   </para>

   <para>
    Certains comportements extrêmes peuvent aussi être créés sur le primaire
    en utilisant le script
    <filename>src/test/regress/sql/hs_primary_extremes.sql</filename> pour
    permettre le test du comportement du serveur en attente.
   </para>
  </sect2>

 </sect1>

 <sect1 id="regress-evaluation">
  <title>Évaluation des tests</title>

  <para>
   Quelques installations de <productname>PostgreSQL</productname>
   proprement installées et totalement fonctionnelles peuvent
   <quote>échouer</quote> sur certains des tests de régression à cause de
   certains points spécifiques à la plateforme comme une représentation de
   nombres à virgules flottantes ou <quote>message wording</quote>. Les tests
   sont actuellement évalués en utilisant une simple comparaison
   <command>diff</command> avec les sorties générées sur un système de
   référence, donc les résultats sont sensibles aux petites différences
   système. Quand un test est rapporté comme <quote>échoué</quote>, toujours
   examiner les différences entre les résultats attendus et ceux obtenus&nbsp;;
   vous pourriez très bien trouver que les différences ne sont pas significatives.
   Néanmoins, nous nous battons toujours pour maintenir des fichiers de
   références précis et à jour pour toutes les plateformes supportés de façon à
   ce que tous les tests puissent réussir.
  </para>

  <para>
   Les sorties actuelles des tests de régression sont dans les fichiers du
   répertoire <filename>src/test/regress/results</filename>. Le script de test
   utilise <command>diff</command> pour comparer chaque fichier de sortie avec
   les sorties de référence stockées dans le répertoire
   <filename>src/test/regress/expected</filename>. Toutes les différences sont
   conservées pour que vous puissiez les regarder dans
   <filename>src/test/regress/regression.diffs</filename>. (Lors de l'exécution
   d'une suite de tests en dehors des tests internes, ces fichiers doivent
   apparaître dans le sous-répertoire adéquat, mais pas
   <filename>src/test/regress</filename>.)
  </para>

  <para>
   Si vous n'aimez pas
   les options utilisées par défaut pour la commande <command>diff</command>,
   configurez la variable d'environnement <envar>PG_REGRESS_DIFF_OPTS</envar>.
   Par exemple <literal>PG_REGRESS_DIFF_OPTS='-c'</literal> (ou vous pouvez
   lancer <command>diff</command> vous-même, si vous préférez).
  </para>

  <para>
   Si, pour certaines raisons, une plateforme particulière génère un
   <quote>échec</quote> pour un test donné mais qu'une revue de la sortie vous
   convaint que le résultat est valide, vous pouvez ajouter un nouveau fichier
   de comparaison pour annuler le rapport d'échec pour les prochains lancements
   du test. Voir la <xref linkend="regress-variant"/> pour les détails.
  </para>

  <sect2>
   <title>Différences dans les messages d'erreurs</title>

   <para>
    Certains des tests de régression impliquent des valeurs en
    entrée intentionnellement invalides. Les messages d'erreur peuvent
    provenir soit du code de <productname>PostgreSQL</productname> soit des
    routines système de la plateforme hôte. Dans ce dernier cas, les messages
    pourraient varier entre plateformes mais devraient toujours refléter des
    informations similaires. Ces différences dans les messages résulteront en
    un échec du test de régression qui pourrait être validé après vérification.
   </para>
  </sect2>

  <sect2>
   <title>Différences au niveau des locales</title>

   <para>
    Si vous lancez des tests sur un serveur initialisé avec
    une locale autre que C, alors il pourrait y avoir des différences dans les
    ordres de tris. La suite de tests de régression est initialisée pour gérer
    ce problème en fournissant des fichiers de résultats alternatifs qui
    gèrent ensemble un grand nombre de locales.
   </para>

   <para>
    Pour exécuter les tests dans une locale différente lors de l'utilisation
    de la méthode d'installation temporaire, passez les variables d'environnement
    relatives à la locale sur la ligne de commande de
    <command>make</command>, par exemple&nbsp;:
    <programlisting>
make check LANG=de_DE.utf8
    </programlisting>
    (Le pilote de tests des régressions déconfigure <envar>LC_ALL</envar>,
    donc choisir la locale par cette variable ne fonctionne pas.) Pour
    ne pas utiliser de locale, vous devez soit déconfigurer toutes les
    variables d'environnement relatives aux locales (ou les configurer à
    <literal>C</literal>) ou utiliser une option spéciale&nbsp;:
    <programlisting>
make check NO_LOCALE=1
    </programlisting>
    Lors de l'exécution des tests sur une installation existante, la
    configuration de la locale est déterminée d'après l'installation
    existante. Pour la modifier, initialiser le cluster avec une locale
    différente en passant les options appropriées à <command>initdb</command>.
   </para>

   <para>
    En général, il est conseillé d'essayer l'exécution des tests de régression
    dans la configuration de locale souhaitée pour l'utilisation en production,
    car cela testera aussi les portions de code relatives à l'encodage et à la
    locale qui pourront être utilisées en production. Suivant l'environnement
    du système d'exploitation, vous pourrez obtenir des échecs, mais vous saurez
    au moins le comportement à attendre sur la locale lorsque vous utiliserez
    vos vraies applications.
   </para>
  </sect2>

  <sect2>
   <title>Différences au niveau des dates/heures</title>

   <para>
    La plupart des résultats date/heure sont dépendants de l'environnement
    de zone horaire. Les fichiers de référence sont générés pour la zone
    horaire <literal>PST8PDT</literal> (Berkeley, Californie), et il y aura
    des échecs apparents si les tests ne sont pas lancés avec ce paramétrage de
    fuseau horaire. Le pilote des tests de régression initialise la variable
    d'environnement <envar>PGTZ</envar> à <literal>PST8PDT</literal> ce qui
    nous assure normalement de bons résultats.
   </para>
  </sect2>

  <sect2>
   <title>Différences sur les nombres à virgules flottantes</title>

   <para>
    Quelques tests impliquent des calculs sur des nombres flottants à 64 bits
    (<type>double precision</type>) à partir de colonnes de tables. Des
    différences dans les résultats appliquant des fonctions mathématiques à des
    colonnes <type>double precision</type> ont été observées. Les tests de
    <literal>float8</literal> et <literal>geometry</literal> sont particulièrement sensibles
    aux différences entre plateformes, voire aux différentes options
    d'optimisation des compilateurs. L'&oelig;il humain
    est nécessaire pour déterminer la véritable signification de ces différences,
    habituellement situées après la dixième décimale.
   </para>

   <para>
    Certains systèmes affichent moins zéro comme <literal>-0</literal> alors que
    d'autres affichent seulement <literal>0</literal>.
   </para>

   <para>
    Certains systèmes signalent des erreurs avec <function>pow()</function> et
    <function>exp()</function> différemment suivant le mécanisme attendu du
    code de <productname>PostgreSQL</productname>.
   </para>
  </sect2>

  <sect2>
   <title>Différences dans l'ordre des lignes</title>

   <para>
    Vous pourriez voir des différences dans lesquelles les mêmes lignes sont
    affichées dans un ordre différent de celui qui apparaît dans le fichier de
    référence. Dans la plupart des cas, ce n'est pas à strictement parlé un
    bogue. La plupart des scripts de tests de régression ne sont pas assez
    stricts pour utiliser un <literal>ORDER BY</literal> sur chaque
    <literal>SELECT</literal> et, du coup, l'ordre des lignes pourrait ne pas être
    correctement défini suivant la spécification SQL. En pratique, comme nous
    sommes avec les mêmes requêtes sur les mêmes données avec le même logiciel,
    nous obtenons habituellement le même résultat sur toutes les plateformes et
    le manque d'<literal>ORDER BY</literal> n'est pas un problème. Quelques requêtes
    affichent des différences d'ordre entre plateformes. Lors de tests avec un
    serveur déjà installé, les différences dans l'ordre des lignes peuvent
    aussi être causées par un paramètrage des locales à une valeur différente
    de C ou par un paramètrage personnalisé, comme des valeurs personnalisées
    de <varname>work_mem</varname> ou du coût du planificateur.
   </para>

   <para>
    Du coup, si vous voyez une différence dans l'ordre, vous n'avez pas à vous
    inquiéter sauf si la requête possède un <literal>ORDER BY</literal> que votre
    résultat ne respecte pas. Néanmoins, rapportez tout de même ce problème que nous
    ajoutions un <literal>ORDER BY</literal> à cette requête pour éliminer les faux
    <quote>échecs</quote> dans les versions suivantes.
   </para>

   <para>
    Vous pourriez vous demander pourquoi nous n'ordonnons pas toutes les
    requêtes des tests de régression explicitement pour supprimer ce problème
    une fois pour toutes. La raison est que cela rendrait les tests de
    régression moins utiles car ils tendraient à exercer des types de plans de
    requêtes produisant des résultats ordonnés à l'exclusion de celles qui ne
    le font pas.
   </para>
  </sect2>

  <sect2>
   <title>Profondeur insuffisante de la pile</title>

   <para>
    Si les tests d'<literal>erreurs</literal> se terminent avec un arrêt
    brutal du serveur pendant la commande <literal>select
     infinite_recurse()</literal>, cela signifie que la limite de la plateforme pour
    la taille de pile du processus est plus petite que le paramètre
    <xref linkend="guc-max-stack-depth"/>
    ne l'indique. Ceci est corrigeable
    en exécutant le postmaster avec une limite pour la taille de pile plus
    importante (4&nbsp;Mo est recommandé avec la valeur par défaut de
    <varname>max_stack_depth</varname>). Si vous n'êtes pas capables de le faire, une
    alternative est de réduire la valeur de <varname>max_stack_depth</varname>.
   </para>

   <para>
    Sur les plateformes supportant <function>getrlimit()</function>, le serveur
    devrait choisir automatiquement une valeur sûre pour
    <varname>max_stack_depth</varname>&nbsp;; donc, à moins de surcharger
    manuellement ce paramètre, un échec de ce type est un bug à reporter.
   </para>
  </sect2>

  <sect2>
   <title>Test <quote>random</quote></title>

   <para>
    Le script de tests <literal>random</literal> a pour but de produire
    des résultats aléatoires. Dans de très rares cas, ceci fait échouer random
    aux tests de régression. Saisir&nbsp;:
    <programlisting>diff results/random.out expected/random.out</programlisting>
ne devrait produire au plus que quelques lignes différentes. Cela est normal
     et ne devient préoccupant que si les tests random échouent en permanence
     lors de tests répétés
    </para>
   </sect2>

   <sect2>
    <title>Paramètres de configuration</title>

    <para>
     Lors de l'exécution de tests contre une installation existante, certains
     paramètres configurés à des valeurs spécifiques pourraient causer l'échec
     des tests. Par exemple, modifier des paramètres comme
     <varname>enable_seqscan</varname> ou <varname>enable_indexscan</varname>
     pourrait être la cause de changements de plan affectant le résultat des
     tests qui utilisent <command>EXPLAIN</command>.
    </para>
   </sect2>
  </sect1>

<!-- We might want to move the following section into the developer's guide. -->
    <sect1 id="regress-variant">
     <title>Fichiers de comparaison de variants</title>

     <para>
      Comme certains de ces tests produisent de façon inhérente des résultats
      dépendants de l'environnement, nous avons fourni des moyens de spécifier
      des fichiers résultats alternatifs <quote>attendus</quote>. Chaque test de
      régression peut voir plusieurs fichiers de comparaison affichant les
      résultats possibles sur différentes plateformes. Il existe deux mécanismes
      indépendants pour déterminer quel fichier de comparaison est utilisé pour
      chaque test.
     </para>

     <para>
      Le premier mécanisme permet de sélectionner les fichiers de comparaison
      suivant des plateformes spécifiques. Le fichier de correspondance
      <filename>src/test/regress/resultmap</filename> définit le fichier de
      comparaison à utiliser pour chaque plateforme. Pour éliminer les tests
      <quote>échoués</quote> par erreur pour une plateforme particulière, vous
      choisissez ou vous créez un fichier variant de résultat, puis vous ajoutez
      une ligne au fichier <filename>resultmap</filename>.
     </para>

     <para>
      Chaque ligne du fichier de correspondance est de la forme
      <synopsis>nomtest:sortie:modeleplateform=fichiercomparaison</synopsis>
    Le nom de tests est juste le nom du module de tests de régression
    particulier. La valeur en sortie indique le fichier à vérifier. Pour les
    tests de régression standards, c'est toujours <literal>out</literal>. La
    valeur correspond à l'extension de fichier du fichier en sortie. Le modèle
    de plateforme est un modèle dans le style des outils
    Unix <command>expr</command> (c'est-à-dire une expression rationnelle avec une
    ancre implicite <literal>^</literal> au début). Il est testé avec le nom de
    plateforme affiche par <command>config.guess</command>. Le nom du fichier de
    comparaison est le nom de base du fichier de comparaison substitué.
   </para>

   <para>
    Par exemple&nbsp;: il manque à certains systèmes une fonction
    <literal>strtof</literal> qui fonctionne, pour laquelle notre
    contournement cause des erreurs d'arrondis dans le test de régression
    <filename>float4</filename>.
    Du coup, nous fournissons un fichier de comparaison
    variable, <filename>float4-misrounded-input.out</filename>, qui inclut les
    résultats attendus sur ces systèmes. Pour faire taire les messages
    d'<quote>échec</quote> erronés sur les plateformes
    <systemitem>HP-UX 10</systemitem>, <filename>resultmap</filename> inclut
<programlisting>+float4:out:hppa.*-hp-hpux10.*=float4-misrounded-input.out</programlisting>
    qui se déclenche sur toute machine où la sortie de
    <command>config.guess</command> correspond à
    <literal>hppa.*-hp-hpux10.*</literal>. D'autres lignes dans
    <filename>resultmap</filename> sélectionnent le fichier de comparaison variable pour
    les autres plateformes si c'est approprié.
   </para>

   <para>
    Le second mécanisme de sélection des fichiers de comapraison variants est
    bien plus automatique&nbsp;: il utilise simplement la <quote>meilleure
    correspondance</quote> parmi les différents fichiers de comparaison fournis.
    Le script pilote des tests de régression considère le fichier de comparaison
    standard pour un test, <literal><replaceable>nomtest</replaceable>.out</literal>, et les
    fichiers variants nommés
    <literal><replaceable>nomtest</replaceable>_<replaceable>chiffre</replaceable>.out</literal>
    (où <replaceable>chiffre</replaceable> est un seul chiffre compris entre
    <literal>0</literal> et <literal>9</literal>). Si un tel fichier établit une
    correspondance exacte, le test est considéré réussi&nbsp;; sinon, celui qui
    génère la plus petite différence est utilisé pour créer le rapport d'échec.
    (Si <filename>resultmap</filename> inclut une entrée pour le test
    particulier, alors le <replaceable>nomtest</replaceable> de base est le nom de
    substitut donné dans <filename>resultmap</filename>.)
   </para>

   <para>
    Par exemple, pour le test <literal>char</literal>, le fichier de comparaison
    <filename>char.out</filename> contient des résultats qui sont attendus dans
    les locales <literal>C</literal> et <literal>POSIX</literal>, alors que le fichier
    <filename>char_1.out</filename> contient des résultats triés comme ils
    apparaissent dans plusieurs autres locales.
   </para>

   <para>
    Le mécanisme de meilleure correspondance a été conçu pour se débrouiller
    avec les résultats dépendant de la locale mais il peut être utilisé dans
    toute situation où les résultats des tests ne peuvent pas être prédits
    facilement à partir de la plateforme seule. Une limitation de ce mécanisme
    est que le pilote test ne peut dire quelle variante est en fait
    <quote>correcte</quote> dans l'environnement en cours&nbsp;; il récupèrera la
    variante qui semble le mieux fonctionner. Du coup, il est plus sûr
    d'utiliser ce mécanisme seulement pour les résultats variants que vous
    voulez considérer comme identiquement valides dans tous les contextes.
   </para>

  </sect1>

  <sect1 id="regress-tap">
   <title>Tests TAP</title>

   <para>
    Différents tests, en particulier les tests des programmes clients sous
    <filename>src/bin</filename>, utilisent les outils TAP de Perl et sont
    exécutés en utilisant le programme de tests Perl appelé
    <command>prove</command>.
    Les programmes de test clients situés dans <filename>src/bin</filename>
    utilisent les outils Perl TAP et sont exécutés par <command>prove</command>.
    Il est possible de passer des options en ligne de commande à
    <command>prove</command> en positionnant la variable <command>make</command>
    <varname>PROVE_FLAGS</varname>, par exemple&nbsp;:
<programlisting>
make -C src/bin check PROVE_FLAGS='--timer'
      </programlisting>
      Voir la page de manuel de <command>prove</command> pour plus
      d'information.
     </para>

   <para>
    La variable <varname>PROVE_TESTS</varname> de la commande
    <command>make</command> peut être utilisée pour définir une liste de
    chemins relatifs séparés par des espaces blancs, vers le
    <filename>Makefile</filename> appelant <command>prove</command> pour
    lancer le sous-ensemble spécifié de tests à la place de la valeur par
    défaut <filename>t/*.pl</filename>. Par exemple&nbsp;:
<programlisting>
make check PROVE_TESTS='t/001_test1.pl t/003_test3.pl'
</programlisting>
   </para>

     <para>
      Les tests TAP nécessitent le module <literal>IPC::Run</literal>.
      Ce module est disponible depuis CPAN ou un paquet du système d'exploitation.
     </para>

     <para>
      En général, les tests TAP testeront les exécutables dans une
      arborescence d'installation préalable si vous exécutez <literal>make
      installcheck</literal>, ou construirons une nouvelle arborescence d'installation
      locale à partir des sources courants si vous dites <literal>make check</literal>.
      Dans tous les cas, ils initialiseront une instance locale (répertoire de données)
      et exécuteront temporairement un serveur dessus. Certains de ces tests
      s'exécutent avec plus d'un serveur. De ce fait, ces tests peuvent être très
      consommateurs en ressource.
     </para>
 
     <para>
      Il est important de réaliser que les tests TAP démarreront les serveurs
      de test même quand vous utilisez <literal>make
      installcheck</literal>&nbsp;; ceci est contraire à l'infrastructure
      traditionnelle de tests (donc non TAP) qui s'attend à utiliser un
      serveur de tests déjà en cours d'exécution. Certains sous-répertoires de
      PostgreSQL contiennent à la fois des tests dans le style traditionnel et
      d'autres dans le style TAP, ceci signifiant que <literal>make
      installcheck</literal> produira un mix de résultats provenant de
      serveurs temporaires et d'un serveur de tests déjà en cours d'exécution.
     </para>
    </sect1>

    <sect1 id="regress-coverage">
     <title>Examen de la couverture des tests</title>

     <para>
      Le code source de PostgreSQL peut être compilé avec des informations
      supplémentaire sur la couverture des tests, pour qu'il devienne possible
      d'examiner les parties du code couvertes par les tests de régression ou
      par toute suite de tests exécutée avec le code. Cette fonctionnalité
      est supportée en compilant avec GCC et nécessite les programmes
      <command>gcov</command> et <command>lcov</command>.
     </para>

     <para>
      La suite typique de commandes ressemble à ceci&nbsp;:
      <screen>
./configure --enable-coverage ... OTHER OPTIONS ...
make
make check # or other test suite
make coverage-html
      </screen>
      Puis pointez votre navigateur HTML vers <filename>coverage/index.html</filename>.
      Les commandes <command>make</command> travaillent aussi dans les
      sous-répertoires.
     </para>

   <para>
    Si vous n'avez pas <command>lcov</command> ou préférez une sortie texte
    par rapport à un rapport HTML, vous pouvez aussi exécuter
<screen>
make coverage
</screen>
    au lieu de <literal>make coverage-html</literal>, qui produira des
    fichiers de sortie <filename>.gcov</filename> pour chaque fichier source
    concerné par le test. (<literal>make coverage</literal> et <literal>make
    coverage-html</literal> surchargeront les fichiers de l'autre, donc les
    mixer pourrait apporter de la confusion.)
   </para>

     <para>
      Pour réinitialiser le compteur des exécutions entre chaque test,
      exécutez&nbsp;:
      <screen>
make coverage-clean
      </screen>
     </para>
    </sect1>

   </chapter>
