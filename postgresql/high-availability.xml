<?xml version="1.0" encoding="UTF-8"?>
<!-- Dernière modification
     le       $Date$
     par      $Author$
     révision $Revision$ -->

<chapter id="high-availability">
 <title>Haute disponibilité, répartition de charge et réplication</title>

 <indexterm><primary>haute disponibilité</primary></indexterm>
 <indexterm><primary>failover</primary></indexterm>
 <indexterm><primary>réplication</primary></indexterm>
 <indexterm><primary>répartition de charge</primary></indexterm>
 <indexterm><primary>clustering</primary></indexterm>
 <indexterm><primary>partitionnement de données</primary></indexterm>

<!-- seamlessly ? -->
 <para>
  Des serveurs de bases de données peuvent travailler ensemble pour permettre
  à un serveur secondaire de prendre rapidement la main si le serveur principal
  échoue (haute disponibilité, ou <foreignphrase>high availability</foreignphrase>),
  ou pour permettre à plusieurs serveurs de servir les mêmes données (répartition
  de charge, ou <foreignphrase>load balancing</foreignphrase>). Idéalement, les
  serveurs de bases de données peuvent travailler ensemble sans jointure.
 </para>
  
 <para>
  Il est aisé de faire coopérer des serveurs web qui traitent des pages web statiques
  en répartissant la charge des requêtes web sur plusieurs
  machines. Dans les faits, les serveurs de bases de données en lecture seule peuvent
  également coopérer facilement. Malheureusement, la plupart des
  serveurs de bases de données traitent des requêtes de lecture/écriture et,
  de ce fait, collaborent plus difficilement. En effet, alors qu'il suffit de
  placer une seule fois les données en lecture seule sur chaque serveur, une
  écriture sur n'importe quel serveur doit, elle, être propagée à tous les
  serveurs afin que les lectures suivantes sur ces serveurs renvoient des résultats
  cohérents.
 </para>

 <para>
  Ce problème de synchronisation représente la difficulté fondamentale à la
  collaboration entre serveurs. Comme la solution au problème de
  synchronisation n'est pas unique pour tous les cas pratiques, plusieurs
  solutions co-existent. Chacune répond de façon différente et minimise
  cet impact au regard d'une charge spécifique.
 </para>

 <para>
  Certaines solutions gèrent la synchronisation en autorisant les modifications
  des données sur un seul serveur. Les serveurs qui peuvent modifier les données
  sont appelés serveur en lecture/écriture, <firstterm>maître</firstterm> ou serveur <firstterm>primaire</firstterm>.
  Les serveurs qui suivent les modifications du maître sont appelés <firstterm>standby</firstterm>,
  ou serveurs <firstterm>esclaves</firstterm>. Un serveur en standby auquel on ne peut pas
  se connecter tant qu'il n'a pas été promu en serveur maître est appelé un serveur en <firstterm>warm
  standby</firstterm>, et un qui peut accepter des connections et répondre à des requêtes en
  lecture seule est appelé un serveur en <firstterm>hot standby</firstterm>.
 </para>

 <para>
  Certaines solutions sont synchrones, ce qui signifie qu'une transaction de
  modification de données n'est pas considérée valide tant que tous les
  serveurs n'ont pas validé la transaction. Ceci garantit qu'un
  <foreignphrase>failover</foreignphrase> ne perd pas de données et que tous
  les serveurs en répartition de charge retournent des résultats cohérents, quel
  que soit le serveur interrogé. Au contraire, les solutions asynchrones
  autorisent un délai entre la validation et sa propagation aux
  autres serveurs. Cette solution implique une éventuelle perte de transactions
  lors de la bascule sur un serveur de sauvegarde, ou l'envoi de données
  obsolètes par les serveurs à charge répartie. La communication asynchrone est
  utilisée lorsque la version synchrone est trop lente.
 </para>

 <para>
  Les solutions peuvent aussi être catégorisées par leur granularité. Certaines
  ne gèrent que la totalité d'un serveur de bases alors que
  d'autres autorisent un contrôle par table ou par base.
 </para>

 <para>
  Il importe de considérer les performances dans tout choix. Il y
  a généralement un compromis à trouver entre les fonctionnalités et les
  performances. Par exemple, une solution complètement synchrone sur un réseau
  lent peut diviser les performances par plus de deux, alors qu'une
  solution asynchrone peut n'avoir qu'un impact minimal sur les performances.
 </para>

 <para>
  Le reste de cette section souligne différentes solutions de
  <foreignphrase>failover</foreignphrase>, de réplication et de répartition de
  charge. Un <ulink
  url="http://www.postgres-r.org/documentation/terms">glossaire</ulink> est
  aussi disponible.
 </para>

 <sect1 id="different-replication-solutions">
 <title>Comparaison de différentes solutions</title>

 <variablelist>

 <varlistentry>
  <term><foreignphrase>Failover</foreignphrase> sur disque partagé</term>
  <listitem>

   <para>
    Le <foreignphrase>failover</foreignphrase> (ou bascule sur incident)
    sur disque partagé élimine la surcharge de synchronisation par
    l'existence d'une seule copie de la base de données. Il utilise un
    seul ensemble de disques partagé par plusieurs serveurs. Si le serveur
    principal échoue, le serveur en attente
    est capable de monter et démarrer la base comme s'il récupérait d'un
    arrêt brutal. Cela permet un <foreignphrase>failover</foreignphrase>
    rapide sans perte de données.
   </para>

   <para>
    La fonctionnalité de matériel partagé est commune aux périphériques de
    stockage en réseau. Il est également possible d'utiliser un système de
    fichiers réseau bien qu'il faille porter une grande attention au système de
    fichiers pour s'assurer qu'il a un comportement <acronym>POSIX</acronym>
    complet (voir <xref linkend="creating-cluster-nfs"/>). Cette méthode
    comporte une limitation significative&nbsp;: si les disques ont un
    problème ou sont corrompus, le serveur primaire et le serveur en attente sont tous
    les deux non fonctionnels. Un autre problème est que le serveur en attente
    ne devra jamais accéder au stockage partagé tant que le serveur principal
    est en cours d'exécution.
   </para>

   </listitem>
  </varlistentry>

  <varlistentry>
   <term>Réplication de système de fichiers (périphérique bloc)</term>
   <listitem>

   <para>
    Il est aussi possible d'utiliser cette fonctionnalité d'une autre façon
    avec une réplication du système de fichiers, où toutes les modifications
    d'un système de fichiers sont renvoyées sur un système de fichiers situé
    sur un autre ordinateur. La seule restriction est que ce miroir doit être
    construit de telle sorte que le serveur en attente dispose d'une
    version cohérente du système de fichiers &mdash; spécifiquement, les
    écritures sur le serveur en attente doivent être réalisées dans le même
    ordre que celles sur le maître. <productname>DRBD</productname> est une
    solution populaire de réplication de systèmes de fichiers pour Linux.
   </para>

<!--
https://forge.continuent.org/pipermail/sequoia/2006-November/004070.html

Oracle RAC is a shared disk approach and just send cache invalidations
to other nodes but not actual data. As the disk is shared, data is
only committed once to disk and there is a distributed locking
protocol to make nodes agree on a serializable transactional order.
-->

  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Envoi des journaux de transactions</term>
  <listitem>

   <para>
    Les serveurs <foreignphrase>warm et hot standby</foreignphrase> (voir <xref
    linkend="warm-standby"/>) peuvent conserver leur cohérence en lisant un flux
    d'enregistrements de <acronym>WAL</acronym>. Si le serveur principal
    échoue, le serveur
    <foreignphrase>standby</foreignphrase> contient pratiquement toutes
    les données du serveur principal et peut rapidement devenir le nouveau
    serveur maître. Ça peut être synchrone mais ça ne peut se faire que pour le
    serveur de bases complet.
   </para>
    <para>
     Un serveur de standby peut être implémenté en utilisant la recopie de journaux par fichier
     (<xref linkend="warm-standby"/>)  ou la streaming replication (réplication en continu, voir 
     <xref linkend="streaming-replication"/>), ou une combinaison des deux. Pour
     des informations sur le hot standby, voyez <xref linkend="hot-standby"/>..
    </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Réplication maître/esclave basé sur des triggers</term>
  <listitem>

   <para>
    Une configuration de réplication maître/esclave envoie toutes les requêtes
    de modification de données au serveur maître. Ce serveur envoie les
    modifications de données de façon asynchrone au serveur esclave. L'esclave
    peut répondre aux requêtes en lecture seule alors que le serveur maître
    est en cours d'exécution. Le serveur esclave est idéal pour les requêtes
    vers un entrepôt de données.
   </para>

   <para>
    <productname>Slony-I</productname> est un exemple de ce type de
    réplication, avec une granularité par
    table et un support des esclaves multiples. Comme il met à jour le serveur
    esclave de façon asynchrone (par lots), il existe une possibilité de perte
    de données pendant un <foreignphrase>failover</foreignphrase>.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term><foreignphrase>Middleware</foreignphrase> de réplication basé sur les
    instructions</term>
  <listitem>

   <para>
    Avec les <foreignphrase>middleware</foreignphrase> de réplication basés
    sur les instructions, un programme intercepte chaque requête SQL et
    l'envoie à un ou tous les serveurs. Chaque serveur opère indépendamment.
    Les requêtes en lecture/écriture doivent être envoyées à tous les
    serveurs pour que chaque serveur reçoive les modifications. Les
    requêtes en lecture seule ne peuvent être envoyées qu'à un seul
    serveur, ce qui permet de distribuer la charge de lecture.
   </para>

   <para>
    Si les requêtes sont envoyées sans modification, les fonctions comme
    <function>random()</function>, <function>CURRENT_TIMESTAMP</function> ainsi
    que les séquences ont des valeurs différentes sur les différents serveurs.
    Cela parce que chaque serveur opère indépendamment alors que
    les requêtes SQL sont diffusées (et non les données
    modifiées). Si cette solution est inacceptable, le
    <foreignphrase>middleware</foreignphrase> ou l'application doivent
    demander ces valeurs à un seul serveur, et les utiliser dans
    des requêtes d'écriture. Une autre solution est d'utiliser cette solution de réplication
    avec une configuration maître-esclave traditionnelle, c'est à dire que les requêtes
    de modification de données ne sont envoyées qu'au maître et sont propagées aux
    esclaves via une réplication maître-esclave, pas par le middleware de
    réplication.  Il est impératif que
    toute transaction soit validée ou annulée sur tous les serveurs,
    éventuellement par validation en deux phases (<xref
    linkend="sql-prepare-transaction"/> et <xref linkend="sql-commit-prepared"/>.
    <productname>Pgpool-II</productname> et <productname>Continuent
    Tungsten</productname> sont des exemples de ce type de réplication.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Réplication asynchrone multi-maîtres</term>
  <listitem>

   <para>
    Pour les serveurs qui ne sont pas connectés en permanence, comme les
    ordinateurs portables ou les serveurs distants, conserver la cohérence des données
    entre les serveurs est un challenge. L'utilisation de la réplication asynchrone
    multi-maîtres permet à chaque serveur de fonctionner indépendamment. Il
    communique alors périodiquement avec les autres serveurs pour identifier les transactions
    conflictuelles. La gestion des conflits est alors confiée aux utilisateurs
    ou à un système de règles de résolution.
		Bucardo est un exemple de ce type de réplication.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Réplication synchrone multi-maîtres</term>
  <listitem>

   <para>
    Dans les réplications synchrones multi-maîtres, tous les serveurs acceptent
    les requêtes en écriture. Les données modifiées sont transmises
    du serveur d'origine à tous les autres serveurs avant toute validation de
    transaction.
   </para>
   <para>
    Une activité importante en écriture peut être la cause d'un
    verrouillage excessif et conduire à un effondrement des performances. Dans
    les faits, les performances en écriture sont souvent pis que celles d'un
    simple serveur.
   </para>
   <para>
    Tous les serveurs acceptent les requêtes en lecture.
   </para>
   <para>
    Certaines implantations utilisent les disques partagés pour réduire la surcharge
    de communication.
   </para>
   <para>
    Les performances de la réplication synchrone multi-maîtres sont meilleures lorsque
    les opérations de lecture représentent l'essentiel de la charge, alors que
    son gros avantage est l'acceptation des requêtes d'écriture par tous les
    serveurs &mdash; 
    il n'est pas nécessaire de répartir la charge entre les serveurs
    maîtres et esclaves et, parce que les modifications de données sont envoyées
    d'un serveur à l'autre, les fonctions non déterministes, comme
    <function>random()</function>, ne posent aucun problème.
   </para>

   <para>
    <productname>PostgreSQL</productname> n'offre pas ce type de réplication,
    mais la validation en deux phases de <productname>PostgreSQL</productname>
    (<xref linkend="sql-prepare-transaction"/> et <xref linkend="sql-commit-prepared"/>)
    autorise son intégration dans une application ou un
    <foreignphrase>middleware</foreignphrase>.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Solutions commerciales</term>
  <listitem>

   <para>
    Parce que <productname>PostgreSQL</productname> est libre et facilement
    extensible, certaines sociétés utilisent <productname>PostgreSQL</productname>
    dans des solutions commerciales fermées
    (<foreignphrase>closed-source</foreignphrase>) proposant des fonctionnalités de
    bascule sur incident (<foreignphrase>failover</foreignphrase>),
    réplication et répartition de charge.
   </para>
  </listitem>
 </varlistentry>

 </variablelist>

 <para>
  La <xref linkend="high-availability-matrix"/> résume les
  possibilités des différentes solutions listées plus-haut.
 </para>

 <table id="high-availability-matrix">
  <title>Matrice de fonctionnalités&nbsp;: haute disponibilité, répartition de
    charge et réplication</title>
  <tgroup cols="8">
   <thead>
    <row>
     <entry>Fonctionnalité</entry>
     <entry>Bascule par disques partagés (<foreignphrase>Shared Disk
     Failover</foreignphrase>)</entry>
     <entry>Réplication par système de fichiers</entry>
     <entry>Envoi des journaux de transactions</entry>
     <entry>Réplication maître/esclave basé sur les triggers</entry>
     <entry><foreignphrase>Middleware</foreignphrase> de réplication
       sur instructions</entry>
     <entry>Réplication asynchrone multi-maîtres</entry>
     <entry>Réplication synchrone multi-maîtres</entry>
    </row>
   </thead>

   <tbody>

    <row>
     <entry>Exemple d'implémentation</entry>
     <entry align="center">NAS</entry>
     <entry align="center">DRBD</entry>
     <entry align="center">Réplication en flux</entry>
     <entry align="center">Slony</entry>
     <entry align="center">pgpool-II</entry>
     <entry align="center">Bucardo</entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>Méthode de communication</entry>
     <entry align="center">Disque partagé</entry>
     <entry align="center">Blocs disque</entry>
     <entry align="center">WAL</entry>
     <entry align="center">Lignes de tables</entry>
     <entry align="center">SQL</entry>
     <entry align="center">Lignes de tables</entry>
     <entry align="center">Lignes de tables et verrous de ligne</entry>
    </row>

    <row>
     <entry>Ne requiert aucun matériel spécial </entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Autorise plusieurs serveurs maîtres </entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Pas de surcharge sur le serveur maître </entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>Pas d'attente entre serveurs</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">avec sync à off</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
    </row>

    <row>
     <entry>Pas de perte de données en cas de panne du maître</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">avec sync à on</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Les esclaves acceptent les requêtes en lecture seule</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">avec un Hot Standby</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Granularité de niveau table</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
    </row>

    <row>
     <entry>Ne nécessite pas de résolution de conflit</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center">&bull;</entry>
     <entry align="center"></entry>
     <entry align="center"></entry>
     <entry align="center">&bull;</entry>
    </row>

   </tbody>
  </tgroup>
 </table>

 <para>
  Certaines solutions n'entrent pas dans les catégories ci-dessus&nbsp;:
 </para>

 <variablelist>

 <varlistentry>
  <term>Partitionnement de données</term>
  <listitem>

   <para>
    Le partitionnement des données divise les tables en ensembles de données.
    Chaque ensemble ne peut être modifié que par un seul serveur. Les
    données peuvent ainsi être partitionnées par bureau, Londres et
    Paris, par exemple, avec un serveur dans chaque bureau. Si certaines
    requêtes doivent combiner des données de Londres et Paris, il est possible
    d'utiliser une application qui requête les deux serveurs ou d'implanter une
    réplication maître/esclave pour conserver sur chaque serveur une copie en lecture
    seule des données de l'autre bureau.
   </para>
  </listitem>
 </varlistentry>

 <varlistentry>
  <term>Exécution de requêtes en parallèle sur plusieurs serveurs</term>
  <listitem>

   <para>
    La plupart des solutions ci-dessus permettent à plusieurs serveurs de
    répondre à des requêtes multiples, mais aucune ne permet à une seule requête
    d'être exécutée sur plusieurs serveurs pour se terminer plus rapidement.
    Cette solution autorisent plusieurs serveurs à travailler ensemble sur une
    seule requête. Ceci s'accomplit habituellement en répartissant les données
    entre les serveurs, chaque serveur exécutant une partie de la
    requête pour renvoyer les résultats à un serveur central qui les combine
    et les renvoie à l'utilisateur. <productname>Pgpool-II</productname>
    offre cette possibilité. Cela peut également être implanté en utilisant les
    outils <productname>PL/Proxy</productname>.
   </para>
  </listitem>
 </varlistentry>

 </variablelist>

 </sect1>

 <sect1 id="warm-standby">
 <title>Serveurs de Standby par transfert de journaux</title>


  <para>
   L'archivage en continu peut être utilisé pour créer une configuration
   de cluster en <firstterm>haute disponibilité</firstterm> (HA) avec un ou
   plusieurs <firstterm>serveurs de standby</firstterm> prêts à prendre la main
   sur les opérations si le serveur primaire fait défaut. Cette fonctionnalité
   est généralement appelée 
   <firstterm>warm standby</firstterm> ou <firstterm>log shipping</firstterm>.
  </para>

  <para>
   Les serveurs primaire et de standby travaillent de concert pour fournir cette fonctionnalité,
   bien que les serveurs ne soient que faiblement couplés. Le serveur primaire opère
   en mode d'archivage en continu, tandis que le serveur de standby opère en
   mode de récupération en continu, en lisant les fichiers WAL provenant du primaire. Aucune
   modification des tables de la base ne sont requises pour activer cette fonctionnalité,
   elle entraîne donc moins de travail d'administration par rapport à d'autres
   solutions de réplication. Cette configuration a aussi un impact relativement
   faible sur les performances du serveur primaire.
  </para>

  <para>
   Déplacer directement des enregistrements de WAL d'un serveur de bases de données à un autre
   est habituellement appelé log shipping. <productname>PostgreSQL</productname>
   implémente le log shipping par fichier, ce qui signifie que les enregistrements de WAL sont
   transférés un fichier (segment de WAL) à la fois. Les fichiers de WAL (16Mo) peuvent être
   transférés facilement et de façon peu coûteuse sur n'importe quelle distance, que ce soit sur un
   système adjacent, un autre système sur le même site, ou un autre système à
   l'autre bout du globe. La bande passante requise pour cette technique
   varie en fonction du débit de transactions du serveur primaire.
   La technique de streaming replication permet d'optimiser cette bande
   passante en utilisant une granularité plus fine que le log shipping
   par fichier. Pour cela, les modifications apportées au journal de
   transactions sont traitées sous forme de flux au travers d'une
   connexion réseau (voir <xref linkend="streaming-replication"/>).
  </para>

  <para>
   Il convient de noter que le log shipping est asynchrone, c'est à dire que les 
   enregistrements de WAL sont transférés après que la transaction ait été validée. Par conséquent, il y a
   un laps de temps pendant lequel une perte de données pourrait se produire si le serveur primaire
   subissait un incident majeur; les transactions pas encore transférées seront perdues. La taille de la fenêtre
   de temps de perte de données peut être réduite par l'utilisation du paramètre
   <varname>archive_timeout</varname>, qui peut être abaissé à des valeurs
   de quelques secondes. Toutefois, un paramètre si bas augmentera de façon 
   considérable la bande passante nécessaire pour le transfert de fichiers.
   L'utilisation de la technique de streaming replication (voir <xref linkend="streaming-replication"/>)
   permet de diminuer la taille de la fenêtre de temps de perte de données.
  </para>

  <para>
   La performance de la récupération est suffisamment bonne pour que le standby ne
   soit en général qu'à quelques instants de la pleine
   disponibilité à partir du moment où il aura été activé. C'est pour cette raison que
   cette configuration de haute disponibilité est appelée warm standby.
   Restaurer un serveur d'une base de sauvegarde archivée, puis appliquer tous les journaux
   prendra largement plus de temps, ce qui fait que cette technique est une solution
   de 'disaster recovery' (reprise après sinistre), pas de haute disponibilité.
   Un serveur de standby peut aussi être utilisé pour des requêtes en lecture seule, dans
   quel cas il est appelé un serveur de Hot Standby. Voir <xref linkend="hot-standby"/> pour
   plus d'information.
 </para>

  <indexterm zone="high-availability">
   <primary>warm standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>PITR standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>serveur de standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>log shipping</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>serveur témoin</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>STONITH</primary>
  </indexterm>

  <sect2 id="standby-planning">
   <title>Préparatifs</title>

   <para>
    Il est habituellement préférable de créer les serveurs primaire et de standby
    de façon à ce qu'ils soient aussi similaires que possible, au moins du
    point de vue du serveur de bases de données. En particulier, les chemins
    associés avec les tablespaces seront passés d'un noeud à l'autre sans conversion, ce qui
    implique que les serveurs primaire et de standby doivent avoir les mêmes chemins de montage pour
    les tablespaces si cette fonctionnalité est utilisée. Gardez en tête que si
    <xref linkend="sql-createtablespace"/> 
    est exécuté sur le primaire, tout nouveau point de montage nécessaire pour cela doit être créé
    sur le primaire et tous les standby avant que la commande ne
    soit exécutée. Le matériel n'a pas besoin d'être exactement le même, mais l'expérience monte
    que maintenir deux systèmes identiques est plus facile que maintenir deux
    différents sur la durée de l'application et du système.
    Quoi qu'il en soit, l'architecture hardware doit être la même &mdash; répliquer
    par exemple d'un serveur 32 bits vers un 64 bits ne fonctionnera pas.
   </para>

   <para>
    De manière générale, le log shipping entre serveurs exécutant des versions 
    majeures différentes de <productname>PostgreSQL</productname> est
    impossible. La politique du PostgreSQL Global Development Group est de ne pas
    réaliser de changement sur les formats disques lors des mises à jour mineures,
    il est par conséquent probable que l'exécution de versions mineures différentes
    sur le primaire et le standby fonctionne correctement. Toutefois, il n'y a 
    aucune garantie formelle de cela et il est fortement conseillé de garder le 
    serveur primaire et celui de standby au même niveau de version autant que faire
    se peut. Lors d'une mise à jour vers une nouvelle version mineure, la politique la
    plus sûre est de mettre à jour les serveurs de standby d'abord &mdash; une nouvelle
    version mineure est davantage susceptible de lire les enregistrements WAL d'une
    ancienne version mineure que l'inverse.
   </para>

  </sect2>

  <sect2 id="standby-server-operation">
   <title>Fonctionnement du Serveur de Standby</title>

   <para>
    En mode de standby, le serveur applique continuellement les WAL reçus du
    serveur maître. Le serveur de standby peut lire les WAL d'une archive WAL
    (voir <xref linkend="restore-command"/>) ou directement du maître via une
    connexion TCP (streaming replication). Le serveur de standby essaiera aussi de
    restaurer tout WAL trouvé dans le répertoire <filename>pg_xlog</filename> du
    cluster de standby. Cela se produit habituellement après un redémarrage de
    serveur, quand le standby rejoue à nouveau les WAL qui ont été reçu du maître
    avant le redémarrage, mais vous pouvez aussi copier manuellement des fichiers dans
    <filename>pg_xlog</filename> à tout moment pour qu'ils soient rejoués.
   </para>

   <para>
    Au démarrage, le serveur de standby commence par restaurer tous les WAL
    disponibles à l'endroit où se trouvent les archives, en appelant la
    <varname>restore_command</varname>. Une fois qu'il a épuisé tous les WAL
    disponibles à cet endroit et que <varname>restore_command</varname>
    échoue, il essaye de restaurer tous les WAL disponibles dans le répertoire
    <filename>pg_xlog</filename>. Si cela échoue, et que la réplication en flux
    a été activée, le standby essaye
    de se connecter au serveur primaire et de démarrer la réception des WAL depuis
    le dernier enregistrement valide trouvé dans les archives ou
    <filename>pg_xlog</filename>. Si cela
    échoue ou que la streaming replication n'est pas configurée, ou que la connexion
    est plus tard déconnectée, le standby retourne à l'étape 1 et essaye de 
    restaurer le fichier à partir de l'archive à nouveau. Cette boucle de
    retentatives de l'archive, <filename>pg_xlog</filename> et par la
    streaming replication continue
    jusqu'à ce que le serveur soit stoppé ou que le failover (bascule) soit
    déclenché par un fichier trigger (déclencheur).
   </para>

   <para>
    Le mode de standby est quitté et le serveur bascule en mode de fonctionnement normal
    quand <command>pg_ctl promote</command> est exécuté ou qu'un fichier de trigger est trouvé (<varname>trigger_file</varname>). 
	Avant de basculer, tout WAL immédiatement disponible dans l'archive ou le
    <filename>pg_xlog</filename> sera
    restauré, mais aucune tentative ne sera faite pour se connecter au maître.
   </para>
  </sect2>

  <sect2 id="preparing-master-for-standby">
   <title>Préparer le Maître pour les Serveurs de Standby</title>

   <para>
    Mettez en place un archivage en continu sur le primaire vers un répertoire
    d'archivage accessible depuis le standby, comme décrit 
    dans <xref linkend="continuous-archiving"/>. La destination d'archivage devrait être
    accessible du standby même quand le maître est inaccessible, c'est à dire qu'il
    devrait se trouver sur le serveur de standby lui-même ou un autre serveur de confiance, pas sur
    le serveur maître.
   </para>

   <para>
    Si vous voulez utiliser la streaming replication, mettez en place l'authentification sur le
    serveur primaire pour autoriser les connexions de réplication à partir du (des) serveur de
    standby&nbsp;; c'est-à-dire, créez un rôle et mettez en place une ou des entrées appropriées dans
    <filename>pg_hba.conf</filename> avec le champ database positionné à
    <literal>replication</literal>. Vérifiez aussi que <varname>max_wal_senders</varname> est positionné
    à une valeur suffisamment grande dans le fichier de configuration du serveur primaire.
   </para>

   <para>
    Effectuez une sauvegarde de base comme décrit dans <xref linkend="backup-base-backup"/>
    pour initialiser le serveur de standby.
   </para>
  </sect2>

  <sect2 id="standby-server-setup">
   <title>Paramétrer un Serveur de Standby</title>

   <para>
    Pour paramétrer le serveur de standby, restaurez la sauvegarde de base effectué sur
    le serveur primaire (voir (see <xref linkend="backup-pitr-recovery"/>). Créez un
    fichier de commande de récupération <filename>recovery.conf</filename> dans
    le répertoire de données du cluster de standby, et positionnez <varname>standby_mode</varname>
    à on. Positionnez <varname>restore_command</varname> à une simple commande qui recopie
    les fichiers de l'archive de WAL. Si vous comptez disposer de plusieurs serveurs de stanby pour
	mettre en &oelig;uvre de la haute disponibilité, définissez <varname>recovery_target_timeline</varname> à
	<literal>latest</literal>, pour indiquer que le serveur de standby devra prendre en compte
	la ligne temporelle définie lors de la bascule à un autre serveur de standby.
   </para>

   <note>
     <para>
     N'utilisez pas pg_standby ou des outils similaires avec le mode de standby intégré
     décrit ici. <varname>restore_command</varname> devrait retourner immédiatement
     si le fichier n'existe pas; le serveur essayera la commande à nouveau si nécessaire.
     Voir <xref linkend="log-shipping-alternative"/> pour utiliser des outils tels que pg_standby.
    </para>
   </note>

   <para>
     Si vous souhaitez utiliser la streaming replication, renseignez
     <varname>primary_conninfo</varname> avec une chaîne de connexion libpq,
     contenant le nom d'hôte (ou l'adresse IP) et tout détail supplémentaire
     nécessaire pour se connecter au serveur primaire. Si le primaire a besoin d'un
     mot de passe pour l'authentification, le mot de passe doit aussi être spécifié dans
     <varname>primary_conninfo</varname>.
   </para>

   <para>
    Si vous mettez en place le serveur de standby pour des besoins de haute disponibilité,
    mettez en place l'archivage de WAL, les connexions et l'authentification à l'identique
    du serveur primaire, parce que le serveur de standby fonctionnera comme un serveur primaire
    après la bascule.
   </para>

   <para>
    Si vous utilisez une archive WAL, sa taille peut être réduite en utilisant
    l'option <xref linkend="archive-cleanup-command"/> pour supprimer les
    fichiers qui ne sont plus nécessaires au serveur de standby. L'outil
    <application>pg_archivecleanup</application> est conçu spécifiquement pour
    être utilisé avec <varname>archive_cleanup_command</varname> dans des
    configurations typiques de standby, voir <xref linkend="pgarchivecleanup"/>.
    Notez toutefois que si vous utilisez l'archive à des fins de sauvegarde,
    vous avez besoin de garder les fichiers nécessaires pour restaurer à partir
    de votre dernière sauvegarde de base, même si ces fichiers ne sont plus
    nécessaires pour le standby.
   </para>

   <para>
    If you're using a WAL archive, its size can be minimized using the  parameter to remove files that are no
    longer required by the standby server.

    Note however, that if you're using the archive for backup purposes, you
    need to retain files needed to recover from at least the latest base
    backup, even if they're no longer needed by the standby.
   </para>

   <para>
    Un simple exemple de <filename>recovery.conf</filename> est:
<programlisting>
standby_mode = 'on'
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
restore_command = 'cp /path/to/archive/%f %p'
archive_cleanup_command = 'pg_archivecleanup /path/to/archive %r'
</programlisting>
   </para>

   <para>
    Vous pouvez avoir n'importe quel nombre de serveurs de standby, mais si vous
    utilisez la streaming replication, assurez vous d'avoir positionné
    <varname>max_wal_senders</varname> suffisamment haut sur le primaire pour leur permettre
    de se connecter simultanément.
   </para>
  </sect2>

  <sect2 id="streaming-replication">
   <title>Streaming Replication</title>

   <indexterm zone="high-availability">
    <primary>Streaming Replication</primary>
   </indexterm>

   <para>
    La streaming replication permet à un serveur de standby de rester plus
    à jour qu'il n'est possible avec l'envoi de journaux par fichiers. Le
    standby se connecte au primaire, qui envoie au standby les enregistrements
    de WAL dès qu'ils sont générés, sans attendre qu'un fichier de WAL soit rempli.
   </para>

   <para>
    La streaming replication est asynchrone par défaut (voir <xref
    linkend="synchronous-replication"/>), auquel cas il y a un petit délai
    entre la validation d'une transaction sur le primaire et le moment où les
    changements sont visibles sur le standby. Le délai est toutefois beaucoup plus petit
    qu'avec l'envoi de fichiers, habituellement en dessous d'une seconde en partant
    de l'hypothèse que le standby est suffisamment puissant pour supporter la charge. Avec
    la streaming replication, <varname>archive_timeout</varname> n'est pas nécessaire
    pour réduire la fenêtre de perte de données.
   </para>

   <para>
    Si vous utilisez la streaming replication sans archivage en continu des fichiers,
    vous devez positionner <varname>wal_keep_segments</varname> sur le maître à une valeur
    suffisamment grande pour garantir que les anciens segments de WAL ne sont pas
    recyclés trop tôt, alors que le standby pourrait toujours avoir besoin d'eux pour
    rattraper son retard. Si le standby prend trop de retard, il aura besoin d'être réinitialisé
    à partir d'une nouvelle sauvegarde de base. Si vous positionnez une archive de WAL qui est accessible
    du standby, <varname>wal_keep_segments</varname> n'est pas nécessaire, puisque le standby peut toujours
    utiliser l'archive pour rattraper son retard.
   </para>

   <para>
    Pour utiliser la streaming replication, mettez en place un serveur de standby
    en mode fichier comme décrit dans <xref linkend="warm-standby"/>. L'étape qui
    transforme un standby en mode fichier en standby en streaming replication est de
    faire pointer <varname>primary_conninfo</varname> dans le fichier
    <filename>recovery.conf</filename> vers le serveur primaire. Positionnez
    <xref linkend="guc-listen-addresses"/> et les options d'authentification
    (voir <filename>pg_hba.conf</filename>) sur le primaire pour que le serveur
    de standby puisse se connecter à la pseudo-base <literal>replication</literal> 
    sur le serveur primaire (voir <xref linkend="streaming-replication-authentication"/>).
   </para>

   <para>
    Sur les systèmes qui supportent l'option de keepalive sur les sockets, positionner
    <xref linkend="guc-tcp-keepalives-idle"/>,
    <xref linkend="guc-tcp-keepalives-interval"/> et
    <xref linkend="guc-tcp-keepalives-count"/> aide le primaire à reconnaître rapidement
    une connexion interrompue.
   </para>

   <para>
    Positionnez le nombre maximum de connexions concurrentes à partir des
    serveurs de standby (voir <xref linkend="guc-max-wal-senders"/> pour les détails).
   </para>

   <para>
    Quand le standby est démarré et que <varname>primary_conninfo</varname> est
    positionné correctement, le standby se connectera au primaire après avoir
    rejoué tous les fichiers WAL disponibles dans l'archive. Si la connexion
    est établie avec succès, vous verrez un processus walreceiver dans le standby, et
    un processus walsender correspondant sur le primaire.
   </para>

   <sect3 id="streaming-replication-authentication">
    <title>Authentification</title>
    <para>
     Il est très important que les privilèges d'accès pour la réplications soient
     paramétrés pour que seuls les utilisateurs de confiance puissent lire le
     flux WAL, parce qu'il est facile d'en extraire des informations
     privilégiées. Les serveurs de standby doivent s'authentifier au serveur
     primaire en tant que superutilisateur ou avec un compte disposant de
     l'attribut <literal>REPLICATION</literal>. Il est recommandé de créer
     un compte utilisateur dédié pour la réplication. Il doit disposer des
     attributs <literal>REPLICATION</literal> et <literal>LOGIN</literal>.
     Alors que l'attribut <literal>REPLICATION</literal> donne beaucoup de
     droits, il ne permet pas à l'utilisateur de modifier de données sur le
     serveur primaire, contrairement à l'attribut <literal>SUPERUSER</literal>.
    </para>

    <para>
     L'authentification cliente pour la réplication est contrôlée par un enregistrement de
     <filename>pg_hba.conf</filename> spécifiant <literal>replication</literal> dans le champ
     <replaceable>database</replaceable>. Par exemple, si le standby s'exécute sur un hôte d'IP
     <literal>192.168.1.100</literal>  et que le nom de l'utilisateur pour la réplication est
     <literal>foo</literal>, l'administrateur peut ajouter la ligne suivante au fichier
     <filename>pg_hba.conf</filename>  sur le primaire:
     

<programlisting>
# Autoriser l'utilisateur "foo" de l'hôte 192.168.1.100 à se connecter au primaire
# en tant que standby de replication si le mot de passe de l'utilisateur est correctement fourni
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    replication     foo             192.168.1.100/32        md5
</programlisting>
    </para>
    <para>
     Le nom d'hôte et le numéro de port du primaire, le nom d'utilisateur de la connexion,
     et le mot de passe sont spécifiés dans le fichier <filename>recovery.conf</filename>.
     Le mot de passe peut aussi être enregistré dans le fichier
     <filename>~/.pgpass</filename> sur le serveur en attente (en précisant
     <literal>replication</literal> dans le champ
     <replaceable>database</replaceable>).
     Par exemple, si le primaire s'exécute sur l'hôte d'IP <literal>192.168.1.50</literal>,
     port <literal>5432</literal>, que le nom de l'utilisateur pour la réplication est
     <literal>foo</literal>, et que le mot de passe est <literal>foopass</literal>, l'administrateur
     peut ajouter la ligne suivante au fichier <filename>recovery.conf</filename> sur le standby:

<programlisting>
# Le standby se connecte au primaire qui s'exécute sur l'hôte 192.168.1.50
# et port 5432 en tant qu'utilisateur "foo" dont le mot de passe est "foopass"
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
</programlisting>
    </para>
   </sect3>

   <sect3 id="streaming-replication-monitoring">
    <title>Supervision</title>
    <para>
     Un important indicateur de santé de la streaming replication est le nombre
     d'enregistrements générés sur le primaire, mais pas encore appliqués sur
     le standby. Vous pouvez calculer ce retard en comparant le point d'avancement
     des écritures du WAL sur le primaire avec le dernier point d'avancement reçu par
     le standby. Ils peuvent être récupérés en utilisant
     <function>pg_current_xlog_location</function> sur le primaire et
     <function>pg_last_xlog_receive_location</function> sur le standby,
     respectivement (voir <xref linkend="functions-admin-backup-table"/> et
     <xref linkend="functions-recovery-info-table"/> pour plus de détails).
     Le point d'avancement de la réception dans le standby est aussi affiché dans
     le statut du processus de réception des WAL (wal receiver), affiché par
     la commande <command>ps</command> (voyez <xref linkend="monitoring-ps"/> pour plus de détails).
    </para>
    <para>
	 Vous pouvez obtenir la liste des processus émetteurs de WAL au moyen de la vue 
	 <link linkend="monitoring-stats-views-table"><literal>pg_stat_replication</literal></link>
	 D'importantes différences entre les champs <function>pg_current_xlog_location</function> et 
	 <literal>sent_location</literal> peuvent indiquer que le serveur maître est en surcharge,
	 tandis que des différences entre <literal>sent_location</literal> et 
	 <function>pg_last_xlog_receive_location</function> sur le standby peuvent soit indiquer une latence
	 réseau importante, soit que le standby est surchargé.
    </para>
   </sect3>

  </sect2>

  <sect2 id="cascading-replication">
   <title>Réplication en cascade</title>

   <indexterm zone="high-availability">
    <primary>Réplication en cascade</primary>
   </indexterm>

   <para>
    La fonctionnalité de la réplication en cascade permet à un serveur
    standard d'accepter les connexions de réplication et d'envoyer un flux
    d'enregistrements de journaux de transactions à d'autres esclaves,
    agissant ainsi comme un relai. C'est généralement utilisé pour réduire
    le nombre de connexions directes au maître et minimise ainsi l'utilisation
    de bande passante entre sites distants.
   </para>

   <para>
    Un serveur standby agissant à la fois comme un receveur et comme un
    émetteur est connu sous le nom de standby en cascade
    (<foreignphrase>cascading standby</foreignphrase>). Les standbys qui
    sont plus proches du serveur maître sont connus sous le nom
    de serveurs <foreignphrase>upstream</foreignphrase> alors que les serveurs
    standby en bout de chaîne sont des serveurs
    <foreignphrase>downstream</foreignphrase>. La réplication en cascade
    ne pose pas de limites sur le nombre ou l'arrangement des serveurs
    <foreignphrase>downstream</foreignphrase>. Chaque standby se connecte à un
    seul serveur <foreignphrase>upstream</foreignphrase>, qui finit par
    arriver à un seul serveur maître/primaire.
   </para>

   <para>
    Un standby en cascade envoie non seulement les enregistrements reçus de
    journaux de transactions mais aussi ceux restaurés des archives. Donc,
    même si la connexion de réplication d'une connexion upstream est
    rompue, la réplication en flux continue vers le serveur downstream tant
    que de nouveaux enregistrements de journaux de transactions sont
    disponibles.
   </para>

   <para>
    La réplication en cascade est actuellement asynchrone. La réplication
    synchrone (voir <xref linkend="synchronous-replication"/>) n'a aucun
    effet sur la réplication en cascade.
   </para>
 
   <para>
    Les messages en retour des serveurs Hot Standby se propagent vers les
    serveurs upstream, quelque soit la configuration de la réplication en
    cascade.
   </para>

   <para>
    If an upstream standby server is promoted to become new master, downstream
    servers will continue to stream from the new master if
    <varname>recovery_target_timeline</varname> is set to <literal>'latest'</literal>.
   </para>

   <para>
    Pour utiliser la réplication en cascade, configurez le standby en cascade
    de façon à ce qu'il accepte les connexions de réplication (configurez
    <xref linkend="guc-max-wal-senders"/> et <xref linkend="guc-hot-standby"/>,
    ainsi que l'<link linkend="auth-pg-hba-conf">authentification</link>).
    Vous aurez aussi besoin de configurer la variable
    <varname>primary_conninfo</varname> dans le standby downstream pour qu'elle
    pointe vers le standby en cascade.
   </para>
  </sect2>

  <sect2 id="synchronous-replication">
   <title>Réplication synchrone</title>

   <indexterm zone="high-availability">
    <primary>Réplication Synchrone</primary>
   </indexterm>

   <para>
    La streaming réplication mise en &oelig;uvre par <productname>PostgreSQL</productname> est asynchrone 
	par défaut. Si le serveur primaire est hors-service, les transactions produites alors
	peuvent ne pas avoir été répliquées sur le serveur de standby, impliquant une perte
	de données. La quantité de données perdues est proportionnelle au délai de réplication
	au moment de la bascule.
   </para>

   <para>
    La réplication synchrone permet de confirmer que tous les changements effectués par une 
	transaction ont bien été transférées à un serveur de standby
	synchrone. Cette propriété étend le niveau de robustesse standard
	offert par un commit. En science informatique, ce niveau de protection est appelé
	<foreignphrase>2-safe replication</foreignphrase>.
   </para>

   <para>
    Lorsque la réplication synchrone est utilisée, chaque validation portant
	sur une écriture va nécessiter d'attendre la confirmation
	de l'écriture de cette validation sur les journaux de transaction des disques
	du serveur primaire et des serveurs en standby. Le seul moyen possible pour que des données
	soient perdues est que les serveur primaire et de standby soient hors service au 
	même moment. Ce mécanisme permet d'assurer un niveau plus élevé de robustesse, en admettant que
	l'administrateur système ait pris garde à l'emplacement et à la gestion de ces deux
	serveurs. Attendre après la confirmation de l'écriture augmente la confiance que l'utilisateur pourra avoir sur
	la conservation des modifications dans le cas où un serveur serait hors service mais il augmente aussi
	en conséquence le temps de réponse à chaque requête.
	Le temps minimum d'attente est celui de l'aller-retour entre les serveurs primaire et de standby.
   </para>

   <para>
    Les transactions où seule une lecture est effectuée ou qui consistent à annuler une transaction ne nécessitent pas d'attendre
	les serveurs de standby. Les validations concernant les transactions imbriquées ne nécessitent pas non plus d'attendre
	la réponse des serveurs de standby, cela n'affecte en fait que les validations principales. De longues
	opérations comme le chargement de données ou la création d'index n'attendent pas
	le commit final pour synchroniser les données. Toutes les actions de validation en deux étapes
	nécessitent d'attendre la validation du standby, incluant autant l'opération de préparation que l'opération de validation.
   </para>

   <sect3 id="synchronous-replication-config">
    <title>Configuration de base</title>

   <para>
    Une fois la streaming replication configurée, la configuration de la réplication synchrone
  	ne demande qu'une unique étape de configuration supplémentaire&nbsp;:
	  la variable <xref linkend="guc-synchronous-standby-names"/> doit être définie à
	  une valeur non vide. La variable <varname>synchronous_commit</varname> doit aussi être définie à
	  <literal>on</literal>, mais comme il s'agit d'une valeur par défaut, il n'est pas nécessaire de la
	  modifier. (Voir <xref linkend="runtime-config-wal-settings"/> et
    <xref linkend="runtime-config-replication-master"/>.)
  Cette configuration va entraîner l'attente de la confirmation de l'écriture permanente de chaque validation
	sur le serveur de standby.
	La variable <varname>synchronous_commit</varname> peut être définie soit par des
	utilisateurs, soit par le fichier de configuration pour des utilisateurs ou des bases de données fixées, soit
	dynamiquement par des applications, pour contrôler la robustesse des échanges transactionnels.
   </para>

   <para>
    Suite à l'enregistrement sur disque d'une validation sur le serveur primaire,
    l'enregistrement WAL est envoyé au serveur de standby. Le serveur de standby
    retourne une réponse à chaque fois qu'un nouveau lot de données WAL est
    écrit sur disque, à moins que la variable
    <varname>wal_receiver_status_interval</varname> soit définie à zéro sur le
    serveur de standby. Lorsque le premier serveur de standby est sollicité,
    tel que spécifié dans la variable <varname>synchronous_standby_names</varname>
    sur le serveur primaire, la réponse de ce serveur de standby sera utilisée
    pour prévenir les utilisateurs en attente de confirmation de l'enregistrement
    du commit. Ces paramètres permettent à l'administrateur de spécifier quels
    serveurs de standby suivront un comportement synchrone. Remarquez ici que
    la configuration de la réplication synchrone se situe sur le serveur maître.
    Les serveurs standbys nommés doivent être directement connectés au
    maître&nbsp;; le maître ne connaît rien des serveurs standbys utilisant
    la réplication en cascade.
   </para>

   <para>
    Configurer <varname>synchronous_commit</varname> à
    <literal>remote_write</literal> fera que chaque COMMIT attendra la
    confirmation de la réception en mémoire de l'enregistrement du COMMIT
    par le standby et son écriture via la système d'exploitation, sans que
    les données du cache du système ne soient vidées sur disque au niveau
    du serveur en standby.  Cette configuration fournit une garantie moindre
    de durabilité que la configuration <literal>on</literal>&nbsp;: le standby
    peut perdre les données dans le cas d'un crash du système d'exploitation,
    mais pas dans le cas du crash de <productname>PostgreSQL</productname>.
    Cependant, il s'agit d'une configuration utile en pratique car il diminue
    le temps de réponse pour la transaction. Des pertes de données ne peuvent
    survenir que si le serveur primaire et le standby tombent en même temps et
    que la base de données du primaire est corrompue.
   </para>

   <para>
    Habituellement, un signal d'arrêt rapide (<foreignphrase>fast shutdown</foreignphrase>)
    annule les transactions en cours sur tous les processus serveur. Cependant, dans
	le cas de la réplication asynchrone, le serveur n'effectuera pas un
	arrêt complet avant que chaque enregistrement WAL ne soit transféré aux serveurs
	de standby connectés.
   </para>

    </sect3>
 
   <sect3 id="synchronous-replication-performance">
    <title>S'organiser pour obtenir de bonnes performances</title>

   <para>
    La réplication synchrone nécessite souvent d'organiser avec une grande attention
	les serveurs de standby pour apporter un bon niveau de performances aux applications. Les phases d'attente d'écriture
	n'utilisent pas les ressources systèmes, mais les verrous transactionnels restent
	positionnés jusqu'à ce que le transfert vers les serveurs de standby soit confirmé. En conséquence, une utilisation non avertie de
	la réplication synchrone aura pour impact une baisse des performances de la base de donnée
	d'une application due à l'augmentation des temps de réponses et à un moins bon support de la charge.
   </para>

   <para>
    <productname>PostgreSQL</productname> permet aux développeurs d'application 
	de spécifier le niveau de robustesse à employer pour la réplication. Cela peut être
	spécifié pour le système entier, mais aussi pour
	des utilisateurs ou des connexions spécifiques, ou encore pour des transactions individuelles.
   </para>

   <para>
    Par exemple, une répartition du travail pour une application pourrait être constituée de&nbsp;:
	10 % de modifications concernant des articles de clients importants, et
	90 % de modifications de moindre importance et qui ne devraient pas avoir d'impact sur le métier
	si elles venaient à être perdues, comme des dialogues de messagerie entre utilisateurs.
   </para>

   <para>
    Les options de réplication synchrone spécifiées par une application
	(sur le serveur primaire) permettent de n'utiliser la réplication synchrone que pour les modifications les plus
	importantes, sans affecter les performances sur la plus grosse partie des traitements.
	Les options modifiables par les applications sont un outil important permettant
	d'apporter les bénéfices de la réplication synchrone aux applications nécessitant de la haute performance.
   </para>

   <para>
    Il est conseillé de disposer d'une bande passante réseau supérieure
	à la quantité de données WAL générées.
   </para>

   </sect3>

   <sect3 id="synchronous-replication-ha">
    <title>S'organiser pour la haute disponibilité</title>

   <para>
    Les opérations de validation effectuées avec la variable <varname>synchronous_commit</varname> définie à <literal>on</literal>
	ou à <literal>remote_write</literal> nécessiteront d'attendre la réponse du serveur de standby. Cette réponse pourrait ne jamais arriver
	si le seul ou le dernier serveur de standby venait à être hors service.
   </para>

   <para>
    La meilleure solution pour éviter la perte de données est de s'assurer de ne jamais perdre
	le dernier serveur de standby. Cette politique peut être mise en oeuvre en définissant plusieurs
	serveurs de standby via la variable <varname>synchronous_standby_names</varname>.
	Le premier serveur de standby nommé dans cette variable sera utilisé comme serveur de standby synchrone. Les serveurs
	suivants prendront le rôle de serveur de standby synchrone si le premier venait à
	être hors service.
   </para>

   <para>
    Au moment où le premier serveur de standby s'attache au serveur primaire, il est possible qu'il ne soit pas exactement
	synchronisé. Cet état est appelé le mode <literal>catchup</literal>. Une fois
	la différence entre le serveur de standby et le serveur primaire ramenée à zéro,
	le mode <literal>streaming</literal> est atteint.
	La durée du mode catchup peut être longue surtout juste après la création du serveur de standby.
	Si le serveur de standby est arrêté sur cette période, alors la durée du mode CATCHUP
	sera d'autant plus longue.
	Le serveur de standby ne peut devenir un serveur de standby synchrone
	que lorsque le mode <literal>streaming</literal> est atteint.
   </para>

   <para>
    Si le serveur primaire redémarre alors que des opérations de commit étaient en attente de confirmation, les
	transactions en attente ne seront réellement enregistrées qu'au moment où la base de donnée du serveur primaire
	sera redémarrée.
	Il n'y a aucun moyen de savoir si tous les serveurs de standby ont reçu toutes
	les données WAL nécessaires au moment où le serveur primaire est déclaré hors-service. Des
	transactions pourraient ne pas être considérées comme sauvegardées sur le serveur de standby, même si
	elles l'étaient sur le serveur primaire. La seule garantie offerte dans ce cadre est que
	l'application ne recevra pas de confirmation explicite de la
	réussite d'une opération de validation avant qu'il soit sûr que les données WAL sont
	reçues proprement par le serveur de standby.
   </para>

   <para>
    Si le dernier serveur de standby est perdu, il est conseillé de désactiver
	la variable <varname>synchronous_standby_names</varname> et de recharger le fichier de configuration
	sur le serveur primaire.
   </para>

   <para>
    Si le serveur primaire n'est pas accessible par les serveurs de standby restants, il est conseillé
	de basculer vers le meilleur candidat possible parmi ces serveurs de standby.
   </para>

   <para>
    S'il est nécessaire de recréer un serveur de standby alors que des transactions sont
	en attente de confirmation, prenez garde à ce que les commandes pg_start_backup() et
	pg_stop_backup() soient exécutées dans un contexte où
	la variable <varname>synchronous_commit</varname> vaut <literal>off</literal> car, dans le cas contraire, ces
	requêtes attendront indéfiniment l'apparition de ce serveur de standby.
   </para>

   </sect3>

  </sect2>
  </sect1>

  <sect1 id="warm-standby-failover">
   <title>Bascule (<foreignphrase>Failover</foreignphrase>)</title>

   <para>
    Si le serveur primaire plante alors le serveur de standby devrait commencer
    les procédures de failover.
   </para>

   <para>
    Si le serveur de standby plante alors il n'est pas nécessaire d'effectuer un failover. Si le
    serveur de standby peut être redémarré, même plus tard, alors le processus de récupération
    peut aussi être redémarré au même moment, en bénéficiant du fait que la récupération sait reprendre
    où elle en était. Si le serveur de standby ne peut pas être redémarré, alors
    une nouvelle instance complète de standby devrait être créé.
   </para>

   <para>
    Si le serveur primaire plante, que le serveur de standby devient le 
    nouveau primaire, et que l'ancien primaire redémarre, vous devez avoir
    un mécanisme pour informer l'ancien primaire qu'il n'est plus primaire. C'est aussi
    quelquefois appelé <acronym>STONITH</acronym> (Shoot The Other Node In The Head, ou
    Tire Dans La Tête De L'Autre Noeud), qui est nécessaire pour éviter les situations où
    les deux systèmes pensent qu'ils sont le primaire, ce qui amènerait de la confusion, et
    finalement de la perte de données.
   </para>

   <para>
    Beaucoup de systèmes de failover n'utilisent que deux systèmes, le primaire et le standby,
    connectés par un mécanisme de type ligne de vie (heartbeat) pour vérifier continuellement la
    connexion entre les deux et la viabilité du primaire. Il est aussi
    possible d'utiliser un troisième système (appelé un serveur témoin) pour éviter
    certains cas de bascule inappropriés, mais la complexité supplémentaire
    peut ne pas être justifiée à moins d'être mise en place avec suffisamment
    de précautions et des tests rigoureux.
   </para>

   <para>
    <productname>PostgreSQL</productname> ne fournit pas le logiciel
    système nécessaire pour identifier un incident sur le primaire et notifier
    le serveur de base de standby. De nombreux outils de ce genre existent et sont bien
    intégrés avec les fonctionnalités du système d'exploitation nécessaires à la bascule,
    telles que la migration d'adresse IP.
   </para>

   <para>
    Une fois que la bascule vers le standby se produit, il n'y a plus qu'un
    seul serveur en fonctionnement. C'est ce qu'on appelle un état dégradé.
    L'ancien standby est maintenant le primaire, mais l'ancien primaire est arrêté
    et pourrait rester arrêté. Pour revenir à un fonctionnement normal, un serveur
    de standby doit être recréé,
    soit sur l'ancien système primaire quand il redevient disponible, ou sur un troisième,
    peut être nouveau, système. Une fois que ceci est effectué, le primaire et le standby peuvent
    être considérés comme ayant changé de rôle. Certaines personnes choisissent d'utiliser un troisième
    serveur pour fournir une sauvegarde du nouveau primaire jusqu'à ce que le nouveau serveur de
    standby soit recréé,
    bien que ceci complique visiblement la configuration du système et les procédures d'exploitation.
   </para>

   <para>
    Par conséquent, basculer du primaire vers le serveur de standby peut être rapide mais requiert
    du temps pour re-préparer le cluster de failobver. Une bascule régulière du
    primaire vers le standby est utile, car cela permet une période d'interruption de production sur
    chaque système pour maintenance. Cela vous permet aussi pour vous assurer que 
    votre mécanisme de bascule fonctionnera réellement quand vous en aurez besoin.
		Il est conseillé que les procédures d'administration soient écrites.
   </para>

   <para>
    Pour déclencher le failover d'un serveur de standby en log-shipping, exécutez la commande <command>pg_ctl promote</command> or créez un fichier
    trigger (déclencheur) avec le nom de fichier et le chemin spécifiés par le paramètre
    <varname>trigger_file</varname> de <filename>recovery.conf</filename>. Si vous comptez utiliser
	la commande <command>pg_ctl promote</command> pour effectuer la bascule, la variable  <varname>trigger_file</varname> n'est
	pas nécessaire. S'il s'agit d'ajouter des serveurs qui ne
	seront utilisés que pour alléger le serveur primaire des requêtes en lecture seule, et non pas pour des considérations de haute
	disponibilité, il n'est pas nécessaire de les réveiller (<foreignphrase>promote</foreignphrase>).
   </para>
  </sect1>

  <sect1 id="log-shipping-alternative">
   <title>Méthode alternative pour le log shipping</title>

   <para>
    Une alternative au mode de standby intégré décrit dans les sections précédentes
    est d'utiliser une <varname>restore_command</varname> qui scrute le dépôt d'archives.
    C'était la seule méthode disponible dans les versions 8.4 et inférieures. Dans cette configuration,
    positionnez <varname>standby_mode</varname>  à off, parce que vous implémentez la scrutation nécessaire
    au fonctionnement standby vous-mêmes. Voir le module <xref linkend="pgstandby"/> pour
    une implémentation de référence de ceci.
   </para>

   <para>
    Veuillez noter que dans ce mode, le serveur appliquera les WAL fichier par fichier,
    ce qui entraîne que si vous requêtez sur le serveur de standby (voir Hot Standby),
    il y a un délai entre une action sur le maître et le moment où cette action
    devient visible sur le standby, correspondant au temps nécessaire à 
    remplir le fichier de WAL. <varname>archive_timeout</varname>  peut être utilisé pour rendre ce délai
    plus court. Notez aussi que vous ne pouvez combiner la streaming replication avec cette méthode.
   </para>

   <para>
    Les opérations qui se produisent sur le primaire et les serveurs de standby sont
    des opérations normales d'archivage et de recovery. Le seul point de
    contact entre les deux serveurs de bases de données est l'archive de fichiers WAL
    qu'ils partagent: le primaire écrivant dans l'archive, le secondaire
    lisant de l'archive. Des précautions doivent être prises pour s'assurer que les archives WAL de serveurs
    primaires différents ne soient pas mélangées ou confondues. L'archive n'a pas besoin
    d'être de grande taille si elle n'est utilisée que pour le fonctionnement de standby.
   </para>

   <para>
    La magie qui permet aux deux serveurs faiblement couplés de fonctionner ensemble est
    une simple <varname>restore_command</varname> utilisée sur le standby qui
    quand on lui demande le prochain fichier de WAL, attend que le primaire le mette
    à disposition. La <varname>restore_command</varname>  est spécifiée dans le
    fichier <filename>recovery.conf</filename>  sur le serveur de standby. La récupération normale
    demanderait un fichier de l'archive WAL, en retournant un échec si le
    fichier n'était pas disponible. Pour un fonctionnement en standby, il est normal que
    le prochain fichier WAL ne soit pas disponible, ce qui entraîne que le standby doive attendre
    qu'il apparaisse. Pour les fichiers se terminant en <literal>.backup</literal> ou
    <literal>.history</literal> il n'y a pas besoin d'attendre, et un code retour
    différent de zéro doit être retourné. Une <varname>restore_command</varname>  d'attente
    peut être écrite comme un script qui boucle après avoir scruté l'existence du prochain fichier de WAL.
    Il doit aussi y avoir un moyen de déclencher la bascule, qui devrait interrompre la
    <varname>restore_command</varname> , sortir le la boucle et retourner une erreur file-not-found 
    au serveur de standby. Cela met fin à la récupération et le standby démarrera alors comme un serveur normal.
   </para>

   <para>
    Le pseudocode pour une <varname>restore_command</varname> appropriée est:
<programlisting>
triggered = false;
while (!NextWALFileReady() &amp;&amp; !triggered)
{
    sleep(100000L);         /* wait for ~0.1 sec */
    if (CheckForExternalTrigger())
        triggered = true;
}
if (!triggered)
        CopyWALFileForRecovery();
</programlisting>
   </para>

   <para>
    Un exemple fonctionnel de <varname>restore_command</varname> d'attente est fournie
    par le module <xref linkend="pgstandby"/>. Il
    devrait être utilisé en tant que référence, comme la bonne façon d'implémenter correctement la logique
    décrite ci-dessus. Il peut aussi être étendu pour supporter des configurations et des 
    environnements spécifiques.
   </para>

   <para>
    La méthode pour déclencher une bascule est une composante importante de la
    planification et de la conception. Une possibilité est d'utiliser la
    commande <varname>restore_command</varname>. Elle est exécutée une fois
    pour chaque fichier WAL, mais le processus exécutant la <varname>restore_command</varname>
    est créé et meurt pour chaque fichier, il n'y a donc ni démon ni processus serveur, et
    on ne peut utiliser ni signaux ni gestionnaire de signaux.  Par conséquent, la
    <varname>restore_command</varname> n'est pas appropriée pour déclencher la bascule.
    Il est possible d'utiliser une simple fonctionnalité de timeout, particulièrement
    si utilisée en conjonction avec un paramètre <varname>archive_timeout</varname>
    sur le primaire. Toutefois, ceci est sujet à erreur, un problème réseau
    ou un serveur primaire chargé pouvant suffire à déclencher une bascule. Un système
    de notification comme la création explicite d'un fichier trigger est idéale, dans la
    mesure du possible.
   </para>

  <sect2 id="warm-standby-config">
   <title>Implémentation</title>

   <para>
    La procédure simplifié pour configurer un serveur de test en utilisant cette
    méthode alternative est la suivante. Pour tous les détails
    sur chaque étape, référez vous aux sections précédentes suivant les indications.
    <orderedlist>
     <listitem>
      <para>
       Paramétrez les systèmes primaire et standby de façon aussi identique que possible,
       y compris deux copies identiques de <productname>PostgreSQL</productname> au même niveau
       de version.
      </para>
     </listitem>
     <listitem>
      <para>
       Activez l'archivage en continu du primaire vers un répertoire d'archives WAL
       sur le serveur de standby. Assurez vous que 
       <xref linkend="guc-archive-mode"/>,
       <xref linkend="guc-archive-command"/> et
       <xref linkend="guc-archive-timeout"/>
       sont positionnés correctement sur le primaire
       (voir <xref linkend="backup-archiving-wal"/>).
      </para>
     </listitem>
     <listitem>
      <para>
       Effectuez une sauvegarde de base du serveur primaire( voir <xref
       linkend="backup-base-backup"/>), , et chargez ces données sur le standby.
      </para>
     </listitem>
     <listitem>
      <para>
       Commencez la récupération sur le serveur de standby à partir de l'archive WAL locale,
       en utilisant un <filename>recovery.conf</filename>  qui spécifie une
       <varname>restore_command</varname>  qui attend comme décrit
       précédemment (voir <xref linkend="backup-pitr-recovery"/>).
      </para>
     </listitem>
    </orderedlist>
   </para>

   <para>
    Le récupération considère l'archive WAL comme étant en lecture seule, donc une fois qu'un fichier WAL
    a été copié sur le système de standby il peut être copié sur bande en même temps
    qu'il est lu par le serveur de bases de données de standby.
    Ainsi, on peut faire fonctionner un serveur de standby pour de la haute disponibilité
    en même temps que les fichiers sont stockés pour de la reprise après sinistre.
   </para>

   <para>
    À des fins de test, il est possible de faire fonctionner le serveur primaire et
    de standby sur le même système. Cela n'apporte rien en termes de robustesse du serveur,
    pas plus que cela ne pourrait être décrit comme de la haute disponibilité.
   </para>
  </sect2>
.
  <sect2 id="warm-standby-record">
   <title>Log Shipping par Enregistrements</title>

   <para>
    Il est aussi possible d'implémenter du log shipping par enregistrements en utilisant
    cette méthode alternative, bien qu'elle nécessite des développements spécifiques,
    et que les modifications ne seront toujours visibles aux requêtes de hot standby qu'après
    que le fichier complet de WAL ait été recopié.
   </para>

   <para>
    Un programme externe peut appeler la fonction <function>pg_xlogfile_name_offset()</function>
    (voir <xref linkend="functions-admin"/>) pour obtenir le nom de fichier et la position exacte
    en octets dans ce fichier de la fin actuelle du WAL. Il peut alors accéder au fichier WAL directement
    et copier les données de la fin précédente connue à la fin courante vers les serveurs de standby.
    Avec cette approche, la fenêtre de perte de données est la période de scrutation du programme de copie,
    qui peut être très petite, et il n'y a pas de bande passante gaspillée en forçant l'archivage 
    de fichiers WAL partiellement remplis. Notez que les scripts <varname>restore_command</varname>
    des serveurs de standby ne peuvent traiter que des fichiers WAL complets, les données copiées
    de façon incrémentale ne sont donc d'ordinaire pas mises à  disposition des serveurs de standby.
    Elles ne sont utiles que si le serveur primaire tombe &mdash; alors le dernier fichier WAL partiel
    est fourni au standby avant de l'autoriser à s'activer. L'implémentation correcte de ce
    mécanisme requiert la coopération entre le script <varname>restore_command</varname> et
    le programme de recopie des données.
   </para>

   <para>
    À partir de <productname>PostgreSQL</productname> version 9.0, vous pouvez utiliser
    la streaming replication (voir <xref linkend="streaming-replication"/>) pour
    bénéficier des mêmes fonctionnalités avec moins d'efforts.
   </para>
  </sect2>
 </sect1>

 <sect1 id="hot-standby">
  <title>Hot Standby</title>

  <indexterm zone="high-availability">
   <primary>Hot Standby</primary>
  </indexterm>

   <para>
    Hot Standby est le terme utilisé pour décrire la possibilité de se
    connecter et d'exécuter des requêtes en lecture seule alors que le
    serveur est en récupération d'archive or standby mode. C'est
    utile à la fois pour la réplication et pour restaurer
    une sauvegarde à un état désiré avec une grande précision.
    Le terme Hot Standby fait aussi référence à la capacité du serveur à passer
    de la récupération au fonctionnement normal tandis-que les utilisateurs
    continuent à exécuter des requêtes et/ou gardent leurs connexions ouvertes.
   </para>

   <para>
    Exécuter des requêtes en mode hot standby est similaire au fonctionnement
    normal des requêtes, bien qu'il y ait quelques différences d'utilisation
    et d'administration notées ci-dessous.
   </para>

  <sect2 id="hot-standby-users">
   <title>Aperçu pour l'utilisateur</title>

   <para>
    Quand le paramètre <xref linkend="guc-hot-standby"/> est configuré à true
    sur un serveur en attente, le serveur commencera à accepter les connexions
    une fois que la restauration est parvenue à un état cohérent. Toutes les
    connexions qui suivront seront des connexions en lecture seule&nbsp;; même
    les tables temporaires ne pourront pas être utilisées.
   </para>

   <para>
    Les données sur le standby mettent un certain temps pour arriver du serveur
    primaire, il y aura donc un délai mesurable entre primaire et standby. La même
    requête exécutée presque simultanément sur le primaire et le standby pourrait par
    conséquent retourner des résultats différents. On dit que la donnée est
    <firstterm>cohérente à terme</firstterm>  avec le primaire. Une fois que
    l'enregistrement de validation (COMMIT) d'une transaction est rejoué sur
    le serveur en attente, les modifications réalisées par cette transaction
    seront visibles par toutes les images de bases obtenues par les transactions
    en cours sur le serveur en attente. Ces images peuvent être prises au début
    de chaque requête ou de chaque transaction, suivant le niveau d'isolation
    des transactions utilisé à ce moment. Pour plus de détails, voir <xref
    linkend="transaction-iso"/>.
   </para>

   <para>
    Les transactions exécutées pendant la période de restauration sur un
    serveur en mode hotstandby peuvent inclure les commandes suivantes&nbsp;:
    <itemizedlist>
     <listitem>
      <para>
       Accès par requête - <command>SELECT</command>, <command>COPY TO</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de curseur - <command>DECLARE</command>, <command>FETCH</command>, <command>CLOSE</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Paramètres - <command>SHOW</command>, <command>SET</command>, <command>RESET</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de gestion de transaction
        <itemizedlist>
         <listitem>
          <para>
           <command>BEGIN</command>, <command>END</command>, <command>ABORT</command>, <command>START TRANSACTION</command>
          </para>
         </listitem>
         <listitem>
          <para>
           <command>SAVEPOINT</command>, <command>RELEASE</command>, <command>ROLLBACK TO SAVEPOINT</command>
          </para>
         </listitem>
         <listitem>
          <para>
           Blocs d'<command>EXCEPTION</command> et autres sous-transactions internes
          </para>
         </listitem>
        </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK TABLE</command>, mais seulement quand explicitement dans un de ces modes:
       <literal>ACCESS SHARE</literal>, <literal>ROW SHARE</literal> ou <literal>ROW EXCLUSIVE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Plans et ressources - <command>PREPARE</command>, <command>EXECUTE</command>,
       <command>DEALLOCATE</command>, <command>DISCARD</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Plugins et extensions - <command>LOAD</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Les transactions lancées pendant la restauration d'un serveur en hotstandby
    ne se verront jamais affectées un identifiant de transactions et ne peuvent
    pas être écrites dans les journaux de transactions. Du coup, les actions
    suivantes produiront des messages d'erreur&nbsp;:

    <itemizedlist>
     <listitem>
      <para>
       Langage de Manipulation de Données (LMD ou DML) - <command>INSERT</command>,
       <command>UPDATE</command>, <command>DELETE</command>, <command>COPY FROM</command>,
       <command>TRUNCATE</command>.
       Notez qu'il n'y a pas d'action autorisée qui entraînerait l'exécution d'un 
       trigger pendant la récupération. Cette restriction s'applique même pour
       les tables temporaires car les lignes de ces tables ne peuvent être
       lues et écrites s'il n'est pas possible d'affecter un identifiant de
       transactions, ce qui n'est actuellement pas possible dans un
       environnement Hot Standby.
      </para>
     </listitem>
     <listitem>
      <para>
       Langage de Définition de Données (LDD ou DDL) - <command>CREATE</command>,
       <command>DROP</command>, <command>ALTER</command>, <command>COMMENT</command>.
       Cette restriction s'applique aussi aux tables temporaires car, pour
       mener à bien ces opérations, cela nécessiterait de mettre à jour les
       catalogues systèmes.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>SELECT ... FOR SHARE | UPDATE</command>, car les verrous de
       lignes ne peuvent pas être pris sans mettre à jour les fichiers de
       données.
      </para>
     </listitem>
     <listitem>
      <para>
       Rules sur des ordres <command>SELECT</command> qui génèrent des commandes LMD.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> qui demandent explicitement un mode supérieur à <literal>ROW EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> dans sa forme courte par défaut, puisqu'il demande <literal>ACCESS EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
      Commandes de gestion de transaction qui positionnent explicitement un état n'étant pas en lecture-seule:
        <itemizedlist>
         <listitem>
          <para>
            <command>BEGIN READ WRITE</command>,
            <command>START TRANSACTION READ WRITE</command>
          </para>
         </listitem>
         <listitem>
          <para>
            <command>SET TRANSACTION READ WRITE</command>,
            <command>SET SESSION CHARACTERISTICS AS TRANSACTION READ WRITE</command>
          </para>
         </listitem>
         <listitem>
          <para>
           <command>SET transaction_read_only = off</command>
          </para>
         </listitem>
        </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de two-phase commit <command>PREPARE TRANSACTION</command>,
       <command>COMMIT PREPARED</command>, <command>ROLLBACK PREPARED</command>
       parce que même les transactions en lecture seule ont besoin d'écrire dans le WAL
       durant la phase de préparation (la première des deux phases du two-phase commit).
      </para>
     </listitem>
     <listitem>
      <para>
       Mise à jour de séquence - <function>nextval()</function>, <function>setval()</function>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LISTEN</command>, <command>UNLISTEN</command>, <command>NOTIFY</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Dans le cadre normal, les transactions <quote>en lecture seule</quote>
    permettent la mise à jour des séquences et l'utilisation des instructions
    <command>LISTEN</command>, <command>UNLISTEN</command> et
    <command>NOTIFY</command>, donc les sessions Hot Standby ont des
    restrictions légèrement inférieures à celles de sessions en lecture seule
    ordinaires. Il est possible que certaines des restrictions soient encore
    moins importantes dans une prochaine version.
   </para>

   <para>
    Lors du fonctionnement en serveur hotstandby, le paramètre
    <varname>transaction_read_only</varname> est toujours à true et ne peut
    pas être modifié. Tant qu'il n'y a pas de tentative de modification sur
    la base de données, les connexions sur un serveur en hotstandby se
    comportent de façon pratiquement identiques à celles sur un serveur normal.
    Quand une bascule (<foreignphrase>failover</foreignphrase> ou
    <foreignphrase>switchover</foreignphrase>) survient, la base de données
    bascule dans le mode de traitement normal. Les sessions resteront
    connectées pendant le changement de mode. Quand le mode hotstandby est
    terminé, il sera possible de lancer des transactions en lecture/écriture,
    y compris pour les sessions connectées avant la bascule.
   </para>

   <para>
    Les utilisateurs pourront déterminer si leur session est en lecture seule en
    exécutant <command>SHOW transaction_read_only</command>. De plus, un jeu de
    fonctions (<xref linkend="functions-recovery-info-table"/>) permettent aux utilisateurs d'
    accéder à des informations à propos du serveur de standby. Ceci vous permet d'écrire
    des programmes qui sont conscients de l'état actuel de la base. Vous pouvez
    vous en servir pour superviser l'avancement de la récupération, ou pour écrire des
    programmes complexes qui restaurent la base dans des états particuliers.
   </para>
  </sect2>

  <sect2 id="hot-standby-conflict">
   <title>Gestion des conflits avec les requêtes</title>

   <para>
    Les noeuds primaire et standby sont de bien des façons faiblement couplés. Des
    actions sur le primaire auront un effet sur le standby. Par conséquent, il y a
    un risque d'interactions négatives ou de conflits entre eux. Le conflit le
    plus simple à comprendre est la performance : si un gros chargement de données a
    lieu sur le primaire, il générera un flux similaire d'enregistrements WAL sur le
    standby, et les requêtes du standby pourrait entrer en compétition pour les ressources
    systèmes, comme les entrées-sorties.
   </para>

   <para>
    Il y a aussi d'autres types de conflits qui peuvent se produire avec le
    Hot Standby. Ces conflits sont des <emphasis>conflits durs</emphasis> dans
    le sens où des requêtes pourraient devoir être annulées et, dans certains
    cas, des sessions déconnectées, pour les résoudre. L'utilisateur dispose
    de plusieurs moyens pour gérer ces conflits. Voici les différents cas de
    conflits possibles&nbsp;:

      <itemizedlist>
       <listitem>
        <para>
         Des verrous en accès exclusif pris sur le serveur maître, incluant à
         la fois les commandes <command>LOCK</command> exclusives et quelques
         actions de type <acronym>DDL</acronym>, entrent en conflit avec les
         accès de table des requêtes en lecture seule.
        </para>
       </listitem>
       <listitem>
        <para>
         La suppression d'un tablespace sur le serveur maître entre en conflit
         avec les requêtes sur le serveur standby qui utilisent ce tablespace
         pour les fichiers temporaires.
        </para>
       </listitem>
       <listitem>
        <para>
         La suppression d'une base de données sur le serveur maître entre en
         conflit avec les sessions connectées sur cette base de données sur
         le serveur en attente.
        </para>
       </listitem>
       <listitem>
        <para>
         La copie d'un enregistrement nettoyé par un VACUUM entre en conflit
         avec les transactions sur le serveur en attente qui peuvent toujours
         <quote>voir</quote> au moins une des lignes à supprimer.
        </para>
       </listitem>
       <listitem>
        <para>
         La copie d'un enregistrement nettoyé par un VACUUM entre en conflit
         avec les requêtes accédant à la page cible sur le serveur en attente,
         qu'elles voient ou non les données à supprimer.
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    Sur le serveur maître, ces cas résultent en une attente
    supplémentaire&nbsp;; l'utilisateur peut choisir d'annuler une des actions
    en conflit. Néanmoins, sur le serveur en attente, il n'y a pas de choix
    possibles&nbsp;: l'action enregistrée dans les journaux de transactions
    est déjà survenue sur le serveur maître et le serveur en standby doit
    absolument réussir à l'appliquer. De plus, permettre que l'enregistrement
    de l'action attende indéfiniment pourrait avoir des effets fortement non
    désirables car le serveur en attente sera de plus en plus en retard par
    rapport au maître. Du coup, un mécanisme est fourni pour forcer
    l'annulation des requêtes sur le serveur en attente qui entreraient en
    conflit avec des enregistrements des journaux de transactions en attente.
   </para>

   <para>
    Voici un exemple de problème type&nbsp;: un administrateur exécute un
    <command>DROP TABLE</command> sur une table du serveur maître qui est
    actuellement utilisé dans des requêtes du serveur en attente. Il est clair
    que la requête ne peut pas continuer à s'exécuter si l'enregistrement
    dans les journaux de transactions, correspondant au <command>DROP
    TABLE</command> est appliqué sur le serveur en attente. Si cette situation
    survient sur le serveur maître, l'instruction <command>DROP TABLE</command>
    attendra jusqu'à ce que l'autre requête se termine. Par contre, quand le
    <command>DROP TABLE</command> est exécuté sur le serveur maître, ce dernier
    ne sait pas les requêtes en cours d'exécution sur le serveur en attente,
    donc il n'attendra pas la fin de l'exécution des requêtes sur le serveur
    en attente. L'enregistrement de cette modification dans les journaux de
    transactions arrivera au serveur en attente alors que la requête sur le
    serveur en attente est toujours en cours d'exécution, causant un conflit.
    Le serveur en attente doit soit retarder l'application des enregistrements
    des journaux de transactions (et tous ceux qui sont après aussi) soit
    annuler la requête en conflit, pour appliquer l'instruction <command>DROP
    TABLE</command>.
   </para>

   <para>
    Quand une requête en conflit est courte, il est généralement préférable
    d'attendre un peu pour l'application du journal de transactions. Mais un
    délai plus long n'est généralement pas souhaitable. Donc, le mécanisme
    d'annulation dans l'application des enregistrements de journaux de
    transactions dispose de deux paramètres, <xref
    linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/>, qui définissent le délai
    maximum autorisé pour appliquer les enregistrements. Les requêtes en
    conflit seront annulées si l'application des enregistrements prend plus de
    temps que celui défini. Il existe deux paramètres pour que des délais
    différents puissent être observés suivant le cas&nbsp;: lecture des
    enregistrements à partir d'un journal archivé (par exemple lors de la
    restauration initiale à partir d'une sauvegarde ou lors d'un
    <quote>rattrapage</quote> si le serveur en attente accumulait du retard
    par rapport au maître) et lecture des enregistrements à partir de la
    réplication en flux.
   </para>

   <para>
    Pour un serveur en attente dont le but principal est la haute-disponibilité,
    il est préférable de configurer des valeurs assez basses pour les
    paramètres de délai, de façon à ce que le serveur en attente ne soit pas
    trop en retard par rapport au serveur maître à cause des délais suivis à
    cause des requêtes exécutées sur le serveur en attente. Par contre, si
    le serveur en attente doit exécuter des requêtes longues, alors une valeur
    haute, voire infinie, du délai pourrait être préférable. Néanmoins, gardez
    en tête qu'une requête mettant du temps à s'exécuter pourrait empêcher
    les autres requêtes de voir les modifications récentes sur le serveur
    primaire si elle retarde l'application des enregistrements de journaux
    de transactions.
   </para>

   <para>
    Une fois que le délai spécifié par
    <varname>max_standby_archive_delay</varname> ou
    <varname>max_standby_streaming_delay</varname> a été dépassé, toutes les
    requêtes en conflit seront annulées. Ceci résulte habituellement en une
    erreur d'annulation, bien que certains cas, comme un <command>DROP
    DATABASE</command>, peuvent occassionner l'arrêt complet de la connexion.
    De plus, si le conflit intervient sur un verrou détenu par une transaction
    en attente, la session en conflit sera terminée (ce comportement pourrait
    changer dans le futur).
   </para>

   <para>
    Les requêtes annulées peuvent être ré-exécutées immédiatement (après avoir
    commencé une nouvelle transaction, bien sûr). Comme l'annulation des
    requêtes dépend de la nature des enregistrements dans le journal de
    transactions, une requête annulée pourrait très bien réussir si elle est
    de nouveau exécutée.
   </para>

   <para>
    Gardez en tête que les paramètres de délai sont comparés au temps passé
    depuis que la donnée du journal de transactions a été reçue par le serveur
    en attente. Du coup, la période de grâce accordée aux requêtes n'est jamais
    supérieur au paramètre de délai, et peut être considérablement inférieur
    si le serveur en attente est déjà en retard suite à l'attente de la fin
    de l'exécution de requêtes précédentes ou suite à son impossibilité de
    conserver le rythme d'une grosse mise à jour.
   </para>

   <para>
    La raison la plus fréquente des conflits entre les requêtes en lecture
    seule et le rejeu des journaux de transactions est le <quote>nettoyage
    avancé</quote>. Habituellement, <productname>PostgreSQL</productname>
    permet le nettoyage des anciennes versions de lignes quand aucune
    transaction ne peut les voir pour s'assurer du respect des règles de MVCC.
    Néanmoins, cette règle peut seulement s'appliquer sur les transactions
    exécutées sur le serveur maître. Donc il est possible que le nettoyage
    effectué sur le maître supprime des versions de lignes toujours visibles
    sur une transaction exécutée sur le serveur en attente.
   </para>

   <para>
    Les utilisateurs expérimentés peuvent noter que le nettoyage des versions
    de ligne ainsi que le gel des versions de ligne peuvent potentiellement
    avoir un conflit avec les requêtes exécutées sur le serveur en attente.
    L'exécution d'un <command>VACUUM FREEZE</command> manuel a de grandes
    chances de causer des conflits, y compris sur les tables sans lignes mises
    à jour ou supprimées.
   </para>

   <para>
    Les utilisateurs doivent s'attendre à ce que les tables fréquemment mises
    à jour sur le serveur primaire seront aussi fréquemment la cause de
    requêtes annulées sur le serveur en attente. Dans un tel cas, le
    paramétrage d'une valeur finie pour
    <varname>max_standby_archive_delay</varname> ou
    <varname>max_standby_streaming_delay</varname> peut être considéré comme
    similaire à la configuration de <varname>statement_timeout</varname>.
   </para>

   <para>
    Si le nombre d'annulations de requêtes sur le serveur en attente est
    jugé inadmissible, quelques solutions existent. La première option est de définir la variable
	<varname>hot_standby_feedback</varname> qui permet d'empêcher les conflits liés au nettoyage
	opéré par la commande <command>VACUUM</command> en lui interdisant de nettoyer les lignes récemment supprimées. Si
    vous le faites, vous devez noter que cela retardera le nettoyage des
    versions de lignes mortes sur le serveur maître, ce qui pourrait résulter
    en une fragmentation non désirée de la table. Néanmoins, cette situation
    ne sera pas meilleure si les requêtes du serveur en attente s'exécutaient
    directement sur le serveur maître. Vous avez toujours le bénéfice de
    l'exécution sur un serveur distant.
    <varname>max_standby_archive_delay</varname> doit être configuré avec une
    valeur suffisamment large dans ce cas car les journaux de transactions
    en retard pourraient déjà contenir des entrées en conflit avec les requêtes
    sur le serveur en attente.
   </para>

   <para>
    Une autre option revient à augmenter <xref
    linkend="guc-vacuum-defer-cleanup-age"/> sur le serveur maître, pour que
    les lignes mortes ne soient pas nettoyées aussi rapidement que d'habitude.
    Cela donnera plus de temps aux requêtes pour s'exécuter avant d'être
    annulées sur le serveur en attente, sans voir à configurer une valeur
    importante de <varname>max_standby_streaming_delay</varname>. Néanmoins,
    il est difficile de garantir une fenêtre spécifique de temps d'exécution
    avec cette approche car <varname>vacuum_defer_cleanup_age</varname> est
    mesuré en nombre de transactions sur le serveur maître.
   </para>

   <para>
    Le nombre de requêtes annulées et le motif de cette annulation peut être visualisé avec
	la vue système <structname>pg_stat_database_conflicts</structname> sur le serveur de 
	standby. La vue système <structname>pg_stat_database</structname> contient aussi
	des informations synthétiques sur ce sujet.
   </para>
  </sect2>

  <sect2 id="hot-standby-admin">
   <title>Aperçu pour l'administrateur</title>

   <para>
    Si <varname>hot_standby</varname> est positionné à <literal>on</literal> dans
    <filename>postgresql.conf</filename>  et qu'une fichier <filename>recovery.conf</filename>
    est présent, le serveur fonctionnera en mode Hot Standby.
    Toutefois, il pourrait s'écouler du temps avant que les connections en
    Hot Standby soient autorisées, parce que le serveur n'acceptera pas de connexions tant
    que la récupération n'aura pas atteint un point garantissant un état cohérent permettant
    aux requêtes de s'exécuter. Pendant cette période, les clients qui tentent de se connecter
    seront rejetés avec un message d'erreur.
    Pour confirmer que le serveur a démarré, vous pouvez soit tenter de vous connecter en
    boucle, ou rechercher ces messages dans les journaux du serveur:

<programlisting>
LOG:  entering standby mode

... puis, plus loin ...

LOG:  consistent recovery state reached
LOG:  database system is ready to accept read only connections
</programlisting>

    L'information sur la cohérence est enregistrée une fois par checkpoint sur le primaire.
    Il n'est pas possible d'activer le hot standby si on lit des WAL générés durant
    une période pendant laquelle <varname>wal_level</varname> n'était pas positionné
    à <literal>hot_standby</literal> sur le primaire. L'arrivée à un état cohérent
    peut aussi être retardée si ces deux conditions se présentent:

      <itemizedlist>
       <listitem>
        <para>
         Une transaction en écriture a plus de 64 sous-transactions
        </para>
       </listitem>
       <listitem>
        <para>
         Des transactions en écriture ont une durée très importante
        </para>
       </listitem>
      </itemizedlist>

    Si vous effectuez du log shipping par fichier ("warm standby"), vous pourriez
    devoir attendre jusqu'à l'arrivée du prochain fichier de WAL, ce qui pourrait
    être aussi long que le paramètre <varname>archive_timeout</varname> du primaire.
   </para>

   <para>
    Certains paramètres sur le standby vont devoir être revus si ils ont été modifiés
    sur le primaire. Pour ces paramètres, la valeur sur le standby devra
    être égale ou supérieure à celle du primaire. Si ces paramètres ne sont pas
    suffisamment élevés le standby refusera de démarrer. Il est tout à fait possible
    de fournir de nouvelles valeurs plus élevées et de redémarrer le serveur pour reprendre
    la récupération. Ces paramètres sont les suivants:

      <itemizedlist>
       <listitem>
        <para>
         <varname>max_connections</varname>
        </para>
       </listitem>
       <listitem>
        <para>
         <varname>max_prepared_transactions</varname>
        </para>
       </listitem>
       <listitem>
        <para>
         <varname>max_locks_per_transaction</varname>
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    Il est important que l'administrateur sélectionne le paramétrage approprié
    pour <xref linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/>. Le meilleur choix varie les
    priorités. Par exemple, si le serveur a comme tâche principale d'être un
    serveur de haute-disponibilité, alors il est préférable d'avoir une
    configuration assez basse, voire à zéro, de ces paramètres. Si le serveur
    en attente est utilisé comme serveur supplémentaire pour des requêtes du
    type décisionnel, il sera acceptable de mettre les paramètres de délai à
    des valeurs allant jusqu'à plusieurs heures, voire même -1 (cette valeur
    signifiant qu'il est possible d'attendre que les requêtes se terminent
    d'elles-même).
   </para>

   <para>
    Les "hint bits" (bits d'indices) écrits sur le primaire ne sont pas journalisés en WAL,
    il est donc probable que les hint bits soient réécrits sur le standby. Ainsi,
    le serveur de standby fera toujours des écritures disques même si tous les utilisateurs
    sont en lecture seule; aucun changement ne se produira sur les données elles mêmes.
    Les utilisateurs écriront toujours les fichiers temporaires pour les gros tris et
    re-génèreront les fichiers d'information relcache, il n'y a donc pas de morceau de la base
    qui soit réellement en lecture seule en mode hot standby.
    Notez aussi que les écritures dans des bases distantes en utilisant le module
    <application>dblink</application> , et d'autres opération en dehors de la base s'appuyant sur
    des fonctions PL seront toujours possibles, même si la transaction est en lecture seule localement.
   </para>

   <para>
    Les types suivants de commandes administratives ne sont pas acceptées
    durant le mode de récupération:

      <itemizedlist>
       <listitem>
        <para>
         Langage de Définition de Données (LDD ou DDL) - comme <command>CREATE INDEX</command>
        </para>
       </listitem>
       <listitem>
        <para>
         Privilège et possession - <command>GRANT</command>, <command>REVOKE</command>,
         <command>REASSIGN</command>
        </para>
       </listitem>
       <listitem>
        <para>
         Commandes de maintenance - <command>ANALYZE</command>, <command>VACUUM</command>,
         <command>CLUSTER</command>, <command>REINDEX</command>
        </para>
       </listitem>
      </itemizedlist>
   </para>

   <para>
    Notez encore une fois que certaines de ces commandes sont en fait
    autorisées durant les transactions en "lecture seule" sur le primaire.
   </para>

   <para>
    Par conséquent, vous ne pouvez pas créer d'index supplémentaires qui existeraient
    uniquement sur le standby, ni des statistiques qui n'existeraient que sur le standby.
    Si ces commandes administratives sont nécessaires, elles doivent être exécutées
    sur le primaire, et ces modifications se propageront à terme au standby.
   </para>

   <para>
    <function>pg_cancel_backend()</function> et <function>pg_terminate_backend()</function>
    fonctionneront sur les processus utilisateurs, mais pas sur
    les processus de démarrage, qui effectuent la récupération. <structname>pg_stat_activity</structname> 
    ne montre pas d'entrée pour le processus de démarrage, et les transactions de récupération
    ne sont pas affichées comme actives. Ainsi, <structname>pg_prepared_xacts</structname> est toujours
    vide durant la récupération. Si vous voulez traiter des transactions préparées douteuses,
    interrogez <structname>pg_prepared_xacts</structname>  sur le primaire, et exécutez les commandes
    pour résoudre le problème à cet endroit.
   </para>

   <para>
    <structname>pg_locks</structname> affichera les verrous possédés par les processus,
    comme en temps normal. <structname>pg_locks</structname> affiche aussi une transaction
    virtuelle gérée par le processus de démarrage qui possède tous les
    <literal>AccessExclusiveLocks</literal> possédés par les transactions rejouées par la récupération.
    Notez que le processus de démarrage n'acquiert pas de verrou pour effectuer les modifications à
    la base, et que par conséquent les verrous autre que <literal>AccessExclusiveLocks</literal> 
    ne sont pas visibles dans <structname>pg_locks</structname> pour le processus de démarrage;
    ils sont simplement censés exister.
   </para>

   <para>
    Le plugin <productname>Nagios</productname> <productname>check_pgsql</productname> fonctionnera,
    parce que les informations simples qu'il vérifie existent.
    Le script de supervision <productname>check_postgres</productname> fonctionnera aussi,
    même si certaines valeurs retournées pourraient être différentes ou sujettes à confusion.
    Par exemple, la date de dernier vacuum ne sera pas mise à jour, puisqu'aucun vacuum ne se déclenche
    sur le standby. Les vacuums s'exécutant sur le primaire envoient toujours leurs modifications
    au standby.
   </para>

   <para>
    Les options de contrôle des fichiers de WAL ne fonctionneront pas durant la récupération,
    comme <function>pg_start_backup</function>, <function>pg_switch_xlog</function>, etc...
   </para>

   <para>
    Les modules à chargement dynamique fonctionnent, comme <structname>pg_stat_statements</structname>.
   </para>

   <para>
    Les verrous consultatifs fonctionnent normalement durant la récupération,
    y compris en ce qui concerne la détection des verrous mortels (deadlocks).
    Notez que les verrous consultatifs ne sont jamais tracés dans les WAL, il est
    donc impossible pour un verrou consultatif sur le primaire ou le standby
    d'être en conflit avec la ré-application des WAL. Pas plus qu'il n'est
    possible d'acquérir un verrou consultatif sur le primaire et que celui-ci
    initie un verrou consultatif similaire sur le standby. Les verrous consultatifs
    n'ont de sens que sur le serveur sur lequel ils sont acquis.
   </para>

   <para>
    Les systèmes de réplications à base de triggers tels que <productname>Slony</productname>,
    <productname>Londiste</productname> et <productname>Bucardo</productname>
    ne fonctionneront pas sur le standby du tout, même s'ils fonctionneront sans problème
    sur le serveur primaire tant que les modifications ne sont pas envoyées sur le serveur standby
    pour y être appliquées. Le rejeu de WAL n'est pas à base de triggers, vous ne pouvez
    donc pas utiliser le standby comme relai vers un système qui aurait besoin d'écritures supplémentaires
    ou utilise des triggers.
   </para>

   <para>
    Il n'est pas possible d'assigner de nouveaux OID, bien que des générateurs d' <acronym>UUID</acronym> 
    puissent tout de même fonctionner, tant qu'ils n'ont pas besoin d'écrire un nouveau statut dans
    la base.
   </para>

   <para>
    À l'heure actuelle, la création de table temporaire n'est pas autorisée durant les
    transactions en lecture seule, certains scripts existants pourraient donc
    ne pas fonctionner correctement. Cette restriction pourrait être levée dans une
    version ultérieure. Il s'agit à la fois d'un problème de respect des standards
    et un problème technique.
   </para>

   <para>
    <command>DROP TABLESPACE</command> ne peut réussir que si le tablespace est vide.
    Certains utilisateurs pourraient utiliser de façon active le tablespace via leur
    paramètre <varname>temp_tablespaces</varname>. S'il y a des fichiers temporaires
    dans le tablespace, toutes les requêtes actives sont annulées pour s'assurer que les
    fichiers temporaires sont supprimés, afin de supprimer le tablespace et de continuer
    l'application des WAL.
   </para>

   <para>
    Exécuter <command>DROP DATABASE</command> ou <command>ALTER DATABASE ...
    SET TABLESPACE</command> sur
    le serveur maître générera un enregistrement dans les journaux de
    transactions qui causera la déconnexion de tous les utilisateurs
    actuellement connectés à cette base de données. Cette action survient
    immédiatement, quelque soit la valeur du paramètre
    <varname>max_standby_streaming_delay</varname>. Notez que
    <command>ALTER DATABASE ... RENAME</command> ne déconnecte pas les
    utilisateurs qui, dans la plupart des cas, ne s'en apercevront pas. Cela
    peut néanmoins confondre un programme qui dépendrait du nom de la base.
   </para>

   <para>
    En fonctionnement normal (pas en récupération), si vous exécutez 
    <command>DROP USER</command> ou <command>DROP ROLE</command>
    pour un rôle ayant le privilège LOGIN alors que cet utilisateur est toujours
    connecté alors rien ne se produit pour cet utilisateur connecté - il reste connecté. L'utilisateur
    ne peut toutefois pas se reconnecter. Ce comportement est le même en récupération, un
    <command>DROP USER</command> sur le primaire ne déconnecte donc pas cet utilisateur sur le standby.
   </para>

   <para>
    Le collecteur de statistiques est actif durant la récupération. Tous les parcours,
    lectures, utilisations de blocs et d'index, etc... seront enregistrés normalement
    sur le standby. Les actions rejouées ne dupliqueront pas leur effets sur le primaire,
    l'application d'insertions n'incrémentera pas la colonne Inserts de pg_stat_user_tables.
    Le fichier de statistiques est effacé au démarrage de la récupération, les statistiques
    du primaire et du standby différeront donc; c'est vu comme une fonctionnalité, pas un bug.
   </para>

   <para>
    Autovacuum n'est pas actif durant la récupération, il démarrera normalement
    à la fin de la récupération.
   </para>

   <para>
    Le processus d'écriture en arrière plan (background writer) est actif durant
    la récupération et effectuera les restartpoints (points de reprise)
    (similaires aux points de synchronisation ou checkpoints sur le primaire) et
    les activités normales de nettoyage de blocs. Ceci peut inclure la mise à jour
    des information de hint bit des données du serveur de standby.
    La commande <command>CHECKPOINT</command>  est acceptée pendant la récupération,
    bien qu'elle déclenche un restartpoint et non un checkpoint.
   </para>
  </sect2>

  <sect2 id="hot-standby-parameters">
   <title>Référence des paramètres de Hot Standby</title>

   <para>
    De nombreux paramètres ont été mentionnés ci-dessus dans 
    <xref linkend="hot-standby-conflict"/>
    et <xref linkend="hot-standby-admin"/>.
   </para>

   <para>
    Sur le primaire, les paramètres <xref linkend="guc-wal-level"/> et
    <xref linkend="guc-vacuum-defer-cleanup-age"/> peuvent être utilisés.
    <xref linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/> n'ont aucun effet sur le primaire.
   </para>

   <para>
    Sur le serveur en attente, les paramètres <xref linkend="guc-hot-standby"/>,
    <xref linkend="guc-max-standby-archive-delay"/> et
    <xref linkend="guc-max-standby-streaming-delay"/> peuvent être utilisés.
    <xref linkend="guc-vacuum-defer-cleanup-age"/> n'a pas d'effet tant que
    le serveur reste dans le mode standby, mais deviendra important quand le
    serveur en attente deviendra un serveur maître.
   </para>
  </sect2>

  <sect2 id="hot-standby-caveats">
   <title>Avertissements</title>

   <para>
    Il y a plusieurs limitations de Hot Standby.
    Elles peuvent et seront probablement résolues dans des versions ultérieures:

  <itemizedlist>
   <listitem>
    <para>
     Les opérations sur les index hash ne sont pas écrits dans la WAL à l'heure
     actuelle, la récupération ne mettra donc pas ces index à jour.
    </para>
   </listitem>
   <listitem>
    <para>
     Une connaissance complète des transactions en cours d'exécution est nécessaire
     avant de pouvoir déclencher des instantanés. Des transactions utilisant un
     grand nombre de sous-transactions (à l'heure actuelle plus de 64) retarderont
     le démarrage des connexions en lecture seule jusqu'à complétion de la plus
     longue transaction en écriture. Si cette situation se produit, des messages
     explicatifs seront envoyés dans la trace du serveur.
    </para>
   </listitem>
   <listitem>
    <para>
     Des points de démarrage valides pour les requêtes de standby sont générés
     à chaque checkpoint sur le maître. Si le standby est éteint alors
     que le maître est déjà éteint, il est tout à fait possible ne pas pouvoir
     repasser en Hot Standby tant que le primaire n'aura pas été redémarré, afin
     qu'il génère de nouveaux points de démarrage dans les journaux WAL. Cette situation
     n'est pas un problème dans la plupart des situations où cela pourrait se produire.
     Généralement, si le primaire est éteint et plus disponible, c'est probablement
     en raison d'un problème sérieux qui va de toutes façons forcer la conversion
     du standby en primaire. Et dans des situations où le primaire est éteint
     intentionnellement, la procédure standard est de promouvoir le maître.
    </para>
   </listitem>
   <listitem>
    <para>
     À la fin de la récupération, les <literal>AccessExclusiveLocks</literal> possédés
     par des transactions préparées nécessiteront deux fois le nombre d'entrées normal dans la
     table de verrous. Si vous pensez soit exécuter un grand nombre de transactions préparées
     prenant des <literal>AccessExclusiveLocks</literal>, ou une grosse transaction prenant
     beaucoup de <literal>AccessExclusiveLocks</literal>, il est conseillé d'augmenter la valeur
     de <varname>max_locks_per_transaction</varname>, peut-être jusqu'à une valeur double
     de celle du serveur primaire. Vous n'avez pas besoin de prendre ceci en compte
     si votre paramètre <varname>max_prepared_transactions</varname> est 0.
    </para>
   </listitem>
   <listitem>
    <para>
	 Il n'est pas encore possible de passer une transaction en mode d'isolation sérialisable 
	 tout en supportant le hot standby (voir <xref linkend="xact-serializable"/> et 
	 <xref linkend="serializable-consistency"/> pour plus de détails).
	 Une tentative de modification du niveau d'isolation d'une transaction à sérialisable
	 en hot standby générera une erreur.
    </para>
   </listitem>
  </itemizedlist>
   </para>
  </sect2>

 </sect1>

</chapter>
