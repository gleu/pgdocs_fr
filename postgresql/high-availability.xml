<?xml version="1.0" encoding="UTF-8"?>
<chapter id="high-availability">
 <title>Haute disponibilité, répartition de charge et réplication</title>

 <indexterm><primary>haute disponibilité</primary></indexterm>
 <indexterm><primary>failover</primary></indexterm>
 <indexterm><primary>réplication</primary></indexterm>
 <indexterm><primary>répartition de charge</primary></indexterm>
 <indexterm><primary>clustering</primary></indexterm>
 <indexterm><primary>partitionnement de données</primary></indexterm>

 <para>
  Des serveurs de bases de données peuvent travailler ensemble pour permettre
  à un serveur secondaire (<foreignphrase>standby</foreignphrase> dans la
  version originale) de prendre rapidement la main si le serveur principal
  échoue (haute disponibilité, ou <foreignphrase>high
  availability</foreignphrase>), ou pour permettre à plusieurs serveurs de
  servir les mêmes données (répartition de charge, ou <foreignphrase>load
  balancing</foreignphrase>). Idéalement, les serveurs de bases de données
  peuvent travailler ensemble sans jointure.
 </para>

 <para>
  Il est aisé de faire coopérer des serveurs web qui traitent des pages web statiques
  en répartissant la charge des requêtes web sur plusieurs
  machines. Dans les faits, les serveurs de bases de données en lecture seule peuvent
  également coopérer facilement. Malheureusement, la plupart des
  serveurs de bases de données traitent des requêtes de lecture/écriture et,
  de ce fait, collaborent plus difficilement. En effet, alors qu'il suffit de
  placer une seule fois les données en lecture seule sur chaque serveur, une
  écriture sur n'importe quel serveur doit, elle, être propagée à tous les
  serveurs afin que les lectures suivantes sur ces serveurs renvoient des résultats
  cohérents.
 </para>

 <para>
  Ce problème de synchronisation représente la difficulté fondamentale à la
  collaboration entre serveurs. Comme la solution au problème de
  synchronisation n'est pas unique pour tous les cas pratiques, plusieurs
  solutions co-existent. Chacune répond de façon différente et minimise
  cet impact au regard d'une charge spécifique.
 </para>

 <para>
  Certaines solutions gèrent la synchronisation en autorisant les
  modifications des données sur un seul serveur. Les serveurs qui peuvent
  modifier les données sont appelés serveur en lecture/écriture,
  <firstterm>primaire</firstterm> ou serveur <firstterm>primaire</firstterm>.
  Les serveurs qui suivent les modifications du primaire sont appelés des
  serveurs <firstterm>standby</firstterm> ou des serveurs
  <firstterm>secondaires</firstterm>. Un serveur secondaire auquel on ne peut
  pas se connecter tant qu'il n'a pas été promu en serveur primaire est
  appelé un <firstterm>warm standby</firstterm>, et un serveur qui peut
  accepter des connexions et répondre à des requêtes en lecture seule est
  appelé un <firstterm>hot standby</firstterm>.
 </para>

 <para>
  Certaines solutions sont synchrones, ce qui signifie qu'une transaction de
  modification de données n'est pas considérée valide tant que tous les
  serveurs n'ont pas validé la transaction. Ceci garantit qu'un
  <foreignphrase>failover</foreignphrase> ne perd pas de données et que tous
  les serveurs en répartition de charge retournent des résultats cohérents, quel
  que soit le serveur interrogé. Au contraire, les solutions asynchrones
  autorisent un délai entre la validation et sa propagation aux
  autres serveurs. Cette solution implique une éventuelle perte de transactions
  lors de la bascule sur un serveur de sauvegarde, ou l'envoi de données
  obsolètes par les serveurs à charge répartie. La communication asynchrone est
  utilisée lorsque la version synchrone est trop lente.
 </para>

 <para>
  Les solutions peuvent aussi être catégorisées par leur granularité. Certaines
  ne gèrent que la totalité d'un serveur de bases de données alors que
  d'autres autorisent un contrôle par table ou par base.
 </para>

 <para>
  Il importe de considérer les performances dans tout choix. Il y
  a généralement un compromis à trouver entre les fonctionnalités et les
  performances. Par exemple, une solution complètement synchrone sur un réseau
  lent peut diviser les performances par plus de deux, alors qu'une
  solution asynchrone peut n'avoir qu'un impact minimal sur les performances.
 </para>

 <para>
  Le reste de cette section souligne différentes solutions de
  <foreignphrase>failover</foreignphrase>, de réplication et de répartition de
  charge.
 </para>

 <sect1 id="different-replication-solutions">
  <title>Comparaison de différentes solutions</title>

  <variablelist>

   <varlistentry>
    <term><foreignphrase>Failover</foreignphrase> sur disque partagé</term>
    <listitem>

     <para>
      Le <foreignphrase>failover</foreignphrase> (ou bascule sur incident)
      sur disque partagé élimine la surcharge de synchronisation par
      l'existence d'une seule copie de la base de données. Il utilise un
      seul ensemble de disques partagé par plusieurs serveurs. Si le serveur
      principal échoue, le serveur en attente
      est capable de monter et démarrer la base comme s'il récupérait d'un
      arrêt brutal. Cela permet un <foreignphrase>failover</foreignphrase>
      rapide sans perte de données.
     </para>

     <para>
      La fonctionnalité de matériel partagé est commune aux périphériques de
      stockage en réseau. Il est également possible d'utiliser un système de
      fichiers réseau bien qu'il faille porter une grande attention au système de
      fichiers pour s'assurer qu'il a un comportement <acronym>POSIX</acronym>
      complet (voir <xref linkend="creating-cluster-nfs"/>). Cette méthode
      comporte une limitation significative&nbsp;: si les disques ont un
      problème ou sont corrompus, le serveur primaire et le serveur en attente sont tous
      les deux non fonctionnels. Un autre problème est que le serveur en attente
      ne devra jamais accéder au stockage partagé tant que le serveur principal
      est en cours d'exécution.
     </para>

    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Réplication de système de fichiers (périphérique bloc)</term>
    <listitem>

     <para>
      Il est aussi possible d'utiliser cette fonctionnalité d'une autre façon
      avec une réplication du système de fichiers, où toutes les modifications
      d'un système de fichiers sont renvoyées sur un système de fichiers situé
      sur un autre ordinateur. La seule restriction est que ce miroir doit être
      construit de telle sorte que le serveur en attente dispose d'une
      version cohérente du système de fichiers &mdash; spécifiquement, les
      écritures sur le serveur en attente doivent être réalisées dans le même
      ordre que celles sur le primaire. <productname>DRBD</productname> est une
      solution populaire de réplication de systèmes de fichiers pour Linux.
     </para>

     <!--
https://forge.continuent.org/pipermail/sequoia/2006-November/004070.html
La technologie Oracle RAC est une approche par disques partagés et renvoie
aux autres nœuds uniquement les annulations de niveau cache mais pas
réellement au niveau des données (physiques).
Puisque les disques sont partagés, les données sont validées une seule
fois en s'appuyant sur un protocole de verrouillage distribué, de façon à
ce que les nœuds s'accordent dans un système transactionnel sérialisable.


-->

    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Envoi des journaux de transactions</term>
    <listitem>

     <para>
      Les serveurs secondaires, <foreignphrase>warm et hot
      standby</foreignphrase>, (voir <xref linkend="warm-standby"/>) peuvent
      conserver leur cohérence en lisant un flux d'enregistrements de
      <acronym>WAL</acronym>. Si le serveur principal échoue, le serveur
      secondaire contient pratiquement toutes les données du serveur
      principal et peut rapidement devenir le nouveau serveur primaire. Ça
      peut être synchrone mais ça ne peut se faire que pour le serveur de
      bases complet.
     </para>
     <para>
      Un serveur secondaire peut être implémenté en utilisant la recopie de
      journaux par fichier(<xref linkend="warm-standby"/>)  ou la
      <foreignphrase>streaming replication</foreignphrase> (réplication en
      continu ou en flux, voir <xref linkend="streaming-replication"/>), ou
      une combinaison des deux. Pour des informations sur le <firstterm>hot
      standby</firstterm>, voyez <xref linkend="hot-standby"/>.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Réplication logique</term>
    <listitem>
     <para>
      La réplication logique autorise un serveur de bases de données à envoyer
      un flux de modifications de données à un autre serveur. La réplication
      logique de <productname>PostgreSQL</productname> construit un flux de
      modifications logiques de données à partir des journaux de transactions.
      La réplication logique permet la réplication des modifications de
      données de tables individuelles. La réplication logique ne requiert pas
      qu'un serveur particulier soit désigné comme serveur primaire ou
      secondaire, mais autorise le flux de données dans plusieurs directions.
      Pour plus d'informations sur la réplication logique, voir <xref
      linkend="logical-replication"/>. Au travers de l'interface de décodage
      logique (<xref linkend="logicaldecoding"/>), les extensions tierces
      peuvent aussi fournir des fonctionnalités similaires.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Réplication primaire/secondaire basé sur des triggers</term>
    <listitem>

     <para>
      Une architecture de réplication basée sur des triggers canalise
      habituellement les requêtes de modification de données vers un serveur
      primaire précis. Opérant table par table, le serveur primaire envoie alors
      les données modifiées (généralement) de façon asynchrone vers les serveurs
      secondaires. Les serveurs secondaires peuvent répondre aux requêtes qu'ils
      reçoivent alors que le serveur primaire est fonctionnel. Ils peuvent
      parfois permettre quelques modifications de données localement ou même une
      activité en écriture. Cette forme de réplication est souvent utilisée pour
      gérer de grosses requêtes analytiques ou de type
      <foreignphrase>Data Warehouse</foreignphrase>.
     </para>

     <para>
      <productname>Slony-I</productname> est un exemple de ce type de
      réplication, avec une granularité par
      table et un support des secondaires multiples. Comme il met à jour le serveur
      secondaire de façon asynchrone (par lots), il existe une possibilité de perte
      de données pendant un <foreignphrase>failover</foreignphrase>.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term><foreignphrase>Middleware</foreignphrase> de réplication basé sur le
     SQL</term>
    <listitem>

     <para>
      Avec les <foreignphrase>middleware</foreignphrase> de réplication basés
      sur le SQL, un programme intercepte chaque requête SQL et
      l'envoie à un ou tous les serveurs. Chaque serveur opère indépendamment.
      Les requêtes en lecture/écriture doivent être envoyées à tous les
      serveurs pour que chaque serveur reçoive les modifications. Les
      requêtes en lecture seule ne peuvent être envoyées qu'à un seul
      serveur, ce qui permet de distribuer la charge de lecture.
     </para>

     <para>
      Si les requêtes sont envoyées sans modification, les fonctions comme
      <function>random()</function>, <function>CURRENT_TIMESTAMP</function> ainsi
      que les séquences ont des valeurs différentes sur les différents serveurs.
      Cela survient parce que chaque serveur opère indépendamment alors que
      les requêtes SQL sont diffusées (et non les données
      modifiées). Si cette solution est inacceptable, le
      <foreignphrase>middleware</foreignphrase> ou l'application doivent
      déterminer ces valeurs à partir d'une seule source, et les utiliser dans
      les requêtes d'écriture. Il est impératif que
      toute transaction soit validée ou annulée sur tous les serveurs,
      éventuellement par validation en deux phases (<xref
      linkend="sql-prepare-transaction"/> et <xref linkend="sql-commit-prepared"/>).
      <productname>Pgpool-II</productname> et <productname>Continuent
      Tungsten</productname> sont des exemples de ce type de réplication.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Réplication asynchrone multi-primaires</term>
    <listitem>

     <para>
      Pour les serveurs qui ne sont pas connectés en permanence ou qui ont des
      liens de communication lents, comme les ordinateurs portables ou les
      serveurs distants, conserver la cohérence des données
      entre les serveurs est un challenge. L'utilisation de la réplication asynchrone
      multi-primaires permet à chaque serveur de fonctionner indépendamment. Il
      communique alors périodiquement avec les autres serveurs pour identifier les transactions
      conflictuelles. La gestion des conflits est alors confiée aux utilisateurs
      ou à un système de règles de résolution.
      Bucardo est un exemple de ce type de réplication.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Réplication synchrone multi-primaires</term>
    <listitem>

     <para>
      Dans les réplications synchrones multi-primaires, tous les serveurs acceptent
      les requêtes en écriture. Les données modifiées sont transmises
      du serveur d'origine à tous les autres serveurs avant toute validation de
      transaction.
     </para>
     <para>
      Une activité importante en écriture peut être la cause d'un verrouillage
      excessif et de délai dans la validation des transactions, ce qui peut
      conduire à un effondrement des performances. Dans les faits, les
      performances en écriture sont souvent pis que celles d'un simple
      serveur.
     </para>
     <para>
      Tous les serveurs acceptent les requêtes en lecture.
     </para>
     <para>
      Certaines implantations utilisent les disques partagés pour réduire la surcharge
      de communication.
     </para>
     <para>
      Les performances de la réplication synchrone multi-primaires sont meilleures lorsque
      les opérations de lecture représentent l'essentiel de la charge, alors que
      son gros avantage est l'acceptation des requêtes d'écriture par tous les
      serveurs &mdash;
      il n'est pas nécessaire de répartir la charge entre les serveurs
      primaires et secondaires et, parce que les modifications de données sont envoyées
      d'un serveur à l'autre, les fonctions non déterministes, comme
      <function>random()</function>, ne posent aucun problème.
     </para>

     <para>
      <productname>PostgreSQL</productname> n'offre pas ce type de réplication,
      mais la validation en deux phases de <productname>PostgreSQL</productname>
      (<xref linkend="sql-prepare-transaction"/> et <xref linkend="sql-commit-prepared"/>)
      autorise son intégration dans une application ou un
      <foreignphrase>middleware</foreignphrase>.
     </para>
    </listitem>
   </varlistentry>

  </variablelist>

  <para>
   La <xref linkend="high-availability-matrix"/> résume les
   possibilités des différentes solutions listées plus-haut.
  </para>

  <table id="high-availability-matrix">
   <title>Matrice de fonctionnalités&nbsp;: haute disponibilité, répartition de
    charge et réplication</title>
   <tgroup cols="9">
    <colspec colname="col1" colwidth="1.1*"/>
    <colspec colname="col2" colwidth="1*"/>
    <colspec colname="col3" colwidth="1*"/>
    <colspec colname="col4" colwidth="1*"/>
    <colspec colname="col5" colwidth="1*"/>
    <colspec colname="col6" colwidth="1*"/>
    <colspec colname="col7" colwidth="1*"/>
    <colspec colname="col8" colwidth="1*"/>
    <colspec colname="col9" colwidth="1*"/>
    <thead>
     <row>
      <entry>Fonctionnalité</entry>
      <entry>Disques partagés</entry>
      <entry>Répl. par système de fichiers</entry>
      <entry>Envoi des journaux de transactions</entry>
      <entry>Répl. logique</entry>
      <entry>Répl. par triggers</entry>
      <entry><foreignphrase>Middleware</foreignphrase> de Répl. SQL</entry>
      <entry>Répl. asynch. MM</entry>
      <entry>Répl. synch. MM</entry>
     </row>
    </thead>

    <tbody>

     <row>
      <entry>Exemple populaires</entry>
      <entry align="center">NAS</entry>
      <entry align="center">DRBD</entry>
      <entry align="center">répl. en flux interne</entry>
      <entry align="center">répl. logique interne, pglogical</entry>
      <entry align="center">Londiste, Slony</entry>
      <entry align="center">pgpool-II</entry>
      <entry align="center">Bucardo</entry>
      <entry align="center"></entry>
     </row>

     <row>
      <entry>Méthode de comm.</entry>
      <entry align="center">Disque partagé</entry>
      <entry align="center">Blocs disque</entry>
      <entry align="center">WAL</entry>
      <entry align="center">décodage logique</entry>
      <entry align="center">Lignes de tables</entry>
      <entry align="center">SQL</entry>
      <entry align="center">Lignes de tables</entry>
      <entry align="center">Lignes de tables et verrous de ligne</entry>
     </row>

     <row>
      <entry>Ne requiert aucun matériel spécial</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
     </row>

     <row>
      <entry>Autorise plusieurs serveurs primaires</entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
     </row>

     <row>
      <entry>Pas de surcharge sur le serveur primaire</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
     </row>

     <row>
      <entry>Pas d'attente entre serveurs</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">with sync off</entry>
      <entry align="center">with sync off</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
     </row>

     <row>
      <entry>Pas de perte de données en cas de panne du primaire</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">with sync on</entry>
      <entry align="center">with sync on</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
     </row>

     <row>
      <entry>Les secondaires acceptent les requêtes en lecture seule</entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
      <entry align="center">avec un hot standby</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
     </row>

     <row>
      <entry>Granularité de niveau table</entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
     </row>

     <row>
      <entry>Ne nécessite pas de résolution de conflit</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <para>
   Certaines solutions n'entrent pas dans les catégories ci-dessus&nbsp;:
  </para>

  <variablelist>

   <varlistentry>
    <term>Partitionnement de données</term>
    <listitem>

     <para>
      Le partitionnement des données divise les tables en ensembles de données.
      Chaque ensemble ne peut être modifié que par un seul serveur. Les
      données peuvent ainsi être partitionnées par bureau, Londres et
      Paris, par exemple, avec un serveur dans chaque bureau. Si certaines
      requêtes doivent combiner des données de Londres et Paris, il est possible
      d'utiliser une application qui requête les deux serveurs ou d'implanter une
      réplication primaire/secondaire pour conserver sur chaque serveur une
      copie en lecture seule des données de l'autre bureau.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Exécution de requêtes en parallèle sur plusieurs serveurs</term>
    <listitem>

     <para>
      La plupart des solutions ci-dessus permettent à plusieurs serveurs de
      répondre à des requêtes multiples, mais aucune ne permet à une seule requête
      d'être exécutée sur plusieurs serveurs pour se terminer plus rapidement.
      Cette solution autorisent plusieurs serveurs à travailler ensemble sur une
      seule requête. Ceci s'accomplit habituellement en répartissant les données
      entre les serveurs, chaque serveur exécutant une partie de la
      requête pour renvoyer les résultats à un serveur central qui les combine
      et les renvoie à l'utilisateur. Cela peut également se faire en utilisant
      <productname>PL/Proxy</productname>.
     </para>
    </listitem>
   </varlistentry>

  </variablelist>

  <para>
   Il faut aussi noter que puisque <productname>PostgreSQL</productname>
   est un outil libre et facilement extensible, un certain nombre de sociétés
   se sont basées sur <productname>PostgreSQL</productname> pour créer leurs
   solutions propriétaires avec des possibilités spécifiques de
   <foreignphrase>failover</foreignphrase>, réplication ou répartition de charge.
   Cela ne sera toutefois pas abordé ici.
  </para>

 </sect1>

 <sect1 id="warm-standby">
  <title>Serveurs secondaires par transfert de journaux</title>

  <para>
   L'archivage en continu peut être utilisé pour créer une configuration
   de cluster en <firstterm>haute disponibilité</firstterm> (HA) avec un ou
   plusieurs <firstterm>serveurs secondaires</firstterm> prêts à prendre la main
   sur les opérations si le serveur primaire fait défaut. Cette fonctionnalité
   est généralement appelée
   <firstterm>warm standby</firstterm> ou <firstterm>log shipping</firstterm>.
  </para>

  <para>
   Les serveurs primaire et secondaire travaillent de concert pour fournir cette fonctionnalité,
   bien que les serveurs ne soient que faiblement couplés. Le serveur primaire opère
   en mode d'archivage en continu, tandis que le serveur secondaire opère en
   mode de récupération en continu, en lisant les fichiers WAL provenant du primaire. Aucune
   modification des tables de la base ne sont requises pour activer cette fonctionnalité,
   elle entraîne donc moins de travail d'administration par rapport à d'autres
   solutions de réplication. Cette configuration a aussi un impact relativement
   faible sur les performances du serveur primaire.
  </para>

  <para>
   Déplacer directement des enregistrements de WAL d'un serveur de bases de données à un autre
   est habituellement appelé log shipping. <productname>PostgreSQL</productname>
   implémente le log shipping par fichier, ce qui signifie que les enregistrements de WAL sont
   transférés un fichier (segment de WAL) à la fois. Les fichiers de WAL (16Mo) peuvent être
   transférés facilement et de façon peu coûteuse sur n'importe quelle distance, que ce soit sur un
   système adjacent, un autre système sur le même site, ou un autre système à
   l'autre bout du globe. La bande passante requise pour cette technique
   varie en fonction du débit de transactions du serveur primaire.
   La technique de la réplication en flux permet d'optimiser cette bande
   passante en utilisant une granularité plus fine que le log shipping
   par fichier. Pour cela, les modifications apportées au journal de
   transactions sont traitées sous forme de flux au travers d'une
   connexion réseau (voir <xref linkend="streaming-replication"/>).
  </para>

  <para>
   Il convient de noter que le log shipping est asynchrone, c'est à dire que les
   enregistrements de WAL sont transférés après que la transaction ait été validée. Par conséquent, il y a
   un laps de temps pendant lequel une perte de données pourrait se produire si le serveur primaire
   subissait un incident majeur; les transactions pas encore transférées seront perdues. La taille de la fenêtre
   de temps de perte de données peut être réduite par l'utilisation du paramètre
   <varname>archive_timeout</varname>, qui peut être abaissé à des valeurs
   de quelques secondes. Toutefois, un paramètre si bas augmentera de façon
   considérable la bande passante nécessaire pour le transfert de fichiers.
   L'utilisation de la technique de la réplication en flux (voir <xref linkend="streaming-replication"/>)
   permet de diminuer la taille de la fenêtre de temps de perte de données.
  </para>

  <para>
   La performance de la récupération est suffisamment bonne pour que le serveur secondaire ne
   soit en général qu'à quelques instants de la pleine
   disponibilité à partir du moment où il aura été activé. C'est pour cette raison que
   cette configuration de haute disponibilité est appelée warm standby.
   Restaurer un serveur d'une base de sauvegarde archivée, puis appliquer tous les journaux
   prendra largement plus de temps, ce qui fait que cette technique est une solution
   de 'disaster recovery' (reprise après sinistre), pas de haute disponibilité.
   Un serveur secondaire peut aussi être utilisé pour des requêtes en lecture seule, dans
   quel cas il est appelé un serveur Hot Standby. Voir <xref linkend="hot-standby"/> pour
   plus d'information.
  </para>

  <indexterm zone="high-availability">
   <primary>warm standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>PITR standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>serveur de standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>log shipping</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>serveur témoin</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>STONITH</primary>
  </indexterm>

  <sect2 id="standby-planning">
   <title>Préparatifs</title>

   <para>
    Il est habituellement préférable de créer les serveurs primaire et secondaires
    de façon à ce qu'ils soient aussi similaires que possible, au moins du
    point de vue du serveur de bases de données. En particulier, les chemins
    associés avec les tablespaces seront passés d'un nœud à l'autre sans conversion, ce qui
    implique que les serveurs primaire et secondaires doivent avoir les mêmes chemins de montage pour
    les tablespaces si cette fonctionnalité est utilisée. Gardez en tête que si
    <xref linkend="sql-createtablespace"/>
    est exécuté sur le primaire, tout nouveau point de montage nécessaire pour cela doit être créé
    sur le primaire et tous les secondaires avant que la commande ne
    soit exécutée. Le matériel n'a pas besoin d'être exactement le même, mais l'expérience monte
    que maintenir deux systèmes identiques est plus facile que maintenir deux
    différents sur la durée de l'application et du système.
    Quoi qu'il en soit, l'architecture hardware doit être la même &mdash; répliquer
    par exemple d'un serveur 32 bits vers un 64 bits ne fonctionnera pas.
   </para>

   <para>
    De manière générale, le log shipping entre serveurs exécutant des versions
    majeures différentes de <productname>PostgreSQL</productname> est
    impossible. La politique du PostgreSQL Global Development Group est de ne pas
    réaliser de changement sur les formats disques lors des mises à jour mineures,
    il est par conséquent probable que l'exécution de versions mineures différentes
    sur le primaire et le secondaire fonctionne correctement. Toutefois, il n'y a
    aucune garantie formelle de cela et il est fortement conseillé de garder le
    serveur primaire et celui de secondaire au même niveau de version autant que faire
    se peut. Lors d'une mise à jour vers une nouvelle version mineure, la politique la
    plus sûre est de mettre à jour les serveurs secondaires d'abord &mdash; une nouvelle
    version mineure est davantage susceptible de lire les enregistrements WAL d'une
    ancienne version mineure que l'inverse.
   </para>

  </sect2>

  <sect2 id="standby-server-operation" xreflabel="Fonctionnement du Serveur de Standby">
   <title>Fonctionnement du serveur secondaire</title>

   <para>
    Un serveur entre en mode standby si un fichier <anchor
    id="file-standby-signal" xreflabel="standby.signal"/>
    <filename>standby.signal</filename>
    <indexterm><primary><filename>standby.signal</filename></primary></indexterm>
    existe dans le répertoire de données du serveur.
   </para>

   <para>
    En mode standby, le serveur applique continuellement les WAL reçus du
    serveur primaire. Le serveur secondaire peut lire les WAL d'une archive WAL
    (voir <xref linkend="guc-restore-command"/>) ou directement du primaire via une
    connexion TCP (réplication en flux). Le serveur secondaire essaiera aussi de
    restaurer tout WAL trouvé dans le répertoire <filename>pg_wal</filename> de
    l'instance du secondaire. Cela se produit habituellement après un redémarrage de
    serveur, quand le secondaire rejoue à nouveau les WAL qui ont été reçu du primaire
    avant le redémarrage, mais vous pouvez aussi copier manuellement des fichiers dans
    <filename>pg_wal</filename> à tout moment pour qu'ils soient rejoués.
   </para>

   <para>
    Au démarrage, le serveur secondaire commence par restaurer tous les WAL
    disponibles à l'endroit où se trouvent les archives, en appelant la
    <varname>restore_command</varname>. Une fois qu'il a épuisé tous les WAL
    disponibles à cet endroit et que <varname>restore_command</varname>
    échoue, il essaie de restaurer tous les WAL disponibles dans le répertoire
    <filename>pg_wal</filename>. Si cela échoue, et que la réplication en flux
    a été activée, le serveur secondaire essaie
    de se connecter au serveur primaire et de démarrer la réception des WAL depuis
    le dernier enregistrement valide trouvé dans les archives ou
    <filename>pg_wal</filename>. Si cela
    échoue ou que la réplication en flux n'est pas configurée, ou que la connexion
    est plus tard déconnectée, le secondaire retourne à l'étape 1 et essaie de
    restaurer le fichier à partir de l'archive à nouveau. Cette boucle de
    retentatives de l'archive, <filename>pg_wal</filename> et par la
    réplication en flux continue
    jusqu'à ce que le serveur soit stoppé ou que le failover (bascule) soit
    déclenché.
   </para>

   <para>
    Le mode standby est quitté et le serveur bascule en mode de
    fonctionnement normal quand <command>pg_ctl promote</command> est exécuté,
    que <command>pg_promote()</command> est appelé ou qu'un fichier de trigger
    est trouvé (<varname>promote_trigger_file</varname>). Avant de basculer,
    tout WAL immédiatement disponible dans l'archive ou le
    <filename>pg_wal</filename> sera restauré, mais aucune tentative ne sera
    faite pour se connecter au primaire.
   </para>
  </sect2>

  <sect2 id="preparing-primary-for-standby">
   <title>Préparer le primaire pour les serveurs secondaires</title>

   <para>
    Mettez en place un archivage en continu sur le primaire vers un répertoire
    d'archivage accessible depuis le secondaire, comme décrit
    dans <xref linkend="continuous-archiving"/>. La destination d'archivage devrait être
    accessible du secondaire même quand le primaire est inaccessible, c'est à dire qu'il
    devrait se trouver sur le serveur secondaire lui-même ou un autre serveur de confiance, pas sur
    le serveur primaire.
   </para>

   <para>
    Si vous voulez utiliser la réplication en flux, mettez en place l'authentification sur le
    serveur primaire pour autoriser les connexions de réplication à partir du (ou des) serveur(s)
    secondaire(s)&nbsp;; c'est-à-dire, créez un rôle et mettez en place une ou des entrées appropriées dans
    <filename>pg_hba.conf</filename> avec le champ database positionné à
    <literal>replication</literal>. Vérifiez aussi que <varname>max_wal_senders</varname> est positionné
    à une valeur suffisamment grande dans le fichier de configuration du serveur primaire.
    Si des slots de réplication seront utilisés, il faut s'assurer que
    <varname>max_replication_slots</varname> est également positionné à une
    valeur suffisamment grande.
   </para>

   <para>
    Effectuez une sauvegarde de base comme décrit dans <xref linkend="backup-base-backup"/>
    pour initialiser le serveur secondaire.
   </para>
  </sect2>

  <sect2 id="standby-server-setup">
   <title>Paramétrer un serveur secondaire</title>

   <para>
    Pour paramétrer le serveur secondaire, restaurez la sauvegarde de base
    effectué sur le serveur primaire (voir <xref
    linkend="backup-pitr-recovery"/>). Créez un fichier
    <link linkend="file-standby-signal"><filename>standby.signal</filename></link>
    <indexterm><primary>standby.signal</primary></indexterm>
    dans le répertoire de données de l'instance du secondaire.
    Positionnez <xref linkend="guc-restore-command"/> à
    une simple commande qui recopie les fichiers de l'archive de WAL. Si vous
    comptez disposer de plusieurs serveurs secondaires pour mettre en
    œuvre de la haute disponibilité, assurez-vous que
    <varname>recovery_target_timeline</varname> est configuré à
    <literal>latest</literal> (la valeur par défaut), pour indiquer que le
    serveur secondaire devra prendre en compte la ligne temporelle définie
    lors de la bascule à un autre serveur secondaire.
   </para>

   <note>
    <para>
     <xref linkend="guc-restore-command"/> devrait retourner immédiatement si
     le fichier n'existe pas&nbsp;; le serveur essaiera la commande à nouveau
     si nécessaire.
    </para>
   </note>

   <para>
    Si vous souhaitez utiliser la réplication en flux, renseignez
    <xref linkend="guc-primary-conninfo"/> avec une chaîne de connexion libpq,
    contenant le nom d'hôte (ou l'adresse IP) et tout détail supplémentaire
    nécessaire pour se connecter au serveur primaire. Si le primaire a besoin d'un
    mot de passe pour l'authentification, le mot de passe doit aussi être spécifié dans
    <xref linkend="guc-primary-conninfo"/>.
   </para>

   <para>
    Si vous mettez en place le serveur secondaire pour des besoins de haute disponibilité,
    mettez en place l'archivage de WAL, les connexions et l'authentification à l'identique
    du serveur primaire, parce que le serveur secondaire fonctionnera comme un serveur primaire
    après la bascule.
   </para>

   <para>
    Si vous utilisez une archive WAL, sa taille peut être réduite en utilisant
    l'option <xref linkend="guc-archive-cleanup-command"/> pour supprimer les
    fichiers qui ne sont plus nécessaires au serveur secondaire. L'outil
    <application>pg_archivecleanup</application> est conçu spécifiquement pour
    être utilisé avec <varname>archive_cleanup_command</varname> dans des
    configurations typiques de secondaire, voir <xref linkend="pgarchivecleanup"/>.
    Notez toutefois que si vous utilisez l'archive à des fins de sauvegarde,
    vous avez besoin de garder les fichiers nécessaires pour restaurer à partir
    de votre dernière sauvegarde de base, même si ces fichiers ne sont plus
    nécessaires pour le serveur secondaire.
   </para>

   <para>
    Voici un simple exemple&nbsp;:
    <programlisting>
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass options=''-c wal_sender_timeout=5000'''
restore_command = 'cp /path/to/archive/%f %p'
archive_cleanup_command = 'pg_archivecleanup /path/to/archive %r'
    </programlisting>
   </para>

   <para>
    Vous pouvez avoir n'importe quel nombre de serveurs secondaires, mais si vous
    utilisez la réplication en flux, assurez vous d'avoir positionné
    <varname>max_wal_senders</varname> suffisamment haut sur le primaire pour leur permettre
    de se connecter simultanément.
   </para>
  </sect2>

  <sect2 id="streaming-replication">
   <title>Réplication en flux</title>

   <indexterm zone="high-availability">
    <primary>Réplication en flux</primary>
   </indexterm>

   <para>
    La réplication en flux permet à un serveur secondaire de rester plus
    à jour qu'il n'est possible avec l'envoi de journaux par fichiers. Le
    standby se connecte au primaire, qui envoie au standby les enregistrements
    de WAL dès qu'ils sont générés, sans attendre qu'un fichier de WAL soit rempli.
   </para>

   <para>
    La réplication en flux est asynchrone par défaut (voir <xref
    linkend="synchronous-replication"/>), auquel cas il y a un petit délai
    entre la validation d'une transaction sur le primaire et le moment où les
    changements sont visibles sur le standby. Le délai est toutefois beaucoup plus petit
    qu'avec l'envoi de fichiers, habituellement en dessous d'une seconde en partant
    de l'hypothèse que le standby est suffisamment puissant pour supporter la charge. Avec
    la réplication en flux, <varname>archive_timeout</varname> n'est pas nécessaire
    pour réduire la fenêtre de perte de données.
   </para>

   <para>
    Si vous utilisez la réplication en flux sans archivage en continu des fichiers,
    le serveur primaire pourrait recycler de vieux journaux de transactions avant que le
    serveur en standby ne les ait reçus. Si cela arrive, le serveur en standby devra être
    recréé à partir d'une nouvelle sauvegarde de l'instance. Vous pouvez éviter cela en
    positionnant <varname>wal_keep_size</varname> à une valeur suffisamment
    grande pour s'assurer que les journaux de transactions ne sont pas recyclés
    trop tôt, ou en configurant un slot de réplication pour le serveur en
    standby. Si un archivage des journaux de transactions est en place, et que
    les fichiers archivés sont disponibles depuis le serveur en standby, cette
    solution n'est pas nécessaire, puisque le serveur en standby peut toujours
    utiliser les fichiers archivés pour rattraper son retard, sous réserve que
    suffisamment de fichiers soient conservés.
   </para>

   <para>
    Pour utiliser la réplication en flux, mettez en place un serveur secondaire
    en mode fichier comme décrit dans <xref linkend="warm-standby"/>. L'étape qui
    transforme un secondaire en mode fichier en secondaire dans une réplication en flux est de
    faire pointer <varname>primary_conninfo</varname> vers le serveur primaire. Positionnez
    <xref linkend="guc-listen-addresses"/> et les options d'authentification
    (voir <filename>pg_hba.conf</filename>) sur le primaire pour que le serveur
    secondaire puisse se connecter à la pseudo-base <literal>replication</literal>
    sur le serveur primaire (voir <xref linkend="streaming-replication-authentication"/>).
   </para>

   <para>
    Sur les systèmes qui supportent l'option de keepalive sur les sockets, positionner
    <xref linkend="guc-tcp-keepalives-idle"/>,
    <xref linkend="guc-tcp-keepalives-interval"/> et
    <xref linkend="guc-tcp-keepalives-count"/> aide le primaire à reconnaître rapidement
    une connexion interrompue.
   </para>

   <para>
    Positionnez le nombre maximum de connexions concurrentes à partir des
    serveurs secondaires (voir <xref linkend="guc-max-wal-senders"/> pour les détails).
   </para>

   <para>
    Quand le secondaire est démarré et que <varname>primary_conninfo</varname> est
    positionné correctement, le secondaire se connectera au primaire après avoir
    rejoué tous les fichiers WAL disponibles dans l'archive. Si la connexion
    est établie avec succès, vous verrez un processus
    <literal>walreceiver</literal> sur le secondaire, et un processus
    <literal>walsender</literal> correspondant sur le primaire.
   </para>

   <sect3 id="streaming-replication-authentication">
    <title>Authentification</title>
    <para>
     Il est très important que les droits d'accès pour la réplication
     soient paramétrés pour que seuls les utilisateurs de confiance puissent
     lire le flux WAL, parce qu'il est facile d'en extraire des informations
     privilégiées. Les serveurs secondaires doivent s'authentifier au serveur
     primaire en tant qu'un compte disposant de l'attribut
     <literal>REPLICATION</literal> ou <literal>SUPERUSER</literal>. Il est
     recommandé de créer un compte utilisateur dédié pour la réplication. Il
     doit disposer des attributs <literal>REPLICATION</literal> et
     <literal>LOGIN</literal>. Alors que l'attribut
     <literal>REPLICATION</literal> donne beaucoup de droits, il ne permet pas
     à l'utilisateur de modifier de données sur le serveur primaire,
     contrairement à l'attribut <literal>SUPERUSER</literal>.
    </para>

    <para>
     L'authentification cliente pour la réplication est contrôlée par un enregistrement de
     <filename>pg_hba.conf</filename> spécifiant <literal>replication</literal> dans le champ
     <replaceable>database</replaceable>. Par exemple, si le secondaire s'exécute sur un hôte d'IP
     <literal>192.168.1.100</literal> et que le nom de l'utilisateur pour la réplication est
     <literal>foo</literal>, l'administrateur peut ajouter la ligne suivante au fichier
     <filename>pg_hba.conf</filename>  sur le primaire:


     <programlisting>
# Autoriser l'utilisateur "foo" de l'hôte 192.168.1.100 à se connecter au primaire
# en tant que secondaire si le mot de passe de l'utilisateur est correctement fourni
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    replication     foo             192.168.1.100/32        md5
     </programlisting>
    </para>
    <para>
     Le nom d'hôte et le numéro de port du primaire, le nom d'utilisateur de la connexion,
     et le mot de passe sont spécifiés dans le paramètre <xref linkend="guc-primary-conninfo"/>.
     Le mot de passe peut aussi être enregistré dans le fichier
     <filename>~/.pgpass</filename> sur le serveur en attente (en précisant
     <literal>replication</literal> dans le champ
     <replaceable>database</replaceable>).
     Par exemple, si le primaire s'exécute sur l'hôte d'IP <literal>192.168.1.50</literal>,
     port <literal>5432</literal>, que le nom de l'utilisateur pour la réplication est
     <literal>foo</literal>, et que le mot de passe est <literal>foopass</literal>, l'administrateur
     peut ajouter la ligne suivante au fichier <filename>postgresql.conf</filename> sur le secondaire&nbsp;:

     <programlisting>
# Le secondaire se connecte au primaire qui s'exécute sur l'hôte 192.168.1.50
# et port 5432 en tant qu'utilisateur "foo" dont le mot de passe est "foopass"
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
     </programlisting>
    </para>
   </sect3>

   <sect3 id="streaming-replication-monitoring">
    <title>Supervision</title>
    <para>
     Un important indicateur de santé de la réplication en flux est le nombre
     d'enregistrements générés sur le primaire, mais pas encore appliqués sur
     le secondaire. Vous pouvez calculer ce retard en comparant le point d'avancement
     des écritures du WAL sur le primaire avec le dernier point d'avancement reçu par
     le secondaire. Ils peuvent être récupérés en utilisant
     <function>pg_current_wal_lsn</function> sur le primaire et
     <function>pg_last_wal_receive_lsn</function> sur le secondaire
     (voir <xref linkend="functions-admin-backup-table"/> et
     <xref linkend="functions-recovery-info-table"/> pour plus de détails).
     Le point d'avancement de la réception dans le secondaire est aussi affiché dans
     le statut du processus de réception des WAL (wal receiver), affiché par
     la commande <command>ps</command> (voir <xref linkend="monitoring-ps"/> pour plus de détails).
    </para>
    <para>
     Vous pouvez obtenir la liste des processus émetteurs de WAL au moyen de la vue
     <link linkend="monitoring-pg-stat-replication-view"><structname>
       pg_stat_replication</structname></link>
     D'importantes différences entre les champs <function>pg_current_wal_lsn</function> et
     <literal>sent_lsn</literal> peuvent indiquer que le serveur primaire est en surcharge,
     tandis que des différences entre <literal>sent_lsn</literal> et
     <function>pg_last_wal_receive_lsn</function> sur le secondaire peuvent soit indiquer une latence
     réseau importante, soit que le secondaire est surchargé.
    </para>
    <para>
     Sur un secondaire en hot standby, le statut du processus wal receiver est récupérable
     avec la vue <link linkend="monitoring-pg-stat-wal-receiver-view">
      <structname>pg_stat_wal_receiver</structname></link>. Une différence
     importante entre <function>pg_last_wal_replay_lsn</function> et la
     colonne <literal>flushed_lsn</literal> de la vue indique que les WAL
     sont reçus plus rapidement qu'ils ne sont rejoués.
    </para>
   </sect3>

  </sect2>

  <sect2 id="streaming-replication-slots">
   <title>Slots de réplication</title>
   <indexterm>
    <primary>slot de réplication</primary>
    <secondary>réplication en flux</secondary>
   </indexterm>
   <para>
    Les slots de réplication fournissent une manière automatisée de s'assurer
    que le primaire ne supprime pas les journaux de transactions avant qu'ils
    n'aient été reçus par tous les serveurs secondaires, et que le serveur
    primaire ne supprime pas des lignes qui pourraient causer un
    <link linkend="hot-standby-conflict">conflit de restauration</link> même si
    le serveur secondaire est déconnecté.
   </para>
   <para>
    Au lieu d'utiliser des slots de réplication, il est possible d'empêcher la
    suppression des anciens journaux de transactions en utilisant
    <xref linkend="guc-wal-keep-size"/>, ou en les stockant dans un
    répertoire d'archive en utilisant <xref linkend="guc-archive-library"/>.
    Cependant, ces méthodes ont souvent pour résultat le stockage de plus de
    journaux de transactions que nécessaire, alors que les slots de réplication
    ne conservent que le nombre nécessaire de journaux de transactions. D'un
    autre côté, les slots de réplication peuvent retenir tellement de journaux
    de transactions qu'ils rempliraient l'espace disque alloué à
    <literal>pg_wal</literal>&nbsp;; <xref linkend="guc-max-slot-wal-keep-size"/>
    limite la taille des journaux de transactions à retenir par les slots de
    réplication.
   </para>
   <para>
    De la même manière, <xref linkend="guc-hot-standby-feedback"/>
    et <xref linkend="guc-vacuum-defer-cleanup-age"/> fournissent des
    protections contre la suppression de lignes utiles par vacuum, mais le
    premier paramètre n'offre aucune protection durant la période pendant
    laquelle le serveur secondaire n'est pas connecté, et le second nécessite
    souvent d'être positionné à une grande valeur pour fournir une protection
    adéquate. Les slots de réplication surmontent ces désavantages.
   </para>
   <sect3 id="streaming-replication-slots-manipulation">
    <title>Requêter et manipuler des slots de réplication</title>
    <para>
     Chaque slot de réplication à un nom, qui peut contenir des lettres en
     minuscule, des nombres ou un tiret bas.
    </para>
    <para>
     Les slots de réplication existants et leur états peuvent être vus dans la vue
     <link linkend="view-pg-replication-slots"><structname>pg_replication_slots</structname></link>.
    </para>
    <para>
     Les slots de réplication peuvent être créés et supprimés soit via le
     protocole de réplication en flux (voir <xref linkend="protocol-replication"/>)
     soit via des fonctions SQL (voir <xref linkend="functions-replication"/>).
    </para>
   </sect3>
   <sect3 id="streaming-replication-slots-config">
    <title>Exemple de configuration</title>
    <para>
     Il est possible de créer un slot de réplication ainsi&nbsp;:
     <programlisting>
postgres=# SELECT * FROM pg_create_physical_replication_slot('node_a_slot');
  slot_name  | lsn
-------------+-----
 node_a_slot |

postgres=# SELECT slot_name, slot_type, active FROM pg_replication_slots;
  slot_name  | slot_type | active
-------------+-----------+--------
 node_a_slot | physical  | f
     </programlisting>
     Pour configurer le serveur secondaire pour utiliser ce slot,
     <varname>primary_slot_name</varname> devrait être configuré sur le secondaire.
     Voici un exemple simple&nbsp;:
     <programlisting>
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
primary_slot_name = 'node_a_slot'
     </programlisting>
    </para>
   </sect3>
  </sect2>

  <sect2 id="cascading-replication">
   <title>Réplication en cascade</title>

   <indexterm zone="high-availability">
    <primary>Réplication en cascade</primary>
   </indexterm>

   <para>
    La fonctionnalité de réplication en cascade permet à un serveur
    secondaire d'accepter les connexions de réplication et d'envoyer un flux
    d'enregistrements de journaux de transactions à d'autres secondaires,
    agissant ainsi comme un relais. C'est généralement utilisé pour réduire
    le nombre de connexions directes au primaire et minimiser ainsi l'utilisation
    de bande passante entre sites distants.
   </para>

   <para>
    Un serveur secondaire agissant à la fois comme un receveur et comme un
    émetteur est connu sous le nom de secondaire en cascade
    (<foreignphrase>cascading standby</foreignphrase>). Les secondaires qui
    sont plus proches du serveur primaire sont connus sous le nom
    de serveurs <foreignphrase>upstream</foreignphrase> alors que les serveurs
    secondaires en bout de chaîne sont des serveurs
    <foreignphrase>downstream</foreignphrase>. La réplication en cascade
    ne pose pas de limites sur le nombre ou l'arrangement des serveurs
    <foreignphrase>downstream</foreignphrase>. Chaque secondaire se connecte à un
    seul serveur <foreignphrase>upstream</foreignphrase>, qui finit par
    arriver à un seul serveur primaire.
   </para>

   <para>
    Un secondaire en cascade envoie non seulement les enregistrements reçus de
    journaux de transactions mais aussi ceux restaurés des archives. Donc,
    même si la connexion de réplication d'une connexion upstream est
    rompue, la réplication en flux continue vers le serveur downstream tant
    que de nouveaux enregistrements de journaux de transactions sont
    disponibles.
   </para>

   <para>
    La réplication en cascade est actuellement asynchrone. La réplication
    synchrone (voir <xref linkend="synchronous-replication"/>) n'a aucun
    effet sur la réplication en cascade.
   </para>

   <para>
    Les messages en retour des serveurs Hot Standby se propagent vers les
    serveurs upstream, quelle que soit la configuration de la réplication en
    cascade.
   </para>

   <para>
    Si un serveur secondaire upstream est promu pour devenir le nouveau serveur
    primaire, les serveurs downstream continueront à recevoir le flux de
    réplication du nouveau primaire si le paramètre
    <varname>recovery_target_timeline</varname> est configuré à
    <literal>'latest'</literal> (valeur par défaut).
   </para>

   <para>
    Pour utiliser la réplication en cascade, configurez le secondaire en cascade
    de façon à ce qu'il accepte les connexions de réplication (configurez
    <xref linkend="guc-max-wal-senders"/> et <xref linkend="guc-hot-standby"/>,
    ainsi que l'<link linkend="auth-pg-hba-conf">authentification</link>).
    Vous aurez aussi besoin de configurer la variable
    <varname>primary_conninfo</varname> dans le secondaire downstream pour qu'elle
    pointe vers le secondaire en cascade.
   </para>
  </sect2>

  <sect2 id="synchronous-replication">
   <title>Réplication synchrone</title>

   <indexterm zone="high-availability">
    <primary>Réplication Synchrone</primary>
   </indexterm>

   <para>
    La réplication en flux mise en œuvre par <productname>PostgreSQL</productname> est asynchrone
    par défaut. Si le serveur primaire est hors-service, les transactions produites alors
    peuvent ne pas avoir été répliquées sur le serveur secondaire, impliquant une perte
    de données. La quantité de données perdues est proportionnelle au délai de réplication
    au moment de la bascule.
   </para>

   <para>
    La réplication synchrone permet de confirmer que tous les changements
    effectués par une transaction ont bien été transférées vers un ou plusieurs
    serveurs secondaires synchrones. Cette propriété étend le niveau de robustesse
    standard offert par un commit. En science informatique, ce niveau de
    protection est appelé réplication à deux états (<foreignphrase>2-safe
     replication</foreignphrase>) et <foreignphrase>group-1-safe</foreignphrase>
    (<foreignphrase>group-safe</foreignphrase> et
    <foreignphrase>1-safe</foreignphrase>) quand
    <varname>synchronous_commit</varname> est configuré à la valeur
    <literal>remote_write</literal>.
   </para>

   <para>
    Lorsque la réplication synchrone est utilisée, chaque validation portant
    sur une écriture va nécessiter d'attendre la confirmation
    de l'écriture de cette validation dans les journaux de transaction sur les disques
    du serveur primaire et des serveurs secondaires. Le seul moyen possible pour que des données
    soient perdues est que les serveur primaire et secondaire soient hors service au
    même moment. Ce mécanisme permet d'assurer un niveau plus élevé de robustesse, en admettant que
    l'administrateur système ait pris garde à l'emplacement et à la gestion de ces deux
    serveurs. Attendre après la confirmation de l'écriture augmente la confiance que l'utilisateur pourra avoir sur
    la conservation des modifications dans le cas où un serveur serait hors service mais il augmente aussi
    en conséquence le temps de réponse de chaque requête.
    Le temps minimum d'attente est celui de l'aller-retour entre les serveurs primaire et secondaire.
   </para>

   <para>
    Les transactions où seule une lecture est effectuée ou qui consistent à annuler une transaction ne nécessitent pas d'attendre
    les serveurs secondaires. Les validations concernant les transactions imbriquées ne nécessitent pas non plus d'attendre
    la réponse des serveurs secondaires, cela n'affecte en fait que les validations principales. De longues
    opérations comme le chargement de données ou la création d'index n'attendent pas
    le commit final pour synchroniser les données. Toutes les actions de validation en deux étapes
    nécessitent d'attendre la validation du secondaire, incluant autant l'opération de préparation que l'opération de validation.
   </para>

   <para>
    Un secondaire synchrone peut être un secondaire de réplication physique ou un
    abonné d'une réplication logique. Il peut aussi être tout autre
    consommateur de flux de réplication physique ou logique qui sait comment
    gérer les messages de retour appropriés. En plus des systèmes intégrés de
    réplication physique et logique, cela inclut le support des outils tels
    que <command>pg_receivewal</command> et <command>pg_recvlogical</command>
    ainsi que les systèmes de réplication et outils personnalisés tiers.
   </para>

   <sect3 id="synchronous-replication-config">
    <title>Configuration de base</title>

    <para>
     Une fois la réplication en flux configurée, la configuration de la réplication synchrone
     ne demande qu'une unique étape de configuration supplémentaire&nbsp;:
     la variable <xref linkend="guc-synchronous-standby-names"/> doit être définie à
     une valeur non vide. La variable <varname>synchronous_commit</varname> doit aussi être définie à
     <literal>on</literal>, mais comme il s'agit d'une valeur par défaut, il n'est pas nécessaire de la
     modifier. (Voir <xref linkend="runtime-config-wal-settings"/> et
     <xref linkend="runtime-config-replication-primary"/>.)
     Cette configuration va entraîner l'attente de la confirmation de l'écriture permanente de chaque validation
     sur le serveur secondaire.
     La variable <varname>synchronous_commit</varname> peut être définie soit par des
     utilisateurs, soit par le fichier de configuration pour des utilisateurs ou des bases de données fixées, soit
     dynamiquement par des applications, pour contrôler la robustesse des échanges transactionnels.
    </para>

    <para>
     Suite à l'enregistrement sur disque d'une validation sur le serveur primaire,
     l'enregistrement WAL est envoyé au serveur secondaire. Le serveur secondaire
     retourne une réponse à chaque fois qu'un nouveau lot de données WAL est
     écrit sur disque, à moins que le paramètre
     <varname>wal_receiver_status_interval</varname> soit défini à zéro sur le
     serveur secondaire. Dans le cas où le paramètre <varname>synchronous_commit</varname>
     est configuré à la valeur <literal>remote_apply</literal>, le serveur
     secondaire envoie des messages de réponse quand l'enregistrement de validation
     (commit) est rejoué, rendant la transaction visible. Si le serveur secondaire
     est configuré en serveur synchrone d'après la configuration du paramètre
     <varname>synchronous_standby_names</varname>
     sur le primaire, le message de réponse provenant du secondaire sera considéré
     parmi ceux des autres serveurs secondaires pour décider du moment de libération
     des transactions attendant la confirmation de la bonne réception de
     l'enregistrement de commit. Ces paramètres permettent à l'administrateur
     de spécifier quels serveurs secondaires suivront un comportement synchrone.
     Remarquez ici que la configuration de la réplication synchrone se situe
     sur le serveur primaire. Les serveurs secondaires nommés doivent être
     directement connectés au primaire&nbsp;; le primaire ne connaît rien des
     serveurs secondaires utilisant la réplication en cascade.
    </para>

    <para>
     Configurer <varname>synchronous_commit</varname> à
     <literal>remote_write</literal> fera que chaque COMMIT attendra la
     confirmation de la réception en mémoire de l'enregistrement du COMMIT
     par le secondaire et son écriture via la système d'exploitation, sans que
     les données du cache du système ne soient vidées sur disque au niveau
     du serveur secondaire. Cette configuration fournit une garantie moindre
     de durabilité que la configuration <literal>on</literal>&nbsp;: le secondaire
     peut perdre les données dans le cas d'un crash du système d'exploitation,
     mais pas dans le cas du crash de <productname>PostgreSQL</productname>.
     Cependant, il s'agit d'une configuration utile en pratique car il diminue
     le temps de réponse pour la transaction. Des pertes de données ne peuvent
     survenir que si le serveur primaire et le secondaire tombent en même temps et
     que la base de données du primaire est corrompue.
    </para>

    <para>
     Configurer <varname>synchronous_commit</varname> à <literal>remote_apply</literal>
     fera en sorte que chaque commit devra attendre le retour des secondaires synchrones
     actuels indiquant qu'ils ont bien rejoué la transaction, la rendant visible aux
     requêtes des utilisateurs. Dans des cas simples, ceci permet une répartition de
     charge sans incohérence.
    </para>

    <para>
     Habituellement, un signal d'arrêt rapide (<foreignphrase>fast shutdown</foreignphrase>)
     annule les transactions en cours sur tous les processus serveur. Cependant, dans
     le cas de la réplication asynchrone, le serveur n'effectuera pas un
     arrêt complet avant que chaque enregistrement WAL ne soit transféré aux serveurs
     secondaires connectés.
    </para>

   </sect3>

   <sect3 id="synchronous-replication-multiple-standbys">
    <title>Multiple secondaires synchrones</title>

    <para>
     La réplication synchrone supporte un ou plusieurs serveurs secondaires
     synchrones. Les transactions attendront que tous les serveurs secondaires
     considérés synchrones confirment la réception de leurs données. Le nombre
     de secondaires dont les transactions doivent attendre la réponse est indiqué
     dans le paramètre <varname>synchronous_standby_names</varname>. Ce paramètre
     indique aussi une liste des noms des serveurs secondaires ou l'emploi
     de la méthode (<literal>FIRST</literal> ou <literal>ANY</literal>) pour choisir
     sur quel serveur synchrone basculer parmi l'ensemble des serveurs listés.
    </para>

    <para>
     La méthode <literal>FIRST</literal> définit une réplication synchrone
     priorisée&nbsp;: elle temporise la validation de la transaction
     jusqu'à ce que les enregistrements WAL soient répliqués en
     fonction de la priorité définie des serveurs secondaires dans une liste
     ordonnée.
     Le serveur secondaire dont le nom apparaît en premier sur la liste est
     prioritaire et est celui qui est considéré comme synchrone.
     Les serveurs secondaires suivants sont considérés comme un/des
     serveurs secondaires synchrones potentiels.
     Si le premier serveur synchrone venait à tomber,
     il serait immédiatement remplacé par le serveur
     secondaire prioritaire suivant.

    </para>
    <para>
     Voici un exemple de configuration de <varname>synchronous_standby_names</varname> pour la
     réplication synchrone priorisée&nbsp;:
     <programlisting>
synchronous_standby_names = 'FIRST 2 (s1, s2, s3)'
     </programlisting>
     Dans cet exemple, si les quatre serveurs secondaires <literal>s1</literal>,
     <literal>s2</literal>, <literal>s3</literal> et <literal>s4</literal>
     sont fonctionnels et en cours d'exécution, les deux serveurs
     <literal>s1</literal> et <literal>s2</literal> seront choisis comme
     secondaires synchrones car leurs noms apparaissent en premier dans la
     liste des serveurs secondaires. <literal>s3</literal> est un serveur
     secondaire synchrone potentiel et prendra le rôle d'un secondaire synchrone
     si <literal>s1</literal> ou <literal>s2</literal> tombe.
     <literal>s4</literal> est un secondaire asynchrone et son nom n'est pas dans la liste.
    </para>

    <para>
     La méthode <literal>ANY</literal> définit une réplication synchrone
     basée sur un quorum&nbsp;: elle temporise la validation de la
     transaction jusqu'à ce que les enregistrements WAL soient répliqués
     <emphasis>au moins</emphasis> sur le nombre de serveurs définis dans
     la liste.
    </para>
    <para>
     Voici un exemple de configuration du paramètre <varname>synchronous_standby_names</varname> pour la
     réplication synchrone avec un quorum&nbsp;:
     <programlisting>
 synchronous_standby_names = 'ANY 2 (s1, s2, s3)'
     </programlisting>
     Dans cet exemple, sur quatre serveurs secondaires démarrés <literal>s1</literal>,
     <literal>s2</literal>,<literal>s3</literal> et <literal>s4</literal>,
     pour obtenir la validation d'une transaction, le serveur primaire
     attendra la réponse d'au minimum deux secondaires parmi <literal>s1</literal>,
     <literal>s2</literal> et <literal>s3</literal>. <literal>s4</literal>
     est un secondaire asynchrone et son nom n'est pas dans la liste.
    </para>
    <para>
     L'état de synchronicité des serveurs secondaires peut être consulté
     avec la vue <structname>pg_stat_replication</structname>.
    </para>
   </sect3>

   <sect3 id="synchronous-replication-performance">
    <title>S'organiser pour obtenir de bonnes performances</title>

    <para>
     La réplication synchrone nécessite souvent d'organiser avec une grande attention
     les serveurs secondaires pour apporter un bon niveau de performances aux applications. Les phases d'attente d'écriture
     n'utilisent pas les ressources systèmes, mais les verrous transactionnels restent
     positionnés jusqu'à ce que le transfert vers les serveurs secondaires soit confirmé. En conséquence, une utilisation non avertie de
     la réplication synchrone aura pour impact une baisse des performances de la base de données
     d'une application due à l'augmentation des temps de réponses et à un moins bon support de la charge.
    </para>

    <para>
     <productname>PostgreSQL</productname> permet aux développeurs d'application
     de spécifier le niveau de robustesse à employer pour la réplication. Cela peut être
     spécifié pour le système entier, mais aussi pour
     des utilisateurs ou des connexions spécifiques, ou encore pour des transactions individuelles.
    </para>

    <para>
     Par exemple, une répartition de charge pour une application pourrait être constituée de&nbsp;:
     10 % de modifications concernant des articles de clients importants, et
     90 % de modifications de moindre importance et qui ne devraient pas avoir d'impact sur le métier
     si elles venaient à être perdues, comme des dialogues de messagerie entre utilisateurs.
    </para>

    <para>
     Les options de réplication synchrone spécifiées par une application
     (sur le serveur primaire) permettent de n'utiliser la réplication synchrone que pour les modifications les plus
     importantes, sans affecter les performances sur la plus grosse partie des traitements.
     Les options modifiables par les applications sont un outil important permettant
     d'apporter les bénéfices de la réplication synchrone aux applications nécessitant de la haute performance.
    </para>

    <para>
     Il est conseillé de disposer d'une bande passante réseau supérieure
     à la quantité de données WAL générées.
    </para>

   </sect3>

   <sect3 id="synchronous-replication-ha">
    <title>S'organiser pour la haute disponibilité</title>

    <para>
     <varname>synchronous_standby_names</varname> indique le nombre et les noms
     des serveurs secondaires synchrones pour lesquels les validations de
     transactions effectuées lorsque <varname>synchronous_commit</varname> est
     configurée à <literal>on</literal>, <literal>remote_apply</literal> ou
     <literal>remote_write</literal>, attendront leur réponse. Ces validations
     de transactions pourraient ne jamais se terminer si un des secondaires
     synchrones s'arrêtait brutalement.
    </para>

    <para>
     La meilleure solution pour la haute disponibilité est de s'assurer que vous
     conservez autant de serveurs secondaires synchrones que demandés. Ceci se fait
     en nommant plusieurs secondaires synchrones potentiels avec
     <varname>synchronous_standby_names</varname>.
    </para>

    <para>
     Dans la réplication synchrone dite priorisée, les serveurs secondaires dont les noms
     apparaissent en premier seront utilisé comme secondaires synchrones.
     Les secondaires définis ensuite prendront la place de serveur
     synchrone si l'un des serveurs venait à tomber.

    </para>

    <para>
     Dans la réplication dite de quorum, tous les secondaires spécifiés dans la liste
     seront utilisés comme des secondaires synchrones potentiels.
     Même si l'un d'entre eux tombe, les autres secondaires continueront de
     prétendre au rôle de secondaire synchrone.
    </para>

    <para>
     Au moment où le premier serveur secondaire s'attache au serveur primaire, il est possible qu'il ne soit pas exactement
     synchronisé. Cet état est appelé le mode <literal>catchup</literal>. Une fois
     la différence entre le serveur secondaire et le serveur primaire ramenée à zéro,
     le mode <literal>streaming</literal> est atteint.
     La durée du mode catchup peut être longue surtout juste après la création du serveur secondaire.
     Si le serveur secondaire est arrêté sur cette période, alors la durée du mode catchup
     sera d'autant plus longue.
     Le serveur secondaire ne peut devenir un serveur secondaire synchrone
     que lorsque le mode <literal>streaming</literal> est atteint.
     L'état de synchronicité des serveurs secondaires peut être consulté
     avec la vue <structname>pg_stat_replication</structname>.
    </para>

    <para>
     Si le serveur primaire redémarre alors que des opérations de commit étaient en attente de confirmation, les
     transactions en attente ne seront réellement enregistrées qu'au moment où la base de données du serveur primaire
     sera redémarrée.
     Il n'y a aucun moyen de savoir si tous les serveurs secondaires ont reçu toutes
     les données WAL nécessaires au moment où le serveur primaire est déclaré hors-service. Des
     transactions pourraient ne pas être considérées comme sauvegardées sur le serveur secondaire, même si
     elles l'étaient sur le serveur primaire. La seule garantie offerte dans ce cadre est que
     l'application ne recevra pas de confirmation explicite de la
     réussite d'une opération de validation avant qu'il ne soit sûr que les données WAL sont
     reçues proprement par tous les serveurs secondaires synchrones.
    </para>

    <para>
     Si vous ne pouvez vraiment pas conserver autant de serveurs secondaires
     synchrones que demandés, alors vous devriez diminuer le nombre de secondaires
     synchrones dont le système doit attendre les réponses aux validations de
     transactions, en modifiant <varname>synchronous_standby_names</varname> (ou
     en le désactivant) et en rechargeant le fichier de configuration du serveur
     primaire.
    </para>

    <para>
     Si le serveur primaire n'est pas accessible par les serveurs secondaires restants, il est conseillé
     de basculer vers le meilleur candidat possible parmi ces serveurs secondaires.
    </para>

    <para>
     S'il est nécessaire de recréer un serveur secondaire alors que des transactions sont
     en attente de confirmation, prenez garde à ce que les commandes pg_backup_start() et
     pg_backup_stop() soient exécutées dans un contexte où
     la variable <varname>synchronous_commit</varname> vaut <literal>off</literal> car, dans le cas contraire, ces
     requêtes attendront indéfiniment l'apparition de ce serveur secondaire.
    </para>

   </sect3>

  </sect2>

  <sect2 id="continuous-archiving-in-standby">
   <title>Archivage continu côté secondaire</title>

   <indexterm>
    <primary>Archivage continu</primary>
    <secondary>côté secondaire</secondary>
   </indexterm>

   <para>
    Lorsque l'archivage continu est utilisé sur un secondaire, il existe deux
    scénarios possibles&nbsp;: soit les archives sont partagées entre le
    serveur primaire et le serveur secondaire, soit le secondaire peut avoir ses
    propres archives. Si le serveur possède ses propres archives, en
    définissant le paramètre <varname>archive_mode</varname> à
    <literal>always</literal>, le secondaire exécutera la commande d'archivage
    pour chaque segment de WAL qu'il aura reçu, peu importe qu'il utilise la
    réplication par les archives ou la réplication en flux. La gestion par
    archivage partagé peut être faite de la même manière, mais
    <varname>archive_library</varname> doit d'abord tester si le segment de
    WAL existe, et si le fichier existant contient les mêmes informations.
    Cela demande plus de précaution lors de la définition de la commande, car
    elle doit vérifier qu'elle n'écrase pas un fichier existant avec un
    contenu différent, et doit renvoyer un succès si le même fichier est
    archivé deux fois. Tout ceci devant être en plus effectué sans concurrence
    si deux serveurs essaient d'archiver le même fichier au même moment.
   </para>

   <para>
    Si <varname>archive_mode</varname> est défini à <literal>on</literal>,
    l'archivage n'est pas actif pendant les modes recovery et standby. Si le
    serveur secondaire est promu, il commencera à réaliser l'archivage après sa
    promotion, et il archivera uniquement les fichiers (WAL et historique)
    qu'il a lui même produit. Pour être sûr d'obtenir un jeu complet
    d'archives, vous devez vous assurer que tous les fichiers WAL ont été
    archivés avant qu'ils atteignent le secondaire. C'est implicitement toujours
    le cas avec un log shipping s'appuyant sur les archives, car le secondaire
    ne récupère que des informations provenant de ces mêmes fichiers
    archivés. Ce n'est pas le cas dans le cadre de la réplication en flux.
    Lorsqu'un serveur n'est pas en mode recovery, il n'y a aucune différence entre les
    modes <literal>on</literal> et <literal>always</literal>.
   </para>
  </sect2>
 </sect1>

 <sect1 id="warm-standby-failover">
  <title>Bascule (<foreignphrase>Failover</foreignphrase>)</title>

  <para>
   Si le serveur primaire plante alors le serveur secondaire devrait commencer
   les procédures de failover.
  </para>

  <para>
   Si le serveur secondaire plante alors il n'est pas nécessaire d'effectuer un failover. Si le
   serveur secondaire peut être redémarré, même plus tard, alors le processus de récupération
   peut aussi être redémarré au même moment, en bénéficiant du fait que la récupération sait reprendre
   où elle en était. Si le serveur secondaire ne peut pas être redémarré, alors
   une nouvelle instance secondaire complète devrait être créée.
  </para>

  <para>
   Si le serveur primaire plante, que le serveur secondaire devient le
   nouveau primaire, et que l'ancien primaire redémarre, vous devez avoir
   un mécanisme pour informer l'ancien primaire qu'il n'est plus primaire. C'est aussi
   quelquefois appelé <acronym>STONITH</acronym> (Shoot The Other Node In The Head, ou
   Tire Dans La Tête De L'Autre Nœud), qui est nécessaire pour éviter les situations où
   les deux systèmes pensent qu'ils sont le primaire, ce qui amènerait de la confusion, et
   finalement de la perte de données.
  </para>

  <para>
   Beaucoup de systèmes de failover n'utilisent que deux systèmes, le primaire et le secondaire,
   connectés par un mécanisme de type ligne de vie (heartbeat) pour vérifier continuellement la
   connexion entre les deux et la viabilité du primaire. Il est aussi
   possible d'utiliser un troisième système (appelé un serveur témoin) pour éviter
   certains cas de bascule inappropriés, mais la complexité supplémentaire
   peut ne pas être justifiée à moins d'être mis en place avec suffisamment
   de précautions et des tests rigoureux.
  </para>

  <para>
   <productname>PostgreSQL</productname> ne fournit pas le logiciel
   système nécessaire pour identifier un incident sur le primaire et notifier
   le serveur de bases de données secondaire. De nombreux outils de ce genre existent et sont bien
   intégrés avec les fonctionnalités du système d'exploitation nécessaires à la bascule,
   telles que la migration d'adresse IP.
  </para>

  <para>
   Une fois que la bascule vers le secondaire se produit, il n'y a plus qu'un
   seul serveur en fonctionnement. C'est ce qu'on appelle un état dégradé.
   L'ancien secondaire est maintenant le primaire, mais l'ancien primaire est arrêté
   et pourrait rester arrêté. Pour revenir à un fonctionnement normal, un serveur
   secondaire doit être recréé,
   soit sur l'ancien système primaire quand il redevient disponible, ou sur un troisième,
   peut-être nouveau, système. L'utilitaire <xref linkend="app-pgrewind"/> peut être utilisé
   pour accélérer ce processus sur de gros clusters. Une fois que ceci est
   effectué, le primaire et le secondaire peuvent
   être considérés comme ayant changé de rôle. Certaines personnes choisissent d'utiliser un troisième
   serveur pour fournir une sauvegarde du nouveau primaire jusqu'à ce que le nouveau serveur
   secondaire soit recréé,
   bien que ceci complique visiblement la configuration du système et les procédures d'exploitation.
  </para>

  <para>
   Par conséquent, basculer du primaire vers le serveur secondaire peut être rapide mais requiert
   du temps pour re-préparer le cluster de failover. Une bascule régulière du
   primaire vers le secondaire est utile, car cela permet une période d'interruption de production sur
   chaque système pour maintenance. Cela vous permet aussi pour vous assurer que
   votre mécanisme de bascule fonctionnera réellement quand vous en aurez besoin.
   Il est conseillé que les procédures d'administration soient écrites.
  </para>

  <para>
   Pour déclencher le failover d'un serveur secondaire en log shipping,
   exécutez la commande <command>pg_ctl promote</command>, lancez la fonction
   <function>pg_promote()</function> ou créez un fichier trigger
   avec le nom de fichier et le chemin spécifiés par le paramètre
   <varname>promote_trigger_file</varname>. Si vous comptez utiliser la commande
   <command>pg_ctl promote</command> ou la fonction
   <function>pg_promote()</function> pour effectuer la bascule, la variable
   <varname>promote_trigger_file</varname> n'est pas nécessaire. S'il s'agit
   d'ajouter des serveurs qui ne seront utilisés que pour alléger le serveur
   primaire des requêtes en lecture seule, et non pas pour des considérations
   de haute disponibilité, il n'est pas nécessaire de les réveiller
   (<foreignphrase>promote</foreignphrase>).
  </para>
 </sect1>

 <sect1 id="hot-standby">
  <title>Hot Standby</title>

  <indexterm zone="high-availability">
   <primary>Hot Standby</primary>
  </indexterm>

  <para>
   Hot Standby est le terme utilisé pour décrire la possibilité de se
   connecter et d'exécuter des requêtes en lecture seule alors que le
   serveur est en récupération d'archive ou mode standby. C'est
   utile à la fois pour la réplication et pour restaurer
   une sauvegarde à un état désiré avec une grande précision.
   Le terme Hot Standby fait aussi référence à la capacité du serveur à passer
   de la récupération au fonctionnement normal tandis-que les utilisateurs
   continuent à exécuter des requêtes et/ou gardent leurs connexions ouvertes.
  </para>

  <para>
   Exécuter des requêtes en mode hot standby est similaire au fonctionnement
   normal des requêtes, bien qu'il y ait quelques différences d'utilisation
   et d'administration notées ci-dessous.
  </para>

  <sect2 id="hot-standby-users">
   <title>Aperçu pour l'utilisateur</title>

   <para>
    Quand le paramètre <xref linkend="guc-hot-standby"/> est configuré à true
    sur un serveur en attente, le serveur commencera à accepter les connexions
    une fois que la restauration est parvenue à un état cohérent. Toutes les
    connexions qui suivront seront des connexions en lecture seule&nbsp;; même
    les tables temporaires ne pourront pas être utilisées.
   </para>

   <para>
    Les données sur le secondaire mettent un certain temps pour arriver du serveur
    primaire, il y aura donc un délai mesurable entre primaire et secondaire. La même
    requête exécutée presque simultanément sur le primaire et le secondaire pourrait par
    conséquent retourner des résultats différents. On dit que la donnée est
    <firstterm>cohérente à terme</firstterm> avec le primaire. Une fois que
    l'enregistrement de validation (COMMIT) d'une transaction est rejoué sur
    le serveur en attente, les modifications réalisées par cette transaction
    seront visibles par toutes les images de bases obtenues par les transactions
    en cours sur le serveur en attente. Ces images peuvent être prises au début
    de chaque requête ou de chaque transaction, suivant le niveau d'isolation
    des transactions utilisé à ce moment. Pour plus de détails, voir <xref
    linkend="transaction-iso"/>.
   </para>

   <para>
    Les transactions exécutées pendant la période de restauration sur un
    serveur en mode hot standby peuvent inclure les commandes suivantes&nbsp;:
    <itemizedlist>
     <listitem>
      <para>
       Accès par requête&nbsp;: <command>SELECT</command>, <command>COPY TO</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de curseur&nbsp;: <command>DECLARE</command>, <command>FETCH</command>, <command>CLOSE</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Paramètres&nbsp;: <command>SHOW</command>, <command>SET</command>, <command>RESET</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de gestion de transaction&nbsp;:
       <itemizedlist>
        <listitem>
         <para>
          <command>BEGIN</command>, <command>END</command>, <command>ABORT</command>, <command>START TRANSACTION</command>
         </para>
        </listitem>
        <listitem>
         <para>
          <command>SAVEPOINT</command>, <command>RELEASE</command>, <command>ROLLBACK TO SAVEPOINT</command>
         </para>
        </listitem>
        <listitem>
         <para>
          Blocs d'<command>EXCEPTION</command> et autres sous-transactions internes
         </para>
        </listitem>
       </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK TABLE</command>, mais seulement quand explicitement dans un de ces modes:
       <literal>ACCESS SHARE</literal>, <literal>ROW SHARE</literal> ou <literal>ROW EXCLUSIVE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Plans et ressources&nbsp;: <command>PREPARE</command>, <command>EXECUTE</command>,
       <command>DEALLOCATE</command>, <command>DISCARD</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Plugins et extensions&nbsp;: <command>LOAD</command>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>UNLISTEN</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Les transactions lancées pendant la restauration d'un serveur en hot standby
    ne se verront jamais affectées un identifiant de transactions et ne peuvent
    pas être écrites dans les journaux de transactions. Du coup, les actions
    suivantes produiront des messages d'erreur&nbsp;:

    <itemizedlist>
     <listitem>
      <para>
       Langage de Manipulation de Données (LMD ou DML)&nbsp;: <command>INSERT</command>,
       <command>UPDATE</command>, <command>DELETE</command>, <command>COPY FROM</command>,
       <command>TRUNCATE</command>.
       Notez qu'il n'y a pas d'action autorisée qui entraînerait l'exécution d'un
       trigger pendant la récupération. Cette restriction s'applique même pour
       les tables temporaires car les lignes de ces tables ne peuvent être
       lues et écrites s'il n'est pas possible d'affecter un identifiant de
       transactions, ce qui n'est actuellement pas possible dans un
       environnement Hot Standby.
      </para>
     </listitem>
     <listitem>
      <para>
       Langage de Définition de Données (LDD ou DDL)&nbsp;: <command>CREATE</command>,
       <command>DROP</command>, <command>ALTER</command>, <command>COMMENT</command>.
       Cette restriction s'applique aussi aux tables temporaires car, pour
       mener à bien ces opérations, cela nécessiterait de mettre à jour les
       catalogues systèmes.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>SELECT ... FOR SHARE | UPDATE</command>, car les verrous de
       lignes ne peuvent pas être pris sans mettre à jour les fichiers de
       données.
      </para>
     </listitem>
     <listitem>
      <para>
       Règles sur des ordres <command>SELECT</command> qui génèrent des commandes LMD.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> qui demandent explicitement un mode supérieur à <literal>ROW EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> dans sa forme courte par défaut, puisqu'il demande <literal>ACCESS EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de gestion de transaction qui positionnent explicitement un état n'étant pas en lecture-seule&nbsp;:
       <itemizedlist>
        <listitem>
         <para>
          <command>BEGIN READ WRITE</command>,
          <command>START TRANSACTION READ WRITE</command>
         </para>
        </listitem>
        <listitem>
         <para>
          <command>SET TRANSACTION READ WRITE</command>,
          <command>SET SESSION CHARACTERISTICS AS TRANSACTION READ WRITE</command>
         </para>
        </listitem>
        <listitem>
         <para>
          <command>SET transaction_read_only = off</command>
         </para>
        </listitem>
       </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de two-phase commit&nbsp;: <command>PREPARE TRANSACTION</command>,
       <command>COMMIT PREPARED</command>, <command>ROLLBACK PREPARED</command>
       parce que même les transactions en lecture seule ont besoin d'écrire dans le WAL
       durant la phase de préparation (la première des deux phases du two-phase commit).
      </para>
     </listitem>
     <listitem>
      <para>
       Mise à jour de séquence&nbsp;: <function>nextval()</function>, <function>setval()</function>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LISTEN</command>, <command>NOTIFY</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Dans le cadre normal, les transactions <quote>en lecture seule</quote>
    permettent l'utilisation des instructions
    <command>LISTEN</command> et
    <command>NOTIFY</command>, donc les sessions Hot Standby ont des
    restrictions légèrement inférieures à celles de sessions en lecture seule
    ordinaires. Il est possible que certaines des restrictions soient encore
    moins importantes dans une prochaine version.
   </para>

   <para>
    Lors du fonctionnement en serveur hot standby, le paramètre
    <varname>transaction_read_only</varname> est toujours à true et ne peut
    pas être modifié. Tant qu'il n'y a pas de tentative de modification sur
    la base de données, les connexions sur un serveur en hot standby se
    comportent de façon pratiquement identiques à celles sur un serveur normal.
    Quand une bascule (<foreignphrase>failover</foreignphrase> ou
    <foreignphrase>switchover</foreignphrase>) survient, la base de données
    bascule dans le mode de traitement normal. Les sessions resteront
    connectées pendant le changement de mode. Quand le mode hot standby est
    terminé, il sera possible de lancer des transactions en lecture/écriture,
    y compris pour les sessions connectées avant la bascule.
   </para>

   <para>
    Les utilisateurs peuvent déterminer si le Hot Standby est actuellement
    actif pour leur session en exécutant <command>SHOW
    in_hot_standby</command>. (Le paramètre <varname>in_hot_standby</varname>
    n'existant pas avant la version 14, <command>SHOW
    transaction_read_only</command> est donc à utiliser sur les serveurs en
    version plus ancienne.) De plus, un jeu de fonctions
    (<xref linkend="functions-recovery-info-table"/>) permettent aux
    utilisateurs d'accéder à des informations à propos du serveur secondaire.
    Ceci vous permet d'écrire des programmes qui sont conscients de
    l'état actuel de la base. Vous pouvez vous en servir pour superviser
    l'avancement de la récupération, ou pour écrire des programmes complexes
    qui restaurent la base dans des états particuliers.
   </para>
  </sect2>

  <sect2 id="hot-standby-conflict">
   <title>Gestion des conflits avec les requêtes</title>

   <para>
    Les nœuds primaire et secondaire sont faiblement couplés à bien des égards. Des
    actions sur le primaire auront un effet sur le secondaire. Par conséquent, il y a
    un risque d'interactions négatives ou de conflits entre eux. Le conflit le
    plus simple à comprendre est la performance : si un gros chargement de données a
    lieu sur le primaire, il générera un flux similaire d'enregistrements WAL sur le
    secondaire, et les requêtes du secondaire pourrait entrer en compétition pour les ressources
    systèmes, comme les entrées-sorties.
   </para>

   <para>
    Il y a aussi d'autres types de conflits qui peuvent se produire avec le
    Hot Standby. Ces conflits sont des <emphasis>conflits durs</emphasis> dans
    le sens où des requêtes pourraient devoir être annulées et, dans certains
    cas, des sessions déconnectées, pour les résoudre. L'utilisateur dispose
    de plusieurs moyens pour gérer ces conflits. Voici les différents cas de
    conflits possibles&nbsp;:

    <itemizedlist>
     <listitem>
      <para>
       Des verrous en accès exclusif pris sur le serveur primaire, incluant à
       la fois les commandes <command>LOCK</command> exclusives et quelques
       actions de type <acronym>DDL</acronym>, entrent en conflit avec les
       accès de table des requêtes en lecture seule.
      </para>
     </listitem>
     <listitem>
      <para>
       La suppression d'un tablespace sur le serveur primaire entre en conflit
       avec les requêtes sur le serveur secondaire qui utilisent ce tablespace
       pour les fichiers temporaires.
      </para>
     </listitem>
     <listitem>
      <para>
       La suppression d'une base de données sur le serveur primaire entre en
       conflit avec les sessions connectées sur cette base de données sur
       le serveur en attente.
      </para>
     </listitem>
     <listitem>
      <para>
       La copie d'un enregistrement nettoyé par un VACUUM entre en conflit
       avec les transactions sur le serveur en attente qui peuvent toujours
       <quote>voir</quote> au moins une des lignes à supprimer.
      </para>
     </listitem>
     <listitem>
      <para>
       La copie d'un enregistrement nettoyé par un VACUUM entre en conflit
       avec les requêtes accédant à la page cible sur le serveur en attente,
       qu'elles voient ou non les données à supprimer.
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Sur le serveur primaire, ces cas résultent en une attente
    supplémentaire&nbsp;; l'utilisateur peut choisir d'annuler une des actions
    en conflit. Néanmoins, sur le serveur en attente, il n'y a pas de choix
    possibles&nbsp;: l'action enregistrée dans les journaux de transactions
    est déjà survenue sur le serveur primaire et le serveur secondaire doit
    absolument réussir à l'appliquer. De plus, permettre que l'enregistrement
    de l'action attende indéfiniment pourrait avoir des effets fortement non
    désirables car le serveur en attente sera de plus en plus en retard par
    rapport au primaire. Du coup, un mécanisme est fourni pour forcer
    l'annulation des requêtes sur le serveur en attente qui entreraient en
    conflit avec des enregistrements des journaux de transactions en attente.
   </para>

   <para>
    Voici un exemple de problème type&nbsp;: un administrateur exécute un
    <command>DROP TABLE</command> sur une table du serveur primaire qui est
    actuellement utilisé dans des requêtes du serveur en attente. Il est clair
    que la requête ne peut pas continuer à s'exécuter si l'enregistrement
    dans les journaux de transactions, correspondant au <command>DROP
     TABLE</command> est appliqué sur le serveur en attente. Si cette situation
    survient sur le serveur primaire, l'instruction <command>DROP TABLE</command>
    attendra jusqu'à ce que l'autre requête se termine. Par contre, quand le
    <command>DROP TABLE</command> est exécuté sur le serveur primaire, ce dernier
    ne sait pas les requêtes en cours d'exécution sur le serveur en attente,
    donc il n'attendra pas la fin de l'exécution des requêtes sur le serveur
    en attente. L'enregistrement de cette modification dans les journaux de
    transactions arrivera au serveur en attente alors que la requête sur le
    serveur en attente est toujours en cours d'exécution, causant un conflit.
    Le serveur en attente doit soit retarder l'application des enregistrements
    des journaux de transactions (et tous ceux qui sont après aussi) soit
    annuler la requête en conflit, pour appliquer l'instruction <command>DROP
     TABLE</command>.
   </para>

   <para>
    Quand une requête en conflit est courte, il est généralement préférable
    d'attendre un peu pour l'application du journal de transactions. Mais un
    délai plus long n'est généralement pas souhaitable. Donc, le mécanisme
    d'annulation dans l'application des enregistrements de journaux de
    transactions dispose de deux paramètres, <xref
    linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/>, qui définissent le délai
    maximum autorisé pour appliquer les enregistrements. Les requêtes en
    conflit seront annulées si l'application des enregistrements prend plus de
    temps que celui défini. Il existe deux paramètres pour que des délais
    différents puissent être observés suivant le cas&nbsp;: lecture des
    enregistrements à partir d'un journal archivé (par exemple lors de la
    restauration initiale à partir d'une sauvegarde ou lors d'un
    <quote>rattrapage</quote> si le serveur en attente accumulait du retard
    par rapport au primaire) et lecture des enregistrements à partir de la
    réplication en flux.
   </para>

   <para>
    Pour un serveur en attente dont le but principal est la haute-disponibilité,
    il est préférable de configurer des valeurs assez basses pour les
    paramètres de délai, de façon à ce que le serveur en attente ne soit pas
    trop en retard par rapport au serveur primaire à cause des délais suivis à
    cause des requêtes exécutées sur le serveur en attente. Par contre, si
    le serveur en attente doit exécuter des requêtes longues, alors une valeur
    haute, voire infinie, du délai pourrait être préférable. Néanmoins, gardez
    en tête qu'une requête mettant du temps à s'exécuter pourrait empêcher
    les autres requêtes de voir les modifications récentes sur le serveur
    primaire si elle retarde l'application des enregistrements de journaux
    de transactions.
   </para>

   <para>
    Une fois que le délai spécifié par
    <varname>max_standby_archive_delay</varname> ou
    <varname>max_standby_streaming_delay</varname> a été dépassé, toutes les
    requêtes en conflit seront annulées. Ceci résulte habituellement en une
    erreur d'annulation, bien que certains cas, comme un <command>DROP
     DATABASE</command>, peuvent occasionner l'arrêt complet de la connexion.
    De plus, si le conflit intervient sur un verrou détenu par une transaction
    en attente, la session en conflit sera terminée (ce comportement pourrait
    changer dans le futur).
   </para>

   <para>
    Les requêtes annulées peuvent être ré-exécutées immédiatement (après avoir
    commencé une nouvelle transaction, bien sûr). Comme l'annulation des
    requêtes dépend de la nature des enregistrements dans le journal de
    transactions, une requête annulée pourrait très bien réussir si elle est
    de nouveau exécutée.
   </para>

   <para>
    Gardez en tête que les paramètres de délai sont comparés au temps passé
    depuis que la donnée du journal de transactions a été reçue par le serveur
    en attente. Du coup, la période de grâce accordée aux requêtes n'est jamais
    supérieure au paramètre de délai, et peut être considérablement inférieure
    si le serveur en attente est déjà en retard suite à l'attente de la fin
    de l'exécution de requêtes précédentes ou suite à son impossibilité de
    conserver le rythme d'une grosse mise à jour.
   </para>

   <para>
    La raison la plus fréquente des conflits entre les requêtes en lecture
    seule et le rejeu des journaux de transactions est le <quote>nettoyage
     avancé</quote>. Habituellement, <productname>PostgreSQL</productname>
    permet le nettoyage des anciennes versions de lignes quand aucune
    transaction ne peut les voir pour s'assurer du respect des règles de MVCC.
    Néanmoins, cette règle peut seulement s'appliquer sur les transactions
    exécutées sur le serveur primaire. Donc il est possible que le nettoyage
    effectué sur le primaire supprime des versions de lignes toujours visibles
    sur une transaction exécutée sur le serveur en attente.
   </para>

   <para>
    Les utilisateurs expérimentés peuvent noter que le nettoyage des versions
    de ligne ainsi que le gel des versions de ligne peuvent potentiellement
    avoir un conflit avec les requêtes exécutées sur le serveur en attente.
    L'exécution d'un <command>VACUUM FREEZE</command> manuel a de grandes
    chances de causer des conflits, y compris sur les tables sans lignes mises
    à jour ou supprimées.
   </para>

   <para>
    Les utilisateurs doivent s'attendre à ce que les tables fréquemment mises
    à jour sur le serveur primaire seront aussi fréquemment la cause de
    requêtes annulées sur le serveur en attente. Dans un tel cas, le
    paramétrage d'une valeur finie pour
    <varname>max_standby_archive_delay</varname> ou
    <varname>max_standby_streaming_delay</varname> peut être considéré comme
    similaire à la configuration de <varname>statement_timeout</varname>.
   </para>

   <para>
    Si le nombre d'annulations de requêtes sur le serveur en attente est
    jugé inadmissible, quelques solutions existent. La première option est de définir la variable
    <varname>hot_standby_feedback</varname> qui permet d'empêcher les conflits liés au nettoyage
    opéré par la commande <command>VACUUM</command> en lui interdisant de nettoyer les lignes récemment supprimées. Si
    vous le faites, vous devez noter que cela retardera le nettoyage des
    versions de lignes mortes sur le serveur primaire, ce qui pourrait résulter
    en une fragmentation non désirée de la table. Néanmoins, cette situation
    ne sera pas meilleure si les requêtes du serveur en attente s'exécutaient
    directement sur le serveur primaire. Vous avez toujours le bénéfice de
    l'exécution sur un serveur distant. Si des serveurs secondaires se
    connectent et se déconnectent fréquemment, vous pourriez vouloir faire des
    ajustements pour gérer la période durant laquelle <varname>hot_standby_feedback</varname>
    n'est pas renvoyé. Par exemple, vous pouvez considérer l'augmentation de
    <varname>max_standby_archive_delay</varname> pour que les requêtes ne
    soient pas annulées rapidement par des conflits avec le journal de transactions
    d'archive durant les périodes de déconnexion.  Vous pouvez également
    considérer l'augmentation de <varname>max_standby_streaming_delay</varname>
    pour éviter des annulations rapides par les nouvelles données de flux de
    transaction après la reconnexion.
   </para>

   <para>
    Une autre option revient à augmenter <xref
    linkend="guc-vacuum-defer-cleanup-age"/> sur le serveur primaire, pour que
    les lignes mortes ne soient pas nettoyées aussi rapidement que d'habitude.
    Cela donnera plus de temps aux requêtes pour s'exécuter avant d'être
    annulées sur le serveur en attente, sans voir à configurer une valeur
    importante de <varname>max_standby_streaming_delay</varname>. Néanmoins,
    il est difficile de garantir une fenêtre spécifique de temps d'exécution
    avec cette approche car <varname>vacuum_defer_cleanup_age</varname> est
    mesuré en nombre de transactions sur le serveur primaire.
   </para>

   <para>
    Le nombre de requêtes annulées et le motif de cette annulation peut être visualisé avec
    la vue système <structname>pg_stat_database_conflicts</structname> sur le serveur
    secondaire. La vue système <structname>pg_stat_database</structname> contient aussi
    des informations synthétiques sur ce sujet.
   </para>

   <para>
    Les utilisateurs peuvent contrôler si un message doit être écrit dans les
    traces du serveur lorsque le rejeu des journaux de transactions est en
    conflit depuis plus longtemps que <varname>deadlock_timeout</varname>.
    Ce comportement est contrôlé par le paramètre
    <xref linkend="guc-log-recovery-conflict-waits"/>.
   </para>
  </sect2>

  <sect2 id="hot-standby-admin">
   <title>Aperçu pour l'administrateur</title>

   <para>
    Si <varname>hot_standby</varname> est positionné à <literal>on</literal>
    dans <filename>postgresql.conf</filename> (valeur par défaut) et qu'un
    fichier <link
    linkend="file-standby-signal"><filename>standby.signal</filename></link><indexterm><primary>standby.signal</primary><secondary>pour
    un hot standby</secondary></indexterm> est présent, le serveur
    fonctionnera en mode Hot Standby.
    Toutefois, il pourrait s'écouler du temps avant que les connections en
    Hot Standby soient autorisées, parce que le serveur n'acceptera pas de connexions tant
    que la récupération n'aura pas atteint un point garantissant un état cohérent permettant
    aux requêtes de s'exécuter. Pendant cette période, les clients qui tentent de se connecter
    seront rejetés avec un message d'erreur.
    Pour confirmer que le serveur a démarré, vous pouvez soit tenter de vous connecter en
    boucle, ou rechercher ces messages dans les journaux du serveur:

    <programlisting>
LOG:  entering standby mode

... puis, plus loin ...

LOG:  consistent recovery state reached
LOG:  database system is ready to accept read-only connections
    </programlisting>

    L'information sur la cohérence est enregistrée une fois par checkpoint sur le primaire.
    Il n'est pas possible d'activer le mode Hot Standby si on lit des WAL générés durant
    une période pendant laquelle <varname>wal_level</varname> n'était pas positionné
    à <literal>replica</literal> ou <literal>logical</literal> sur le primaire. L'arrivée à un état cohérent
    peut aussi être retardée si ces deux conditions se présentent:

    <itemizedlist>
     <listitem>
      <para>
       Une transaction en écriture a plus de 64 sous-transactions
      </para>
     </listitem>
     <listitem>
      <para>
       Des transactions en écriture ont une durée très importante
      </para>
     </listitem>
    </itemizedlist>

    Si vous effectuez du log shipping par fichier (<foreignphrase>warm standby</foreignphrase>), vous pourriez
    devoir attendre jusqu'à l'arrivée du prochain fichier de WAL, ce qui pourrait
    être aussi long que le paramètre <varname>archive_timeout</varname> du primaire.
   </para>

   <para>
    Certains paramètres déterminent la taille de la mémoire partagée pour le
    suivi des identifiants de transaction, des verrous, et des transactions
    préparées. Ces structures mémoires partagées ne peuvent pas être plus
    petites sur le secondaire que sur le primaire pour s'assurer que le secondaire ne
    tombera pas à court de mémoire partagée pendant la récupération. Par
    exemple, si le primaire avait utilisé des transactions préparées mais que le
    secondaire n'avait pas alloué de mémoire partagée pour suivre ces transactions
    préparées, alors la récupération ne pourrait reprendre avant que la
    configuration du secondaire ne soit adaptée. Les paramètres concernés sont:

    <itemizedlist>
     <listitem>
      <para>
       <varname>max_connections</varname>
      </para>
     </listitem>
     <listitem>
      <para>
       <varname>max_prepared_transactions</varname>
      </para>
     </listitem>
     <listitem>
      <para>
       <varname>max_locks_per_transaction</varname>
      </para>
     </listitem>
     <listitem>
      <para>
       <varname>max_wal_senders</varname>
      </para>
     </listitem>
     <listitem>
      <para>
       <varname>max_worker_processes</varname>
      </para>
     </listitem>
    </itemizedlist>

    La façon la plus simple pour éviter que cela ne devienne un problème est
    d'avoir tous ces paramètres définis sur les serveurs secondaires à des valeurs
    égales ou supérieures que celles du primaire. C'est pourquoi, si vous voulez
    augmenter ces valeurs, vous devez le faire d'abord sur tous les serveurs
    secondaires avant de le faire sur le serveur primaire. De la même façon, si
    vous voulez diminuer ces valeurs, vous devez d'abord le faire sur le serveur
    primaire, puis sur tous les serveurs secondaires. Gardez également à l'esprit
    que lorsqu'un secondaire est promu, il devient alors la nouvelle référence des
    valeurs de ces paramètres pour les secondaires qui s'y raccrochent. De ce fait,
    pour éviter que cela ne devienne un problème lors d'une bascule
    (<foreignphrase>switchover</foreignphrase> ou
    <foreignphrase>failover</foreignphrase>), il est recommandé de conserver ces
    paramètres identiques sur tous les serveurs secondaires.
   </para>

   <para>
    Les journaux de transactions tracent le changement de ces paramètres sur le
    serveur primaire. Si un serveur hot standby rejoue un journal de
    transactions qui indique que la valeur actuelle sur le primaire est plus
    élevée que la sienne, un message d'avertissement sera écrit dans les traces
    du serveur et le rejeu mis en pause. Par exemple :
<screen>
WARNING:  hot standby is not possible because of insufficient parameter settings
DETAIL:  max_connections = 80 is a lower setting than on the primary server, where its value was 100.
LOG:  recovery has paused
DETAIL:  If recovery is unpaused, the server will shut down.
HINT:  You can then restart the server after making the necessary configuration changes.
</screen>
    A ce stade, les paramètres du secondaire doivent être ajustés et l'instance
    redémarrée avant que le rejeu ne puisse reprendre. Si le secondaire n'est pas
    en mode hot standby alors, lorsqu'il rencontrera un changement de paramètre
    incompatible, il s'éteindra immédiatement sans pause, puisqu'il serait
    inutile de rester actif.
   </para>

   <para>
    Il est important que l'administrateur sélectionne le paramétrage approprié
    pour <xref linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/>. Le meilleur choix varie les
    priorités. Par exemple, si le serveur a comme tâche principale d'être un
    serveur de haute-disponibilité, alors il est préférable d'avoir une
    configuration assez basse, voire à zéro, de ces paramètres. Si le serveur
    en attente est utilisé comme serveur supplémentaire pour des requêtes du
    type décisionnel, il sera acceptable de mettre les paramètres de délai à
    des valeurs allant jusqu'à plusieurs heures, voire même -1 (cette valeur
    signifiant qu'il est possible d'attendre que les requêtes se terminent
    d'elles-même).
   </para>

   <para>
    Les "hint bits" (bits d'indices) écrits sur le primaire ne sont pas journalisés en WAL,
    il est donc probable que les hint bits soient réécrits sur le secondaire. Ainsi,
    le serveur secondaire fera toujours des écritures disques même si tous les utilisateurs
    sont en lecture seule&nbsp;; aucun changement ne se produira sur les données elles mêmes.
    Les utilisateurs écriront toujours les fichiers temporaires pour les gros tris et
    re-génèreront les fichiers d'information relcache, il n'y a donc pas de morceau de la base
    qui soit réellement en lecture seule en mode hot standby.
    Notez aussi que les écritures dans des bases distantes en utilisant le module
    <application>dblink</application>, et d'autres opérations en dehors de la base s'appuyant sur
    des fonctions PL seront toujours possibles, même si la transaction est en lecture seule localement.
   </para>

   <para>
    Les types suivants de commandes administratives ne sont pas acceptées
    durant le mode de récupération:

    <itemizedlist>
     <listitem>
      <para>
       Langage de Définition de Données (LDD ou DDL)&nbsp;: comme <command>CREATE INDEX</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Droits et propriété&nbsp;: <command>GRANT</command>, <command>REVOKE</command>,
       <command>REASSIGN</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de maintenance&nbsp;: <command>ANALYZE</command>, <command>VACUUM</command>,
       <command>CLUSTER</command>, <command>REINDEX</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Notez encore une fois que certaines de ces commandes sont en fait
    autorisées durant les transactions en "lecture seule" sur le primaire.
   </para>

   <para>
    Par conséquent, vous ne pouvez pas créer d'index supplémentaires qui existeraient
    uniquement sur le secondaire, ni des statistiques qui n'existeraient que sur le secondaire.
    Si ces commandes administratives sont nécessaires, elles doivent être exécutées
    sur le primaire, et ces modifications se propageront à terme au secondaire.
   </para>

   <para>
    <function>pg_cancel_backend()</function> et
    <function>pg_terminate_backend()</function> fonctionneront sur les
    processus utilisateurs, mais pas sur les processus de démarrage, qui
    effectuent la récupération. <structname>pg_stat_activity</structname> ne
    montre pas les transactions de récupération comme actives. Ainsi,
    <structname>pg_prepared_xacts</structname> est toujours vide durant la
    récupération. Si vous voulez traiter des transactions préparées douteuses,
    interrogez <structname>pg_prepared_xacts</structname> sur le primaire, et
    exécutez les commandes pour résoudre le problème à cet endroit ou
    résolvez-les après la fin de la restauration.
   </para>

   <para>
    <structname>pg_locks</structname> affichera les verrous posés par les processus,
    comme en temps normal. <structname>pg_locks</structname> affiche aussi une transaction
    virtuelle gérée par le processus de démarrage qui possède tous les
    <literal>AccessExclusiveLocks</literal> posés par les transactions rejouées par la récupération.
    Notez que le processus de démarrage n'acquiert pas de verrou pour effectuer les modifications à
    la base, et que par conséquent les verrous autre que <literal>AccessExclusiveLocks</literal>
    ne sont pas visibles dans <structname>pg_locks</structname> pour le processus de démarrage;
    ils sont simplement censés exister.
   </para>

   <para>
    Le plugin <productname>Nagios</productname> <productname>check_pgsql</productname> fonctionnera,
    parce que les informations simples qu'il vérifie existent.
    Le script de supervision <productname>check_postgres</productname> fonctionnera aussi,
    même si certaines valeurs retournées pourraient être différentes ou sujettes à confusion.
    Par exemple, la date de dernier vacuum ne sera pas mise à jour, puisqu’aucun vacuum ne se déclenche
    sur le secondaire. Les vacuums s'exécutant sur le primaire envoient toujours leurs modifications
    au secondaire.
   </para>

   <para>
    Les options de contrôle des fichiers de WAL ne fonctionneront pas durant la récupération,
    comme <function>pg_backup_start</function>, <function>pg_switch_wal</function>, etc...
   </para>

   <para>
    Les modules à chargement dynamique fonctionnent, comme <structname>pg_stat_statements</structname>.
   </para>

   <para>
    Les verrous consultatifs fonctionnent normalement durant la récupération,
    y compris en ce qui concerne la détection des verrous mortels (deadlocks).
    Notez que les verrous consultatifs ne sont jamais tracés dans les WAL, il est
    donc impossible pour un verrou consultatif sur le primaire ou le secondaire
    d'être en conflit avec la ré-application des WAL. Pas plus qu'il n'est
    possible d'acquérir un verrou consultatif sur le primaire et que celui-ci
    initie un verrou consultatif similaire sur le secondaire. Les verrous consultatifs
    n'ont de sens que sur le serveur sur lequel ils sont acquis.
   </para>

   <para>
    Les systèmes de réplications à base de triggers tels que <productname>Slony</productname>,
    <productname>Londiste</productname> et <productname>Bucardo</productname>
    ne fonctionneront pas sur le secondaire du tout, même s'ils fonctionneront sans problème
    sur le serveur primaire tant que les modifications ne sont pas envoyées sur le serveur secondaire
    pour y être appliquées. Le rejeu de WAL n'est pas à base de triggers, vous ne pouvez
    donc pas utiliser le secondaire comme relais vers un système qui aurait besoin d'écritures supplémentaires
    ou utilise des triggers.
   </para>

   <para>
    Il n'est pas possible d'assigner de nouveaux OID, bien que des générateurs d'<acronym>UUID</acronym>
    puissent tout de même fonctionner, tant qu'ils n'ont pas besoin d'écrire un nouveau statut dans
    la base.
   </para>

   <para>
    À l'heure actuelle, la création de table temporaire n'est pas autorisée durant les
    transactions en lecture seule, certains scripts existants pourraient donc
    ne pas fonctionner correctement. Cette restriction pourrait être levée dans une
    version ultérieure. Il s'agit à la fois d'un problème de respect des standards
    et d'un problème technique.
   </para>

   <para>
    <command>DROP TABLESPACE</command> ne peut réussir que si le tablespace est vide.
    Certains utilisateurs pourraient utiliser de façon active le tablespace via leur
    paramètre <varname>temp_tablespaces</varname>. S'il y a des fichiers temporaires
    dans le tablespace, toutes les requêtes actives sont annulées pour s'assurer que les
    fichiers temporaires sont supprimés, afin de supprimer le tablespace et de continuer
    l'application des WAL.
   </para>

   <para>
    Exécuter <command>DROP DATABASE</command> ou <command>ALTER DATABASE ...
     SET TABLESPACE</command> sur
    le serveur primaire générera un enregistrement dans les journaux de
    transactions qui causera la déconnexion de tous les utilisateurs
    actuellement connectés à cette base de données. Cette action survient
    immédiatement, quelque soit la valeur du paramètre
    <varname>max_standby_streaming_delay</varname>. Notez que
    <command>ALTER DATABASE ... RENAME</command> ne déconnecte pas les
    utilisateurs qui, dans la plupart des cas, ne s'en apercevront pas. Cela
    peut néanmoins semer de la confusion pour un programme qui dépendrait du nom de la base.
   </para>

   <para>
    En fonctionnement normal (pas en restauration), si vous exécutez
    <command>DROP USER</command> ou <command>DROP ROLE</command>
    pour un rôle ayant l'attribut LOGIN alors que cet utilisateur est toujours
    connecté alors rien ne se produit pour cet utilisateur connecté &mdash; il reste connecté. L'utilisateur
    ne peut toutefois pas se reconnecter. Ce comportement est le même en récupération, un
    <command>DROP USER</command> sur le primaire ne déconnecte donc pas cet utilisateur sur le secondaire.
   </para>

   <para>
    Le système de statistiques cumulatives est actif pendant la restauration.
    Tous les parcours, lectures, blocs, utilisations d'index, etc. seront
    enregistrés normalement sur le standby. Néanmoins, le rejeu des journaux de
    transactions n'incrémentera pas les compteurs spécifiques des relations et
    des bases. Le rejeu n'incrémentera pas les colonnes de pg_stat_all_tables
    (comme n_tup_ins), pas plus que les lectures et écritures réalisées par le
    processus de démarrage ne seront tracées dans les vues pg_statio, ou que les
    colonnes associées de pg_stat_database ne seront incrémentées.
   </para>

   <para>
    Autovacuum n'est pas actif durant la récupération, il démarrera normalement
    à la fin de la récupération.
   </para>

   <para>
    Les processus d'écriture en arrière plan (checkpointer et background
    writer) sont actifs durant la restauration. Le processus checkpointer
    process effectuera les restartpoints (similaires aux checkpoints sur le
    primaire) et le processus background writer réalisera les activités
    normales de nettoyage de blocs. Ceci peut inclure la mise à jour des
    information de hint bit des données du serveur secondaire. La commande
    <command>CHECKPOINT</command> est acceptée pendant la récupération, bien
    qu'elle déclenche un restartpoint et non un checkpoint.
   </para>
  </sect2>

  <sect2 id="hot-standby-parameters">
   <title>Référence des paramètres de Hot Standby</title>

   <para>
    De nombreux paramètres ont été mentionnés ci-dessus dans
    <xref linkend="hot-standby-conflict"/>
    et <xref linkend="hot-standby-admin"/>.
   </para>

   <para>
    Sur le primaire, les paramètres <xref linkend="guc-wal-level"/> et
    <xref linkend="guc-vacuum-defer-cleanup-age"/> peuvent être utilisés.
    <xref linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/> n'ont aucun effet sur le primaire.
   </para>

   <para>
    Sur le serveur en attente, les paramètres <xref linkend="guc-hot-standby"/>,
    <xref linkend="guc-max-standby-archive-delay"/> et
    <xref linkend="guc-max-standby-streaming-delay"/> peuvent être utilisés.
    <xref linkend="guc-vacuum-defer-cleanup-age"/> n'a pas d'effet tant que
    le serveur reste dans le mode standby, mais deviendra important quand le
    serveur en attente deviendra un serveur primaire.
   </para>
  </sect2>

  <sect2 id="hot-standby-caveats">
   <title>Avertissements</title>

   <para>
    Il y a plusieurs limitations au Hot Standby.
    Elles peuvent et seront probablement résolues dans des versions ultérieures:

    <itemizedlist>
     <listitem>
      <para>
       Une connaissance complète des transactions en cours d'exécution est nécessaire
       avant de pouvoir déclencher des instantanés. Des transactions utilisant un
       grand nombre de sous-transactions (à l'heure actuelle plus de 64) retarderont
       le démarrage des connexions en lecture seule jusqu'à complétion de la plus
       longue transaction en écriture. Si cette situation se produit, des messages
       explicatifs seront envoyés dans la trace du serveur.
      </para>
     </listitem>
     <listitem>
      <para>
       Des points de démarrage valides pour les requêtes du secondaire sont générés
       à chaque checkpoint sur le primaire. Si le secondaire est éteint alors
       que le primaire est déjà éteint, il est tout à fait possible ne pas pouvoir
       repasser en Hot Standby tant que le primaire n'aura pas été redémarré, afin
       qu'il génère de nouveaux points de démarrage dans les journaux WAL. Cette situation
       n'est pas un problème dans la plupart des situations où cela pourrait se produire.
       Généralement, si le primaire est éteint et plus disponible, c'est probablement
       en raison d'un problème sérieux qui va de toutes façons forcer la conversion
       du secondaire en primaire. Et dans des situations où le primaire est éteint
       intentionnellement, la procédure standard est également de promouvoir le secondaire.
      </para>
     </listitem>
     <listitem>
      <para>
       À la fin de la récupération, les <literal>AccessExclusiveLocks</literal> possédés
       par des transactions préparées nécessiteront deux fois le nombre d'entrées normal dans la
       table de verrous. Si vous pensez soit exécuter un grand nombre de transactions préparées
       prenant des <literal>AccessExclusiveLocks</literal>, ou une grosse transaction prenant
       beaucoup de <literal>AccessExclusiveLocks</literal>, il est conseillé d'augmenter la valeur
       de <varname>max_locks_per_transaction</varname>, peut-être jusqu'à une valeur double
       de celle du serveur primaire. Vous n'avez pas besoin de prendre ceci en compte
       si votre paramètre <varname>max_prepared_transactions</varname> est 0.
      </para>
     </listitem>
     <listitem>
      <para>
       Il n'est pas encore possible de passer une transaction en mode d'isolation sérialisable
       tout en supportant le hot standby (voir <xref linkend="xact-serializable"/> et
       <xref linkend="serializable-consistency"/> pour plus de détails).
       Une tentative de modification du niveau d'isolation d'une transaction à sérialisable
       en hot standby générera une erreur.
      </para>
     </listitem>
    </itemizedlist>
   </para>
  </sect2>

 </sect1>

</chapter>
