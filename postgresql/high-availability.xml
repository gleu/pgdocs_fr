<?xml version="1.0" encoding="UTF-8"?>
<chapter id="high-availability">
 <title>Haute disponibilité, répartition de charge et réplication</title>

 <indexterm><primary>haute disponibilité</primary></indexterm>
 <indexterm><primary>failover</primary></indexterm>
 <indexterm><primary>réplication</primary></indexterm>
 <indexterm><primary>répartition de charge</primary></indexterm>
 <indexterm><primary>clustering</primary></indexterm>
 <indexterm><primary>partitionnement de données</primary></indexterm>

 <para>
  Des serveurs de bases de données peuvent travailler ensemble pour permettre
  à un serveur standby de prendre rapidement la main si le serveur principal
  échoue (haute disponibilité, ou <foreignphrase>high availability</foreignphrase>),
  ou pour permettre à plusieurs serveurs de servir les mêmes données (répartition
  de charge, ou <foreignphrase>load balancing</foreignphrase>). Idéalement, les
  serveurs de bases de données peuvent travailler ensemble sans jointure.
 </para>

 <para>
  Il est aisé de faire coopérer des serveurs web qui traitent des pages web statiques
  en répartissant la charge des requêtes web sur plusieurs
  machines. Dans les faits, les serveurs de bases de données en lecture seule peuvent
  également coopérer facilement. Malheureusement, la plupart des
  serveurs de bases de données traitent des requêtes de lecture/écriture et,
  de ce fait, collaborent plus difficilement. En effet, alors qu'il suffit de
  placer une seule fois les données en lecture seule sur chaque serveur, une
  écriture sur n'importe quel serveur doit, elle, être propagée à tous les
  serveurs afin que les lectures suivantes sur ces serveurs renvoient des résultats
  cohérents.
 </para>

 <para>
  Ce problème de synchronisation représente la difficulté fondamentale à la
  collaboration entre serveurs. Comme la solution au problème de
  synchronisation n'est pas unique pour tous les cas pratiques, plusieurs
  solutions co-existent. Chacune répond de façon différente et minimise
  cet impact au regard d'une charge spécifique.
 </para>

 <para>
  Certaines solutions gèrent la synchronisation en autorisant les modifications
  des données sur un seul serveur. Les serveurs qui peuvent modifier les données
  sont appelés serveur en lecture/écriture, <firstterm>primaire</firstterm> ou serveur <firstterm>primaire</firstterm>.
  Les serveurs qui suivent les modifications du primaire sont appelés <firstterm>standby</firstterm>,
  ou serveurs <firstterm>standbys</firstterm>, ou encore serveurs <firstterm>secondaires</firstterm>. Un serveur en standby auquel on ne peut pas
  se connecter tant qu'il n'a pas été promu en serveur primaire est appelé un serveur en <firstterm>warm
   standby</firstterm>, et un qui peut accepter des connections et répondre à des requêtes en
  lecture seule est appelé un serveur en <firstterm>hot standby</firstterm>.
 </para>

 <para>
  Certaines solutions sont synchrones, ce qui signifie qu'une transaction de
  modification de données n'est pas considérée valide tant que tous les
  serveurs n'ont pas validé la transaction. Ceci garantit qu'un
  <foreignphrase>failover</foreignphrase> ne perd pas de données et que tous
  les serveurs en répartition de charge retournent des résultats cohérents, quel
  que soit le serveur interrogé. Au contraire, les solutions asynchrones
  autorisent un délai entre la validation et sa propagation aux
  autres serveurs. Cette solution implique une éventuelle perte de transactions
  lors de la bascule sur un serveur de sauvegarde, ou l'envoi de données
  obsolètes par les serveurs à charge répartie. La communication asynchrone est
  utilisée lorsque la version synchrone est trop lente.
 </para>

 <para>
  Les solutions peuvent aussi être catégorisées par leur granularité. Certaines
  ne gèrent que la totalité d'un serveur de bases alors que
  d'autres autorisent un contrôle par table ou par base.
 </para>

 <para>
  Il importe de considérer les performances dans tout choix. Il y
  a généralement un compromis à trouver entre les fonctionnalités et les
  performances. Par exemple, une solution complètement synchrone sur un réseau
  lent peut diviser les performances par plus de deux, alors qu'une
  solution asynchrone peut n'avoir qu'un impact minimal sur les performances.
 </para>

 <para>
  Le reste de cette section souligne différentes solutions de
  <foreignphrase>failover</foreignphrase>, de réplication et de répartition de
  charge.
 </para>

 <sect1 id="different-replication-solutions">
  <title>Comparaison de différentes solutions</title>

  <variablelist>

   <varlistentry>
    <term><foreignphrase>Failover</foreignphrase> sur disque partagé</term>
    <listitem>

     <para>
      Le <foreignphrase>failover</foreignphrase> (ou bascule sur incident)
      sur disque partagé élimine la surcharge de synchronisation par
      l'existence d'une seule copie de la base de données. Il utilise un
      seul ensemble de disques partagé par plusieurs serveurs. Si le serveur
      principal échoue, le serveur en attente
      est capable de monter et démarrer la base comme s'il récupérait d'un
      arrêt brutal. Cela permet un <foreignphrase>failover</foreignphrase>
      rapide sans perte de données.
     </para>

     <para>
      La fonctionnalité de matériel partagé est commune aux périphériques de
      stockage en réseau. Il est également possible d'utiliser un système de
      fichiers réseau bien qu'il faille porter une grande attention au système de
      fichiers pour s'assurer qu'il a un comportement <acronym>POSIX</acronym>
      complet (voir <xref linkend="creating-cluster-nfs"/>). Cette méthode
      comporte une limitation significative&nbsp;: si les disques ont un
      problème ou sont corrompus, le serveur primaire et le serveur en attente sont tous
      les deux non fonctionnels. Un autre problème est que le serveur en attente
      ne devra jamais accéder au stockage partagé tant que le serveur principal
      est en cours d'exécution.
     </para>

    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Réplication de système de fichiers (périphérique bloc)</term>
    <listitem>

     <para>
      Il est aussi possible d'utiliser cette fonctionnalité d'une autre façon
      avec une réplication du système de fichiers, où toutes les modifications
      d'un système de fichiers sont renvoyées sur un système de fichiers situé
      sur un autre ordinateur. La seule restriction est que ce miroir doit être
      construit de telle sorte que le serveur en attente dispose d'une
      version cohérente du système de fichiers &mdash; spécifiquement, les
      écritures sur le serveur en attente doivent être réalisées dans le même
      ordre que celles sur le primaire. <productname>DRBD</productname> est une
      solution populaire de réplication de systèmes de fichiers pour Linux.
     </para>

     <!--
https://forge.continuent.org/pipermail/sequoia/2006-November/004070.html
La technologie Oracle RAC est une approche par disques partagés et renvoie
aux autres nœuds uniquement les annulations de niveau cache mais pas
réellement au niveau des données (physiques).
Puisque les disques sont partagés, les données sont validées une seule
fois en s'appuyant sur un protocole de verrouillage distribué, de façon à
ce que les nœuds s'accordent dans un système transactionnel sérialisable.


-->

    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Envoi des journaux de transactions</term>
    <listitem>

     <para>
      Les serveurs <foreignphrase>warm et hot standby</foreignphrase> (voir <xref
      linkend="warm-standby"/>) peuvent conserver leur cohérence en lisant un flux
      d'enregistrements de <acronym>WAL</acronym>. Si le serveur principal
      échoue, le serveur
      <foreignphrase>standby</foreignphrase> contient pratiquement toutes
      les données du serveur principal et peut rapidement devenir le nouveau
      serveur primaire. Ça peut être synchrone mais ça ne peut se faire que pour le
      serveur de bases complet.
     </para>
     <para>
      Un serveur de standby peut être implémenté en utilisant la recopie de journaux par fichier
      (<xref linkend="warm-standby"/>)  ou la streaming replication (réplication en continu, voir
      <xref linkend="streaming-replication"/>), ou une combinaison des deux. Pour
      des informations sur le hot standby, voyez <xref linkend="hot-standby"/>..
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Réplication logique</term>
    <listitem>
     <para>
      La réplication logique autorise un serveur de bases de données d'envoyer
      un flux de modifications de données à un autre serveur. La réplication
      logique de <productname>PostgreSQL</productname> construit un flux de
      modifications logiques de données à partir des journaux de transactions.
      La réplication logique permet la réplication des modifications de
      données de tables individuelles. La réplication logique ne requiert pas
      qu'un serveur particulier soit désigné comme serveur primaire ou
      secondaire, mais autorise le flux de données dans plusieurs directions.
      Pour plus d'informations sur la réplication logique, voir <xref
      linkend="logical-replication"/>. Au travers de l'interface de décodage
      logique (<xref linkend="logicaldecoding"/>), les extensions tierces
      peuvent aussi fournir des fonctionnalités similaires.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Réplication primaire/secondaire basé sur des triggers</term>
    <listitem>

     <para>
      Une configuration de réplication primaire/secondaire envoie toutes les requêtes
      de modification de données au serveur primaire. Ce serveur envoie les
      modifications de données de façon asynchrone au serveur secondaire. Le secondaire
      peut répondre aux requêtes en lecture seule alors que le serveur primaire
      est en cours d'exécution. Le serveur secondaire est idéal pour les requêtes
      vers un entrepôt de données.
     </para>

     <para>
      <productname>Slony-I</productname> est un exemple de ce type de
      réplication, avec une granularité par
      table et un support des secondaires multiples. Comme il met à jour le serveur
      secondaire de façon asynchrone (par lots), il existe une possibilité de perte
      de données pendant un <foreignphrase>failover</foreignphrase>.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term><foreignphrase>Middleware</foreignphrase> de réplication basé sur le
     SQL</term>
    <listitem>

     <para>
      Avec les <foreignphrase>middleware</foreignphrase> de réplication basés
      sur le SQL, un programme intercepte chaque requête SQL et
      l'envoie à un ou tous les serveurs. Chaque serveur opère indépendamment.
      Les requêtes en lecture/écriture doivent être envoyées à tous les
      serveurs pour que chaque serveur reçoive les modifications. Les
      requêtes en lecture seule ne peuvent être envoyées qu'à un seul
      serveur, ce qui permet de distribuer la charge de lecture.
     </para>

     <para>
      Si les requêtes sont envoyées sans modification, les fonctions comme
      <function>random()</function>, <function>CURRENT_TIMESTAMP</function> ainsi
      que les séquences ont des valeurs différentes sur les différents serveurs.
      Cela parce que chaque serveur opère indépendamment alors que
      les requêtes SQL sont diffusées (et non les données
      modifiées). Si cette solution est inacceptable, le
      <foreignphrase>middleware</foreignphrase> ou l'application doivent
      demander ces valeurs à un seul serveur, et les utiliser dans
      des requêtes d'écriture. Une autre solution est d'utiliser cette solution de réplication
      avec une configuration primaire-secondaire traditionnelle, c'est à dire que les requêtes
      de modification de données ne sont envoyées qu'au primaire et sont propagées aux
      secondaires via une réplication primaire-secondaire, pas par le middleware de
      réplication.  Il est impératif que
      toute transaction soit validée ou annulée sur tous les serveurs,
      éventuellement par validation en deux phases (<xref
      linkend="sql-prepare-transaction"/> et <xref linkend="sql-commit-prepared"/>).
      <productname>Pgpool-II</productname> et <productname>Continuent
       Tungsten</productname> sont des exemples de ce type de réplication.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Réplication asynchrone multi-primaires</term>
    <listitem>

     <para>
      Pour les serveurs qui ne sont pas connectés en permanence ou qui ont des
      liens de communication lents, comme les ordinateurs portables ou les
      serveurs distants, conserver la cohérence des données
      entre les serveurs est un challenge. L'utilisation de la réplication asynchrone
      multi-primaires permet à chaque serveur de fonctionner indépendamment. Il
      communique alors périodiquement avec les autres serveurs pour identifier les transactions
      conflictuelles. La gestion des conflits est alors confiée aux utilisateurs
      ou à un système de règles de résolution.
      Bucardo est un exemple de ce type de réplication.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Réplication synchrone multi-primaires</term>
    <listitem>

     <para>
      Dans les réplications synchrones multi-primaires, tous les serveurs acceptent
      les requêtes en écriture. Les données modifiées sont transmises
      du serveur d'origine à tous les autres serveurs avant toute validation de
      transaction.
     </para>
     <para>
      Une activité importante en écriture peut être la cause d'un verrouillage
      excessif et de délai dans la validation des transactions, ce qui peut
      conduire à un effondrement des performances. Dans les faits, les
      performances en écriture sont souvent pis que celles d'un simple
      serveur.
     </para>
     <para>
      Tous les serveurs acceptent les requêtes en lecture.
     </para>
     <para>
      Certaines implantations utilisent les disques partagés pour réduire la surcharge
      de communication.
     </para>
     <para>
      Les performances de la réplication synchrone multi-primaires sont meilleures lorsque
      les opérations de lecture représentent l'essentiel de la charge, alors que
      son gros avantage est l'acceptation des requêtes d'écriture par tous les
      serveurs &mdash;
      il n'est pas nécessaire de répartir la charge entre les serveurs
      primaires et secondaires et, parce que les modifications de données sont envoyées
      d'un serveur à l'autre, les fonctions non déterministes, comme
      <function>random()</function>, ne posent aucun problème.
     </para>

     <para>
      <productname>PostgreSQL</productname> n'offre pas ce type de réplication,
      mais la validation en deux phases de <productname>PostgreSQL</productname>
      (<xref linkend="sql-prepare-transaction"/> et <xref linkend="sql-commit-prepared"/>)
      autorise son intégration dans une application ou un
      <foreignphrase>middleware</foreignphrase>.
     </para>
    </listitem>
   </varlistentry>

  </variablelist>

  <para>
   La <xref linkend="high-availability-matrix"/> résume les
   possibilités des différentes solutions listées plus-haut.
  </para>

  <table id="high-availability-matrix">
   <title>Matrice de fonctionnalités&nbsp;: haute disponibilité, répartition de
    charge et réplication</title>
   <tgroup cols="9">
    <colspec colname="col1" colwidth="1.1*"/>
    <colspec colname="col2" colwidth="1*"/>
    <colspec colname="col3" colwidth="1*"/>
    <colspec colname="col4" colwidth="1*"/>
    <colspec colname="col5" colwidth="1*"/>
    <colspec colname="col6" colwidth="1*"/>
    <colspec colname="col7" colwidth="1*"/>
    <colspec colname="col8" colwidth="1*"/>
    <colspec colname="col9" colwidth="1*"/>
    <thead>
     <row>
      <entry>Fonctionnalité</entry>
      <entry>Disques partagés</entry>
      <entry>Répl. par système de fichiers</entry>
      <entry>Envoi des journaux de transactions</entry>
      <entry>Répl. logique</entry>
      <entry>Répl. par triggers</entry>
      <entry><foreignphrase>Middleware</foreignphrase> de Répl. SQL</entry>
      <entry>Répl. asynch. MM</entry>
      <entry>Répl. synch. MM</entry>
     </row>
    </thead>

    <tbody>

     <row>
      <entry>Exemple populaires</entry>
      <entry align="center">NAS</entry>
      <entry align="center">DRBD</entry>
      <entry align="center">répl. en flux interne</entry>
      <entry align="center">répl. logique interne, pglogical</entry>
      <entry align="center">Londiste, Slony</entry>
      <entry align="center">pgpool-II</entry>
      <entry align="center">Bucardo</entry>
      <entry align="center"></entry>
     </row>

     <row>
      <entry>Méthode de comm.</entry>
      <entry align="center">Disque partagé</entry>
      <entry align="center">Blocs disque</entry>
      <entry align="center">WAL</entry>
      <entry align="center">décodage logique</entry>
      <entry align="center">Lignes de tables</entry>
      <entry align="center">SQL</entry>
      <entry align="center">Lignes de tables</entry>
      <entry align="center">Lignes de tables et verrous de ligne</entry>
     </row>

     <row>
      <entry>Ne requiert aucun matériel spécial</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
     </row>

     <row>
      <entry>Autorise plusieurs serveurs primaires</entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
     </row>

     <row>
      <entry>Pas de surcharge sur le serveur primaire</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
     </row>

     <row>
      <entry>Pas d'attente entre serveurs</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">with sync off</entry>
      <entry align="center">with sync off</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
     </row>

     <row>
      <entry>Pas de perte de données en cas de panne du primaire</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">with sync on</entry>
      <entry align="center">with sync on</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
     </row>

     <row>
      <entry>Les secondaires acceptent les requêtes en lecture seule</entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
      <entry align="center">avec un hot standby</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
     </row>

     <row>
      <entry>Granularité de niveau table</entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
     </row>

     <row>
      <entry>Ne nécessite pas de résolution de conflit</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
      <entry align="center">&bull;</entry>
      <entry align="center"></entry>
      <entry align="center">&bull;</entry>
     </row>
    </tbody>
   </tgroup>
  </table>

  <para>
   Certaines solutions n'entrent pas dans les catégories ci-dessus&nbsp;:
  </para>

  <variablelist>

   <varlistentry>
    <term>Partitionnement de données</term>
    <listitem>

     <para>
      Le partitionnement des données divise les tables en ensembles de données.
      Chaque ensemble ne peut être modifié que par un seul serveur. Les
      données peuvent ainsi être partitionnées par bureau, Londres et
      Paris, par exemple, avec un serveur dans chaque bureau. Si certaines
      requêtes doivent combiner des données de Londres et Paris, il est possible
      d'utiliser une application qui requête les deux serveurs ou d'implanter une
      réplication primaire/secondaire pour conserver sur chaque serveur une
      copie en lecture seule des données de l'autre bureau.
     </para>
    </listitem>
   </varlistentry>

   <varlistentry>
    <term>Exécution de requêtes en parallèle sur plusieurs serveurs</term>
    <listitem>

     <para>
      La plupart des solutions ci-dessus permettent à plusieurs serveurs de
      répondre à des requêtes multiples, mais aucune ne permet à une seule requête
      d'être exécutée sur plusieurs serveurs pour se terminer plus rapidement.
      Cette solution autorisent plusieurs serveurs à travailler ensemble sur une
      seule requête. Ceci s'accomplit habituellement en répartissant les données
      entre les serveurs, chaque serveur exécutant une partie de la
      requête pour renvoyer les résultats à un serveur central qui les combine
      et les renvoie à l'utilisateur. Cela peut également se faire en utilisant
      <productname>PL/Proxy</productname>.
     </para>
    </listitem>
   </varlistentry>

  </variablelist>

  <para>
   Il faut aussi noter que puisque <productname>PostgreSQL</productname>
   est un outil libre et facilement extensible, un certain nombre de sociétés
   se sont basées sur <productname>PostgreSQL</productname> pour créer leurs
   solutions propriétaires avec des possibilités spécifiques de
   <foreignphrase>failover</foreignphrase>, réplication ou répartition de charge.
   Cela ne sera toutefois pas abordé ici.
  </para>

 </sect1>

 <sect1 id="warm-standby">
  <title>Serveurs de Standby par transfert de journaux</title>

  <para>
   L'archivage en continu peut être utilisé pour créer une configuration
   de cluster en <firstterm>haute disponibilité</firstterm> (HA) avec un ou
   plusieurs <firstterm>serveurs de standby</firstterm> prêts à prendre la main
   sur les opérations si le serveur primaire fait défaut. Cette fonctionnalité
   est généralement appelée
   <firstterm>warm standby</firstterm> ou <firstterm>log shipping</firstterm>.
  </para>

  <para>
   Les serveurs primaire et de standby travaillent de concert pour fournir cette fonctionnalité,
   bien que les serveurs ne soient que faiblement couplés. Le serveur primaire opère
   en mode d'archivage en continu, tandis que le serveur de standby opère en
   mode de récupération en continu, en lisant les fichiers WAL provenant du primaire. Aucune
   modification des tables de la base ne sont requises pour activer cette fonctionnalité,
   elle entraîne donc moins de travail d'administration par rapport à d'autres
   solutions de réplication. Cette configuration a aussi un impact relativement
   faible sur les performances du serveur primaire.
  </para>

  <para>
   Déplacer directement des enregistrements de WAL d'un serveur de bases de données à un autre
   est habituellement appelé log shipping. <productname>PostgreSQL</productname>
   implémente le log shipping par fichier, ce qui signifie que les enregistrements de WAL sont
   transférés un fichier (segment de WAL) à la fois. Les fichiers de WAL (16Mo) peuvent être
   transférés facilement et de façon peu coûteuse sur n'importe quelle distance, que ce soit sur un
   système adjacent, un autre système sur le même site, ou un autre système à
   l'autre bout du globe. La bande passante requise pour cette technique
   varie en fonction du débit de transactions du serveur primaire.
   La technique de streaming replication permet d'optimiser cette bande
   passante en utilisant une granularité plus fine que le log shipping
   par fichier. Pour cela, les modifications apportées au journal de
   transactions sont traitées sous forme de flux au travers d'une
   connexion réseau (voir <xref linkend="streaming-replication"/>).
  </para>

  <para>
   Il convient de noter que le log shipping est asynchrone, c'est à dire que les
   enregistrements de WAL sont transférés après que la transaction ait été validée. Par conséquent, il y a
   un laps de temps pendant lequel une perte de données pourrait se produire si le serveur primaire
   subissait un incident majeur; les transactions pas encore transférées seront perdues. La taille de la fenêtre
   de temps de perte de données peut être réduite par l'utilisation du paramètre
   <varname>archive_timeout</varname>, qui peut être abaissé à des valeurs
   de quelques secondes. Toutefois, un paramètre si bas augmentera de façon
   considérable la bande passante nécessaire pour le transfert de fichiers.
   L'utilisation de la technique de streaming replication (voir <xref linkend="streaming-replication"/>)
   permet de diminuer la taille de la fenêtre de temps de perte de données.
  </para>

  <para>
   La performance de la récupération est suffisamment bonne pour que le standby ne
   soit en général qu'à quelques instants de la pleine
   disponibilité à partir du moment où il aura été activé. C'est pour cette raison que
   cette configuration de haute disponibilité est appelée warm standby.
   Restaurer un serveur d'une base de sauvegarde archivée, puis appliquer tous les journaux
   prendra largement plus de temps, ce qui fait que cette technique est une solution
   de 'disaster recovery' (reprise après sinistre), pas de haute disponibilité.
   Un serveur de standby peut aussi être utilisé pour des requêtes en lecture seule, dans
   quel cas il est appelé un serveur de Hot Standby. Voir <xref linkend="hot-standby"/> pour
   plus d'information.
  </para>

  <indexterm zone="high-availability">
   <primary>warm standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>PITR standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>serveur de standby</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>log shipping</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>serveur témoin</primary>
  </indexterm>

  <indexterm zone="high-availability">
   <primary>STONITH</primary>
  </indexterm>

  <sect2 id="standby-planning">
   <title>Préparatifs</title>

   <para>
    Il est habituellement préférable de créer les serveurs primaire et de standby
    de façon à ce qu'ils soient aussi similaires que possible, au moins du
    point de vue du serveur de bases de données. En particulier, les chemins
    associés avec les tablespaces seront passés d'un nœud à l'autre sans conversion, ce qui
    implique que les serveurs primaire et de standby doivent avoir les mêmes chemins de montage pour
    les tablespaces si cette fonctionnalité est utilisée. Gardez en tête que si
    <xref linkend="sql-createtablespace"/>
    est exécuté sur le primaire, tout nouveau point de montage nécessaire pour cela doit être créé
    sur le primaire et tous les standby avant que la commande ne
    soit exécutée. Le matériel n'a pas besoin d'être exactement le même, mais l'expérience monte
    que maintenir deux systèmes identiques est plus facile que maintenir deux
    différents sur la durée de l'application et du système.
    Quoi qu'il en soit, l'architecture hardware doit être la même &mdash; répliquer
    par exemple d'un serveur 32 bits vers un 64 bits ne fonctionnera pas.
   </para>

   <para>
    De manière générale, le log shipping entre serveurs exécutant des versions
    majeures différentes de <productname>PostgreSQL</productname> est
    impossible. La politique du PostgreSQL Global Development Group est de ne pas
    réaliser de changement sur les formats disques lors des mises à jour mineures,
    il est par conséquent probable que l'exécution de versions mineures différentes
    sur le primaire et le standby fonctionne correctement. Toutefois, il n'y a
    aucune garantie formelle de cela et il est fortement conseillé de garder le
    serveur primaire et celui de standby au même niveau de version autant que faire
    se peut. Lors d'une mise à jour vers une nouvelle version mineure, la politique la
    plus sûre est de mettre à jour les serveurs de standby d'abord &mdash; une nouvelle
    version mineure est davantage susceptible de lire les enregistrements WAL d'une
    ancienne version mineure que l'inverse.
   </para>

  </sect2>

  <sect2 id="standby-server-operation">
   <title>Fonctionnement du Serveur de Standby</title>

   <para>
    En mode de standby, le serveur applique continuellement les WAL reçus du
    serveur primaire. Le serveur de standby peut lire les WAL d'une archive WAL
    (voir <xref linkend="guc-restore-command"/>) ou directement du primaire via une
    connexion TCP (streaming replication). Le serveur de standby essaiera aussi de
    restaurer tout WAL trouvé dans le répertoire <filename>pg_wal</filename> du
    cluster de standby. Cela se produit habituellement après un redémarrage de
    serveur, quand le standby rejoue à nouveau les WAL qui ont été reçu du primaire
    avant le redémarrage, mais vous pouvez aussi copier manuellement des fichiers dans
    <filename>pg_wal</filename> à tout moment pour qu'ils soient rejoués.
   </para>

   <para>
    Au démarrage, le serveur de standby commence par restaurer tous les WAL
    disponibles à l'endroit où se trouvent les archives, en appelant la
    <varname>restore_command</varname>. Une fois qu'il a épuisé tous les WAL
    disponibles à cet endroit et que <varname>restore_command</varname>
    échoue, il essaie de restaurer tous les WAL disponibles dans le répertoire
    <filename>pg_wal</filename>. Si cela échoue, et que la réplication en flux
    a été activée, le standby essaie
    de se connecter au serveur primaire et de démarrer la réception des WAL depuis
    le dernier enregistrement valide trouvé dans les archives ou
    <filename>pg_wal</filename>. Si cela
    échoue ou que la streaming replication n'est pas configurée, ou que la connexion
    est plus tard déconnectée, le standby retourne à l'étape 1 et essaie de
    restaurer le fichier à partir de l'archive à nouveau. Cette boucle de
    retentatives de l'archive, <filename>pg_wal</filename> et par la
    streaming replication continue
    jusqu'à ce que le serveur soit stoppé ou que le failover (bascule) soit
    déclenché par un fichier trigger (déclencheur).
   </para>

   <para>
    Le mode de standby est quitté et le serveur bascule en mode de
    fonctionnement normal quand <command>pg_ctl promote</command> est exécuté,
    que <command>pg_promote()</command> est appelé ou qu'un fichier de trigger
    est trouvé (<varname>promote_trigger_file</varname>). Avant de basculer,
    tout WAL immédiatement disponible dans l'archive ou le
    <filename>pg_wal</filename> sera restauré, mais aucune tentative ne sera
    faite pour se connecter au primaire.
   </para>
  </sect2>

  <sect2 id="preparing-master-for-standby">
   <title>Préparer le primaire pour les serveurs de standby</title>

   <para>
    Mettez en place un archivage en continu sur le primaire vers un répertoire
    d'archivage accessible depuis le standby, comme décrit
    dans <xref linkend="continuous-archiving"/>. La destination d'archivage devrait être
    accessible du standby même quand le primaire est inaccessible, c'est à dire qu'il
    devrait se trouver sur le serveur de standby lui-même ou un autre serveur de confiance, pas sur
    le serveur primaire.
   </para>

   <para>
    Si vous voulez utiliser la streaming replication, mettez en place l'authentification sur le
    serveur primaire pour autoriser les connexions de réplication à partir du (des) serveur de
    standby&nbsp;; c'est-à-dire, créez un rôle et mettez en place une ou des entrées appropriées dans
    <filename>pg_hba.conf</filename> avec le champ database positionné à
    <literal>replication</literal>. Vérifiez aussi que <varname>max_wal_senders</varname> est positionné
    à une valeur suffisamment grande dans le fichier de configuration du serveur primaire.
    Si des slots de réplication seront utilisés, il faut s'assurer que
    <varname>max_replication_slots</varname> est également positionné à une
    valeur suffisamment grande.
   </para>

   <para>
    Effectuez une sauvegarde de base comme décrit dans <xref linkend="backup-base-backup"/>
    pour initialiser le serveur de standby.
   </para>
  </sect2>

  <sect2 id="standby-server-setup">
   <title>Paramétrer un serveur de standby</title>

   <para>
    Pour paramétrer le serveur de standby, restaurez la sauvegarde de base
    effectué sur le serveur primaire (voir <xref
    linkend="backup-pitr-recovery"/>). Créez un fichier
    <filename>standby.signal</filename> dans le répertoire de données du
    cluster de standby. Positionnez <xref linkend="guc-restore-command"/> à
    une simple commande qui recopie les fichiers de l'archive de WAL. Si vous
    comptez disposer de plusieurs serveurs de stanby pour mettre en
    &oelig;uvre de la haute disponibilité, assurez-vous que
    <varname>recovery_target_timeline</varname> est configuré à
    <literal>latest</literal> (la valeur par défaut), pour indiquer que le
    serveur de standby devra prendre en compte la ligne temporelle définie
    lors de la bascule à un autre serveur de standby.
   </para>

   <note>
    <para>
     N'utilisez pas pg_standby ou des outils similaires avec le mode de standby intégré
     décrit ici. <xref linkend="guc-restore-command"/> devrait retourner immédiatement
     si le fichier n'existe pas; le serveur essaiera la commande à nouveau si nécessaire.
     Voir <xref linkend="log-shipping-alternative"/> pour utiliser des outils tels que pg_standby.
    </para>
   </note>

   <para>
    Si vous souhaitez utiliser la streaming replication, renseignez
    <xref linkend="guc-primary-conninfo"/> avec une chaîne de connexion libpq,
    contenant le nom d'hôte (ou l'adresse IP) et tout détail supplémentaire
    nécessaire pour se connecter au serveur primaire. Si le primaire a besoin d'un
    mot de passe pour l'authentification, le mot de passe doit aussi être spécifié dans
    <xref linkend="guc-primary-conninfo"/>.
   </para>

   <para>
    Si vous mettez en place le serveur de standby pour des besoins de haute disponibilité,
    mettez en place l'archivage de WAL, les connexions et l'authentification à l'identique
    du serveur primaire, parce que le serveur de standby fonctionnera comme un serveur primaire
    après la bascule.
   </para>

   <para>
    Si vous utilisez une archive WAL, sa taille peut être réduite en utilisant
    l'option <xref linkend="guc-archive-cleanup-command"/> pour supprimer les
    fichiers qui ne sont plus nécessaires au serveur de standby. L'outil
    <application>pg_archivecleanup</application> est conçu spécifiquement pour
    être utilisé avec <varname>archive_cleanup_command</varname> dans des
    configurations typiques de standby, voir <xref linkend="pgarchivecleanup"/>.
    Notez toutefois que si vous utilisez l'archive à des fins de sauvegarde,
    vous avez besoin de garder les fichiers nécessaires pour restaurer à partir
    de votre dernière sauvegarde de base, même si ces fichiers ne sont plus
    nécessaires pour le standby.
   </para>

   <para>
    Voici un simple exemple&nbsp;:
    <programlisting>
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass options=''-c wal_sender_timeout=5000'''
restore_command = 'cp /path/to/archive/%f %p'
archive_cleanup_command = 'pg_archivecleanup /path/to/archive %r'
    </programlisting>
   </para>

   <para>
    Vous pouvez avoir n'importe quel nombre de serveurs de standby, mais si vous
    utilisez la streaming replication, assurez vous d'avoir positionné
    <varname>max_wal_senders</varname> suffisamment haut sur le primaire pour leur permettre
    de se connecter simultanément.
   </para>
  </sect2>

  <sect2 id="streaming-replication">
   <title>Streaming Replication</title>

   <indexterm zone="high-availability">
    <primary>Streaming Replication</primary>
   </indexterm>

   <para>
    La streaming replication permet à un serveur de standby de rester plus
    à jour qu'il n'est possible avec l'envoi de journaux par fichiers. Le
    standby se connecte au primaire, qui envoie au standby les enregistrements
    de WAL dès qu'ils sont générés, sans attendre qu'un fichier de WAL soit rempli.
   </para>

   <para>
    La streaming replication est asynchrone par défaut (voir <xref
    linkend="synchronous-replication"/>), auquel cas il y a un petit délai
    entre la validation d'une transaction sur le primaire et le moment où les
    changements sont visibles sur le standby. Le délai est toutefois beaucoup plus petit
    qu'avec l'envoi de fichiers, habituellement en dessous d'une seconde en partant
    de l'hypothèse que le standby est suffisamment puissant pour supporter la charge. Avec
    la streaming replication, <varname>archive_timeout</varname> n'est pas nécessaire
    pour réduire la fenêtre de perte de données.
   </para>

   <para>
    Si vous utilisez la streaming replication sans archivage en continu des fichiers,
    le serveur pourrait recycler de vieux journaux de transactions avant que le
    serveur ne les ait reçus.  Si cela arrive, le serveur en standby devra être
    recréé d'une nouvelle sauvegarde de l'instance.  Vous pouvez éviter cela en
    positionnant <varname>wal_keep_size</varname> à une valeur suffisamment
    grande pour s'assurer que les journaux de transactions ne sont pas recyclés
    trop tôt, ou en configurant un slot de réplication pour le serveur en
    standby.  Si un archivage des journaux de transactions est en place, et que
    les fichiers archivés sont disponibles depuis le serveur en standby, cette
    solution n'est pas nécessaire, puisque le serveur en standby peut toujours
    utiliser les fichiers archivés pour rattraper son retard, sous réserve que
    suffisamment de fichiers soient conservés.
   </para>

   <para>
    Pour utiliser la streaming replication, mettez en place un serveur de standby
    en mode fichier comme décrit dans <xref linkend="warm-standby"/>. L'étape qui
    transforme un standby en mode fichier en standby en streaming replication est de
    faire pointer <varname>primary_conninfo</varname> vers le serveur primaire. Positionnez
    <xref linkend="guc-listen-addresses"/> et les options d'authentification
    (voir <filename>pg_hba.conf</filename>) sur le primaire pour que le serveur
    de standby puisse se connecter à la pseudo-base <literal>replication</literal>
    sur le serveur primaire (voir <xref linkend="streaming-replication-authentication"/>).
   </para>

   <para>
    Sur les systèmes qui supportent l'option de keepalive sur les sockets, positionner
    <xref linkend="guc-tcp-keepalives-idle"/>,
    <xref linkend="guc-tcp-keepalives-interval"/> et
    <xref linkend="guc-tcp-keepalives-count"/> aide le primaire à reconnaître rapidement
    une connexion interrompue.
   </para>

   <para>
    Positionnez le nombre maximum de connexions concurrentes à partir des
    serveurs de standby (voir <xref linkend="guc-max-wal-senders"/> pour les détails).
   </para>

   <para>
    Quand le standby est démarré et que <varname>primary_conninfo</varname> est
    positionné correctement, le standby se connectera au primaire après avoir
    rejoué tous les fichiers WAL disponibles dans l'archive. Si la connexion
    est établie avec succès, vous verrez un processus
    <literal>walreceiver</literal> dans le standby, et un processus
    <literal>walsender</literal> correspondant sur le primaire.
   </para>

   <sect3 id="streaming-replication-authentication">
    <title>Authentification</title>
    <para>
     Il est très important que les droits d'accès pour la réplication
     soient paramétrés pour que seuls les utilisateurs de confiance puissent
     lire le flux WAL, parce qu'il est facile d'en extraire des informations
     privilégiées. Les serveurs de standby doivent s'authentifier au serveur
     primaire en tant qu'un compte disposant de l'attribut
     <literal>REPLICATION</literal> ou <literal>SUPERUSER</literal>. Il est
     recommandé de créer un compte utilisateur dédié pour la réplication. Il
     doit disposer des attributs <literal>REPLICATION</literal> et
     <literal>LOGIN</literal>. Alors que l'attribut
     <literal>REPLICATION</literal> donne beaucoup de droits, il ne permet pas
     à l'utilisateur de modifier de données sur le serveur primaire,
     contrairement à l'attribut <literal>SUPERUSER</literal>.
    </para>

    <para>
     L'authentification cliente pour la réplication est contrôlée par un enregistrement de
     <filename>pg_hba.conf</filename> spécifiant <literal>replication</literal> dans le champ
     <replaceable>database</replaceable>. Par exemple, si le standby s'exécute sur un hôte d'IP
     <literal>192.168.1.100</literal>  et que le nom de l'utilisateur pour la réplication est
     <literal>foo</literal>, l'administrateur peut ajouter la ligne suivante au fichier
     <filename>pg_hba.conf</filename>  sur le primaire:


     <programlisting>
# Autoriser l'utilisateur "foo" de l'hôte 192.168.1.100 à se connecter au primaire
# en tant que standby de replication si le mot de passe de l'utilisateur est correctement fourni
#
# TYPE  DATABASE        USER            ADDRESS                 METHOD
host    replication     foo             192.168.1.100/32        md5
     </programlisting>
    </para>
    <para>
     Le nom d'hôte et le numéro de port du primaire, le nom d'utilisateur de la connexion,
     et le mot de passe sont spécifiés dans le paramètre <xref linkend="guc-primary-conninfo"/>.
     Le mot de passe peut aussi être enregistré dans le fichier
     <filename>~/.pgpass</filename> sur le serveur en attente (en précisant
     <literal>replication</literal> dans le champ
     <replaceable>database</replaceable>).
     Par exemple, si le primaire s'exécute sur l'hôte d'IP <literal>192.168.1.50</literal>,
     port <literal>5432</literal>, que le nom de l'utilisateur pour la réplication est
     <literal>foo</literal>, et que le mot de passe est <literal>foopass</literal>, l'administrateur
     peut ajouter la ligne suivante au fichier <filename>postgresql.conf</filename> sur le standby&nbsp;:

     <programlisting>
# Le standby se connecte au primaire qui s'exécute sur l'hôte 192.168.1.50
# et port 5432 en tant qu'utilisateur "foo" dont le mot de passe est "foopass"
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
     </programlisting>
    </para>
   </sect3>

   <sect3 id="streaming-replication-monitoring">
    <title>Supervision</title>
    <para>
     Un important indicateur de santé de la streaming replication est le nombre
     d'enregistrements générés sur le primaire, mais pas encore appliqués sur
     le standby. Vous pouvez calculer ce retard en comparant le point d'avancement
     des écritures du WAL sur le primaire avec le dernier point d'avancement reçu par
     le standby. Ils peuvent être récupérés en utilisant
     <function>pg_current_wal_lsn</function> sur le primaire et
     <function>pg_last_wal_receive_lsn</function> sur le standby,
     respectivement (voir <xref linkend="functions-admin-backup-table"/> et
     <xref linkend="functions-recovery-info-table"/> pour plus de détails).
     Le point d'avancement de la réception dans le standby est aussi affiché dans
     le statut du processus de réception des WAL (wal receiver), affiché par
     la commande <command>ps</command> (voir <xref linkend="monitoring-ps"/> pour plus de détails).
    </para>
    <para>
     Vous pouvez obtenir la liste des processus émetteurs de WAL au moyen de la vue
     <link linkend="monitoring-pg-stat-replication-view"><structname>
       pg_stat_replication</structname></link>
     D'importantes différences entre les champs <function>pg_current_wal_lsn</function> et
     <literal>sent_lsn</literal> peuvent indiquer que le serveur primaire est en surcharge,
     tandis que des différences entre <literal>sent_lsn</literal> et
     <function>pg_last_wal_receive_lsn</function> sur le standby peuvent soit indiquer une latence
     réseau importante, soit que le standby est surchargé.
    </para>
    <para>
     Sur un hot standby, le statut du processus wal receiver est récupérable
     avec la vue <link linkend="monitoring-pg-stat-wal-receiver-view">
      <structname>pg_stat_wal_receiver</structname></link>. Une différence
     importante entre <function>pg_last_wal_replay_lsn</function> et la
     colonne <literal>flushed_lsn</literal> de la vue indique que les WAL
     sont reçus plus rapidement qu'ils ne sont rejoués.
    </para>
   </sect3>

  </sect2>

  <sect2 id="streaming-replication-slots">
   <title>Slots de réplication</title>
   <indexterm>
    <primary>slot de réplication</primary>
    <secondary>réplication en flux</secondary>
   </indexterm>
   <para>
    Les slots de réplication fournissent une manière automatisée de s'assurer
    que le primaire ne supprime pas les journaux de transactions avant qu'ils
    n'aient été reçus par tous les serveurs en standby, et que le serveur
    primaire ne supprime pas des lignes qui pourraient causer un
    <link linkend="hot-standby-conflict">conflit de restauration</link> même si
    le serveur en standby est déconnecté.
   </para>
   <para>
    Au lieu d'utiliser des slots de réplication, il est possible d'empêcher la
    suppression des anciens journaux de transactions en utilisant
    <xref linkend="guc-wal-keep-size"/>, ou en les stockant dans un
    répertoire d'archive en utilisant <xref linkend="guc-archive-command"/>.
    Cependant, ces méthodes ont souvent pour résultat le stockage de plus de
    journaux de transactions que nécessaire, alors que les slots de réplication
    ne conservent que le nombre nécessaire de journaux de transactions. D'un
    autre côté, les slots de réplication peuvent retenir tellement de journaux
    de transactions qu'ils rempliraient l'espace disque alloué à
    <literal>pg_wal</literal>&nbsp;; <xref linkend="guc-max-slot-wal-keep-size"/>
    limite la taille des journaux de transactions à retenir par les slots de
    réplication.
   </para>
   <para>
    De la même manière, <xref linkend="guc-hot-standby-feedback"/>
    et <xref linkend="guc-vacuum-defer-cleanup-age"/> fournissent des
    protections contre la suppression de lignes utiles par vacuum, mais le
    premier paramètre n'offre aucune protection durant la période pendant
    laquelle le serveur de standby n'est pas connecté, et le second nécessite
    souvent d'être positionné à une grande valeur pour fournir une protection
    adéquate. Les slots de réplication surmontent ces désavantages.
   </para>
   <sect3 id="streaming-replication-slots-manipulation">
    <title>Requêter et manipuler des slots de réplication</title>
    <para>
     Chaque slot de réplication à un nom, qui peut contenir des lettres en
     minuscule, des nombres ou un tiret bas.
    </para>
    <para>
     Les slots de réplication existants et leur états peuvent être vus dans la vue
     <link linkend="view-pg-replication-slots"><structname>pg_replication_slots</structname></link>.
    </para>
    <para>
     Les slots de réplication peuvent être créés et supprimés soit via le
     protocole de réplication en flux (voir <xref linkend="protocol-replication"/>)
     soit via des fonctions SQL (voir <xref linkend="functions-replication"/>).
    </para>
   </sect3>
   <sect3 id="streaming-replication-slots-config">
    <title>Exemple de configuration</title>
    <para>
     Il est possible de créer un slot de réplication ainsi&nbsp;:
     <programlisting>
postgres=# SELECT * FROM pg_create_physical_replication_slot('node_a_slot');
  slot_name  | lsn
-------------+-----
 node_a_slot |

postgres=# SELECT slot_name, slot_type, active FROM pg_replication_slots;
  slot_name  | slot_type | active
-------------+-----------+--------
 node_a_slot | physical  | f
     </programlisting>
     Pour configurer le serveur en standby pour utiliser ce slot,
     <varname>primary_slot_name</varname> devrait être configuré sur le secondaire.
     Voici un exemple simple&nbsp;:
     <programlisting>
primary_conninfo = 'host=192.168.1.50 port=5432 user=foo password=foopass'
primary_slot_name = 'node_a_slot'
     </programlisting>
    </para>
   </sect3>
  </sect2>

  <sect2 id="cascading-replication">
   <title>Réplication en cascade</title>

   <indexterm zone="high-availability">
    <primary>Réplication en cascade</primary>
   </indexterm>

   <para>
    La fonctionnalité de réplication en cascade permet à un serveur
    standby d'accepter les connexions de réplication et d'envoyer un flux
    d'enregistrements de journaux de transactions à d'autres secondaires,
    agissant ainsi comme un relais. C'est généralement utilisé pour réduire
    le nombre de connexions directes au primaire et minimise ainsi l'utilisation
    de bande passante entre sites distants.
   </para>

   <para>
    Un serveur standby agissant à la fois comme un receveur et comme un
    émetteur est connu sous le nom de standby en cascade
    (<foreignphrase>cascading standby</foreignphrase>). Les standbys qui
    sont plus proches du serveur primaire sont connus sous le nom
    de serveurs <foreignphrase>upstream</foreignphrase> alors que les serveurs
    standby en bout de chaîne sont des serveurs
    <foreignphrase>downstream</foreignphrase>. La réplication en cascade
    ne pose pas de limites sur le nombre ou l'arrangement des serveurs
    <foreignphrase>downstream</foreignphrase>. Chaque standby se connecte à un
    seul serveur <foreignphrase>upstream</foreignphrase>, qui finit par
    arriver à un seul serveur primaire/primaire.
   </para>

   <para>
    Un standby en cascade envoie non seulement les enregistrements reçus de
    journaux de transactions mais aussi ceux restaurés des archives. Donc,
    même si la connexion de réplication d'une connexion upstream est
    rompue, la réplication en flux continue vers le serveur downstream tant
    que de nouveaux enregistrements de journaux de transactions sont
    disponibles.
   </para>

   <para>
    La réplication en cascade est actuellement asynchrone. La réplication
    synchrone (voir <xref linkend="synchronous-replication"/>) n'a aucun
    effet sur la réplication en cascade.
   </para>

   <para>
    Les messages en retour des serveurs Hot Standby se propagent vers les
    serveurs upstream, quelle que soit la configuration de la réplication en
    cascade.
   </para>

   <para>
    Si un serveur standby upstream est promu pour devenir le nouveau serveur
    primaire, les serveurs downstream continueront à recevoir le flux de
    réplication du nouveau primaire si le paramètre
    <varname>recovery_target_timeline</varname> est configuré à
    <literal>'latest'</literal> (valeur par défaut).
   </para>

   <para>
    Pour utiliser la réplication en cascade, configurez le standby en cascade
    de façon à ce qu'il accepte les connexions de réplication (configurez
    <xref linkend="guc-max-wal-senders"/> et <xref linkend="guc-hot-standby"/>,
    ainsi que l'<link linkend="auth-pg-hba-conf">authentification</link>).
    Vous aurez aussi besoin de configurer la variable
    <varname>primary_conninfo</varname> dans le standby downstream pour qu'elle
    pointe vers le standby en cascade.
   </para>
  </sect2>

  <sect2 id="synchronous-replication">
   <title>Réplication synchrone</title>

   <indexterm zone="high-availability">
    <primary>Réplication Synchrone</primary>
   </indexterm>

   <para>
    La streaming réplication mise en &oelig;uvre par <productname>PostgreSQL</productname> est asynchrone
    par défaut. Si le serveur primaire est hors-service, les transactions produites alors
    peuvent ne pas avoir été répliquées sur le serveur de standby, impliquant une perte
    de données. La quantité de données perdues est proportionnelle au délai de réplication
    au moment de la bascule.
   </para>

   <para>
    La réplication synchrone permet de confirmer que tous les changements
    effectués par une transaction ont bien été transférées à un ou plusieurs
    serveurs de standby synchrone. Cette propriété étend le niveau de robustesse
    standard offert par un commit. En science informatique, ce niveau de
    protection est appelé réplication à deux états (<foreignphrase>2-safe
     replication</foreignphrase>) et <foreignphrase>group-1-safe</foreignphrase>
    (<foreignphrase>group-safe</foreignphrase> et
    <foreignphrase>1-safe</foreignphrase>) quand
    <varname>synchronous_commit</varname> est configuré à la valeur
    <literal>remote_write</literal>.
   </para>

   <para>
    Lorsque la réplication synchrone est utilisée, chaque validation portant
    sur une écriture va nécessiter d'attendre la confirmation
    de l'écriture de cette validation sur les journaux de transaction des disques
    du serveur primaire et des serveurs en standby. Le seul moyen possible pour que des données
    soient perdues est que les serveur primaire et de standby soient hors service au
    même moment. Ce mécanisme permet d'assurer un niveau plus élevé de robustesse, en admettant que
    l'administrateur système ait pris garde à l'emplacement et à la gestion de ces deux
    serveurs. Attendre après la confirmation de l'écriture augmente la confiance que l'utilisateur pourra avoir sur
    la conservation des modifications dans le cas où un serveur serait hors service mais il augmente aussi
    en conséquence le temps de réponse à chaque requête.
    Le temps minimum d'attente est celui de l'aller-retour entre les serveurs primaire et de standby.
   </para>

   <para>
    Les transactions où seule une lecture est effectuée ou qui consistent à annuler une transaction ne nécessitent pas d'attendre
    les serveurs de standby. Les validations concernant les transactions imbriquées ne nécessitent pas non plus d'attendre
    la réponse des serveurs de standby, cela n'affecte en fait que les validations principales. De longues
    opérations comme le chargement de données ou la création d'index n'attendent pas
    le commit final pour synchroniser les données. Toutes les actions de validation en deux étapes
    nécessitent d'attendre la validation du standby, incluant autant l'opération de préparation que l'opération de validation.
   </para>

   <para>
    Un standby synchrone peut être un standby de réplication physique ou un
    abonné d'une réplication logique. Il peut aussi être tout autre
    consommateur de flux de réplication physique ou logique qui sait comment
    gérer les messages de retour appropriés. En plus des systèmes intégrés de
    réplication physique et logique, cela inclut le support des outils tels
    que <command>pg_receivewal</command> et <command>pg_recvlogical</command>
    ainsi que les systèmes de réplication et outils personnalisés tiers.
   </para>

   <sect3 id="synchronous-replication-config">
    <title>Configuration de base</title>

    <para>
     Une fois la streaming replication configurée, la configuration de la réplication synchrone
     ne demande qu'une unique étape de configuration supplémentaire&nbsp;:
     la variable <xref linkend="guc-synchronous-standby-names"/> doit être définie à
     une valeur non vide. La variable <varname>synchronous_commit</varname> doit aussi être définie à
     <literal>on</literal>, mais comme il s'agit d'une valeur par défaut, il n'est pas nécessaire de la
     modifier. (Voir <xref linkend="runtime-config-wal-settings"/> et
     <xref linkend="runtime-config-replication-master"/>.)
     Cette configuration va entraîner l'attente de la confirmation de l'écriture permanente de chaque validation
     sur le serveur de standby.
     La variable <varname>synchronous_commit</varname> peut être définie soit par des
     utilisateurs, soit par le fichier de configuration pour des utilisateurs ou des bases de données fixées, soit
     dynamiquement par des applications, pour contrôler la robustesse des échanges transactionnels.
    </para>

    <para>
     Suite à l'enregistrement sur disque d'une validation sur le serveur primaire,
     l'enregistrement WAL est envoyé au serveur de standby. Le serveur de standby
     retourne une réponse à chaque fois qu'un nouveau lot de données WAL est
     écrit sur disque, à moins que le paramètre
     <varname>wal_receiver_status_interval</varname> soit défini à zéro sur le
     serveur standby. Dans le cas où le paramètre <varname>synchronous_commit</varname>
     est configuré à la valeur <literal>remote_apply</literal>, le serveur
     standby envoie des messages de réponse quand l'enregistrement de validation
     (commit) est rejoué, rendant la transaction visible. Si le serveur standby
     est configuré en serveur synchrone d'après la configuration du paramètre
     <varname>synchronous_standby_names</varname>
     sur le primaire, le message de réponse provenant du standby sera considéré
     parmi ceux des autres serveurs standby pour décider du moment de libération
     des transactions attendant la confirmation de la bonne réception de
     l'enregistrement de commit. Ces paramètres permettent à l'administrateur
     de spécifier quels serveurs de standby suivront un comportement synchrone.
     Remarquez ici que la configuration de la réplication synchrone se situe
     sur le serveur primaire. Les serveurs standbys nommés doivent être
     directement connectés au primaire&nbsp;; le primaire ne connaît rien des
     serveurs standbys utilisant la réplication en cascade.
    </para>

    <para>
     Configurer <varname>synchronous_commit</varname> à
     <literal>remote_write</literal> fera que chaque COMMIT attendra la
     confirmation de la réception en mémoire de l'enregistrement du COMMIT
     par le standby et son écriture via la système d'exploitation, sans que
     les données du cache du système ne soient vidées sur disque au niveau
     du serveur en standby.  Cette configuration fournit une garantie moindre
     de durabilité que la configuration <literal>on</literal>&nbsp;: le standby
     peut perdre les données dans le cas d'un crash du système d'exploitation,
     mais pas dans le cas du crash de <productname>PostgreSQL</productname>.
     Cependant, il s'agit d'une configuration utile en pratique car il diminue
     le temps de réponse pour la transaction. Des pertes de données ne peuvent
     survenir que si le serveur primaire et le standby tombent en même temps et
     que la base de données du primaire est corrompue.
    </para>

    <para>
     Configurer <varname>synchronous_commit</varname> à <literal>remote_apply</literal>
     fera en sorte que chaque commit devra attendre le retour des standbys synchrones
     actuels indiquant qu'ils ont bien rejoué la transaction, la rendant visible aux
     requêtes des utilisateurs. Dans des cas simples, ceci permet une répartition de
     chaque sans incohérence.
    </para>

    <para>
     Habituellement, un signal d'arrêt rapide (<foreignphrase>fast shutdown</foreignphrase>)
     annule les transactions en cours sur tous les processus serveur. Cependant, dans
     le cas de la réplication asynchrone, le serveur n'effectuera pas un
     arrêt complet avant que chaque enregistrement WAL ne soit transféré aux serveurs
     de standby connectés.
    </para>

   </sect3>

   <sect3 id="synchronous-replication-multiple-standbys">
    <title>Multiple standbys synchrones</title>

    <para>
     La réplication synchrone supporte un ou plusieurs serveurs standbys
     synchrones. Les transactions attendront que tous les serveurs en standby
     considérés synchrones confirment la réception de leurs données. Le nombre
     de standbys dont les transactions doivent attendre la réponse est indiqué
     dans le paramètre <varname>synchronous_standby_names</varname>. Ce paramètre
     indique aussi une liste des noms des serveurs standbys ou l'emploi
     de la méthode (<literal>FIRST</literal> ou <literal>ANY</literal>) pour choisir
     sur quel serveur synchrone basculer parmi l'ensemble des serveurs listés.
    </para>

    <para>
     La méthode <literal>FIRST</literal> définit une réplication synchrone
     priorisée&nbsp;: elle temporise la validation de la transaction
     jusqu'à ce que les enregistrements WAL soient répliqués en
     fonction de la priorité définie des serveurs standbys dans une liste
     ordonnée.
     Le serveur standby dont le nom apparaît en premier sur la liste est
     prioritaire et est celui qui est considéré comme synchrone.
     Les serveurs standbys suivants sont considérés comme un/des
     serveurs standbys synchrones potentiels.
     Si le premier serveur synchrone venait à tomber,
     il serait immédiatement remplacé par le serveur
     standby prioritaire suivant.

    </para>
    <para>
     Voici un exemple de configuration de <varname>synchronous_standby_names</varname> pour la
     réplication synchrone priorisée&nbsp;:
     <programlisting>
synchronous_standby_names = 'FIRST 2 (s1, s2, s3)'
     </programlisting>
     Dans cet exemple, si les quatre serveurs standbys <literal>s1</literal>,
     <literal>s2</literal>, <literal>s3</literal> et <literal>s4</literal>
     sont fonctionnels et en cours d'exécution, les deux serveurs
     <literal>s1</literal> et <literal>s2</literal> seront choisis comme
     standbys synchrones car leurs noms apparaissent en premier dans la
     liste des serveurs standbys. <literal>s3</literal> est un serveur
     standby synchrone potentiel et prendra le rôle d'un standby synchrone
     si <literal>s1</literal> ou <literal>s2</literal> tombe.
     <literal>s4</literal> est un standby asynchrone et son nom n'est pas dans la liste.
    </para>

    <para>
     La méthode <literal>ANY</literal> définit une réplication synchrone
     basée sur un quorum&nbsp;: elle temporise la validation de la
     transaction jusqu'à ce que les enregistrements WAL soient répliqués
     <emphasis>au moins</emphasis> sur le nombre de serveurs définis dans
     la liste.
    </para>
    <para>
     Voici un exemple de configuration du paramètre <varname>synchronous_standby_names</varname> pour la
     réplication synchrone avec un quorum&nbsp;:
     <programlisting>
 synchronous_standby_names = 'ANY 2 (s1, s2, s3)'
     </programlisting>
     Dans cet exemple, sur quatre serveurs standbys démarrés <literal>s1</literal>,
     <literal>s2</literal>,<literal>s3</literal> et <literal>s4</literal>,
     pour obtenir la validation d'une transaction, le serveur primaire
     attendra la réponse d'au minimum deux standbys parmi <literal>s1</literal>,
     <literal>s2</literal> et <literal>s3</literal>. <literal>s4</literal>
     est un standby asynchrone et son nom n'est pas dans la liste.
    </para>
    <para>
     L'état de synchronicité des serveurs standbys peut être consulté
     avec la vue <structname>pg_stat_replication</structname>.
    </para>
   </sect3>

   <sect3 id="synchronous-replication-performance">
    <title>S'organiser pour obtenir de bonnes performances</title>

    <para>
     La réplication synchrone nécessite souvent d'organiser avec une grande attention
     les serveurs de standby pour apporter un bon niveau de performances aux applications. Les phases d'attente d'écriture
     n'utilisent pas les ressources systèmes, mais les verrous transactionnels restent
     positionnés jusqu'à ce que le transfert vers les serveurs de standby soit confirmé. En conséquence, une utilisation non avertie de
     la réplication synchrone aura pour impact une baisse des performances de la base de données
     d'une application due à l'augmentation des temps de réponses et à un moins bon support de la charge.
    </para>

    <para>
     <productname>PostgreSQL</productname> permet aux développeurs d'application
     de spécifier le niveau de robustesse à employer pour la réplication. Cela peut être
     spécifié pour le système entier, mais aussi pour
     des utilisateurs ou des connexions spécifiques, ou encore pour des transactions individuelles.
    </para>

    <para>
     Par exemple, une répartition du travail pour une application pourrait être constituée de&nbsp;:
     10 % de modifications concernant des articles de clients importants, et
     90 % de modifications de moindre importance et qui ne devraient pas avoir d'impact sur le métier
     si elles venaient à être perdues, comme des dialogues de messagerie entre utilisateurs.
    </para>

    <para>
     Les options de réplication synchrone spécifiées par une application
     (sur le serveur primaire) permettent de n'utiliser la réplication synchrone que pour les modifications les plus
     importantes, sans affecter les performances sur la plus grosse partie des traitements.
     Les options modifiables par les applications sont un outil important permettant
     d'apporter les bénéfices de la réplication synchrone aux applications nécessitant de la haute performance.
    </para>

    <para>
     Il est conseillé de disposer d'une bande passante réseau supérieure
     à la quantité de données WAL générées.
    </para>

   </sect3>

   <sect3 id="synchronous-replication-ha">
    <title>S'organiser pour la haute disponibilité</title>

    <para>
     <varname>synchronous_standby_names</varname> indique le nombre et les noms
     des serveurs standbys synchrones pour lesquels les validations de
     transactions effectuées lorsque <varname>synchronous_commit</varname> est
     configurée à <literal>on</literal>, <literal>remote_apply</literal> ou
     <literal>remote_write</literal>, attendront leur réponse. Ces validations
     de transactions pourraient ne jamais se terminer si un des standbys
     synchrones s'arrêtait brutalement.
    </para>

    <para>
     La meilleure solution pour la haute disponibilité est de s'assurer que vous
     conservez autant de serveurs standbys synchrones que demandés. Ceci se fait
     en nommant plusieurs standbys synchrones potentiels avec
     <varname>synchronous_standby_names</varname>.
    </para>

    <para>
     Dans la réplication synchrone dite priorisée, les serveurs standbys dont les noms
     apparaissent en premier seront utilisé comme standbys synchrones.
     Les standbys définis ensuite prendront la place de serveur
     synchrone si l'un des serveurs venait à tomber.

    </para>

    <para>
     Dans la réplication dite de quorum, tous les standbys spécifiés dans la liste
     seront utilisés comme des standbys synchrones potentiels.
     Même si l'un d'entre eux tombe, les autres standbys continueront de
     prétendre au rôle de standby synchrone.
    </para>

    <para>
     Au moment où le premier serveur de standby s'attache au serveur primaire, il est possible qu'il ne soit pas exactement
     synchronisé. Cet état est appelé le mode <literal>catchup</literal>. Une fois
     la différence entre le serveur de standby et le serveur primaire ramenée à zéro,
     le mode <literal>streaming</literal> est atteint.
     La durée du mode catchup peut être longue surtout juste après la création du serveur de standby.
     Si le serveur de standby est arrêté sur cette période, alors la durée du mode catchup
     sera d'autant plus longue.
     Le serveur de standby ne peut devenir un serveur de standby synchrone
     que lorsque le mode <literal>streaming</literal> est atteint.
     L'état de synchronicité des serveurs standbys peut être consulté
     avec la vue <structname>pg_stat_replication</structname>.
    </para>

    <para>
     Si le serveur primaire redémarre alors que des opérations de commit étaient en attente de confirmation, les
     transactions en attente ne seront réellement enregistrées qu'au moment où la base de données du serveur primaire
     sera redémarrée.
     Il n'y a aucun moyen de savoir si tous les serveurs de standby ont reçu toutes
     les données WAL nécessaires au moment où le serveur primaire est déclaré hors-service. Des
     transactions pourraient ne pas être considérées comme sauvegardées sur le serveur de standby, même si
     elles l'étaient sur le serveur primaire. La seule garantie offerte dans ce cadre est que
     l'application ne recevra pas de confirmation explicite de la
     réussite d'une opération de validation avant qu'il ne soit sûr que les données WAL sont
     reçues proprement par tous les serveurs de standby synchrones.
    </para>

    <para>
     Si vous ne pouvez vraiment pas conserver autant de serveurs standbys
     synchrones que demandés, alors vous devriez diminuer le nombre de standbys
     synchrones dont le système doit attendre les réponses aux validations de
     transactions, en modifiant <varname>synchronous_standby_names</varname> (ou
     en le désactivant) et en rechargeant le fichier de configuration du serveur
     primaire.
    </para>

    <para>
     Si le serveur primaire n'est pas accessible par les serveurs de standby restants, il est conseillé
     de basculer vers le meilleur candidat possible parmi ces serveurs de standby.
    </para>

    <para>
     S'il est nécessaire de recréer un serveur de standby alors que des transactions sont
     en attente de confirmation, prenez garde à ce que les commandes pg_start_backup() et
     pg_stop_backup() soient exécutées dans un contexte où
     la variable <varname>synchronous_commit</varname> vaut <literal>off</literal> car, dans le cas contraire, ces
     requêtes attendront indéfiniment l'apparition de ce serveur de standby.
    </para>

   </sect3>

  </sect2>

  <sect2 id="continuous-archiving-in-standby">
   <title>Archivage continu côté standby</title>

   <indexterm>
    <primary>Archivage continu</primary>
    <secondary>côté standby</secondary>
   </indexterm>

   <para>
    Lorsque l'archivage continu est utilisé sur un standby, il existe deux
    scénarios possibles&nbsp;: soit les archives sont partagées entre le
    serveur primaire et le serveur de standby, soit le standby peut avoir ses
    propres archives. Si le serveur possède ses propres archives, en
    définissant le paramètre <varname>archive_mode</varname> à
    <literal>always</literal>, le standby exécutera la commande d'archivage
    pour chaque segment de WAL qu'il aura reçu, peu importe qu'il utilise la
    réplication par les archives ou la réplication streaming. La gestion par
    archivage partagé peut être faite de la même manière, mais
    <varname>archive_command</varname> doit d'abord tester si le segment de
    WAL existe, et si le fichier existant contient les mêmes informations.
    Cela demande plus de précaution lors de la définition de la commande, car
    elle doit vérifier qu'elle n'écrase pas un fichier existant avec un
    contenu différent, et doit renvoyer un succès si le même fichier est
    archivé deux fois. Tout ceci devant être en plus effectué sans concurrence
    si deux serveurs essaient d'archiver le même fichier au même moment.
   </para>

   <para>
    Si <varname>archive_mode</varname> est défini à <literal>on</literal>,
    l'archivage n'est pas actif pendant les modes recovery et standby. Si le
    serveur standby est promu, il commencera à réaliser l'archivage après sa
    promotion, et il archivera uniquement les fichiers (WAL et historique)
    qu'il a lui même produit. Pour être sûr d'obtenir un jeu complet
    d'archives, vous devez vous assurer que tous les fichiers WAL ont été
    archivés avant qu'ils atteignent le standby. C'est implicitement toujours
    le cas avec un log-shipping s'appuyant sur les archives, car le standby
    ne récupère que des informations provenant de ces mêmes fichiers
    archivés. Ce n'est pas le cas dans le cadre de la réplication streaming.
    Lorsqu'un serveur est en standby il n'y a aucune différence entre les
    modes <literal>on</literal> et <literal>always</literal>.
   </para>
  </sect2>
 </sect1>

 <sect1 id="warm-standby-failover">
  <title>Bascule (<foreignphrase>Failover</foreignphrase>)</title>

  <para>
   Si le serveur primaire plante alors le serveur de standby devrait commencer
   les procédures de failover.
  </para>

  <para>
   Si le serveur de standby plante alors il n'est pas nécessaire d'effectuer un failover. Si le
   serveur de standby peut être redémarré, même plus tard, alors le processus de récupération
   peut aussi être redémarré au même moment, en bénéficiant du fait que la récupération sait reprendre
   où elle en était. Si le serveur de standby ne peut pas être redémarré, alors
   une nouvelle instance complète de standby devrait être créée.
  </para>

  <para>
   Si le serveur primaire plante, que le serveur de standby devient le
   nouveau primaire, et que l'ancien primaire redémarre, vous devez avoir
   un mécanisme pour informer l'ancien primaire qu'il n'est plus primaire. C'est aussi
   quelquefois appelé <acronym>STONITH</acronym> (Shoot The Other Node In The Head, ou
   Tire Dans La Tête De L'Autre Nœud), qui est nécessaire pour éviter les situations où
   les deux systèmes pensent qu'ils sont le primaire, ce qui amènerait de la confusion, et
   finalement de la perte de données.
  </para>

  <para>
   Beaucoup de systèmes de failover n'utilisent que deux systèmes, le primaire et le standby,
   connectés par un mécanisme de type ligne de vie (heartbeat) pour vérifier continuellement la
   connexion entre les deux et la viabilité du primaire. Il est aussi
   possible d'utiliser un troisième système (appelé un serveur témoin) pour éviter
   certains cas de bascule inappropriés, mais la complexité supplémentaire
   peut ne pas être justifiée à moins d'être mise en place avec suffisamment
   de précautions et des tests rigoureux.
  </para>

  <para>
   <productname>PostgreSQL</productname> ne fournit pas le logiciel
   système nécessaire pour identifier un incident sur le primaire et notifier
   le serveur de base de standby. De nombreux outils de ce genre existent et sont bien
   intégrés avec les fonctionnalités du système d'exploitation nécessaires à la bascule,
   telles que la migration d'adresse IP.
  </para>

  <para>
   Une fois que la bascule vers le standby se produit, il n'y a plus qu'un
   seul serveur en fonctionnement. C'est ce qu'on appelle un état dégradé.
   L'ancien standby est maintenant le primaire, mais l'ancien primaire est arrêté
   et pourrait rester arrêté. Pour revenir à un fonctionnement normal, un serveur
   de standby doit être recréé,
   soit sur l'ancien système primaire quand il redevient disponible, ou sur un troisième,
   peut être nouveau, système. L'utilitaire <xref linkend="app-pgrewind"/> peut être utilisé
   pour accélérer ce processus sur de gros clusters. Une fois que ceci est
   effectué, le primaire et le standby peuvent
   être considérés comme ayant changé de rôle. Certaines personnes choisissent d'utiliser un troisième
   serveur pour fournir une sauvegarde du nouveau primaire jusqu'à ce que le nouveau serveur de
   standby soit recréé,
   bien que ceci complique visiblement la configuration du système et les procédures d'exploitation.
  </para>

  <para>
   Par conséquent, basculer du primaire vers le serveur de standby peut être rapide mais requiert
   du temps pour re-préparer le cluster de failover. Une bascule régulière du
   primaire vers le standby est utile, car cela permet une période d'interruption de production sur
   chaque système pour maintenance. Cela vous permet aussi pour vous assurer que
   votre mécanisme de bascule fonctionnera réellement quand vous en aurez besoin.
   Il est conseillé que les procédures d'administration soient écrites.
  </para>

  <para>
   Pour déclencher le failover d'un serveur de standby en log-shipping,
   exécutez la commande <command>pg_ctl promote</command>, lancez la fonction
   <function>pg_promote()</function> ou créez un fichier trigger (déclencheur)
   avec le nom de fichier et le chemin spécifiés par le paramètre
   <varname>promote_trigger_file</varname>. Si vous comptez utiliser la commande
   <command>pg_ctl promote</command> ou la fonction
   <function>pg_promote()</function> pour effectuer la bascule, la variable
   <varname>promote_trigger_file</varname> n'est pas nécessaire. S'il s'agit
   d'ajouter des serveurs qui ne seront utilisés que pour alléger le serveur
   primaire des requêtes en lecture seule, et non pas pour des considérations
   de haute disponibilité, il n'est pas nécessaire de les réveiller
   (<foreignphrase>promote</foreignphrase>).
  </para>
 </sect1>

 <sect1 id="log-shipping-alternative">
  <title>Méthode alternative pour le log shipping</title>

  <para>
   Une alternative au mode de standby intégré décrit dans les sections
   précédentes est d'utiliser une <varname>restore_command</varname> qui
   scrute le dépôt d'archives. C'était la seule méthode disponible dans les
   versions 8.4 et inférieures. Voir le module <xref linkend="pgstandby"/>
   pour une implémentation de référence de ceci.
  </para>

  <para>
   Veuillez noter que dans ce mode, le serveur appliquera les WAL fichier par fichier,
   ce qui entraîne que si vous requêtez sur le serveur de standby (voir Hot Standby),
   il y a un délai entre une action sur le primaire et le moment où cette action
   devient visible sur le standby, correspondant au temps nécessaire à
   remplir le fichier de WAL. <varname>archive_timeout</varname>  peut être utilisé pour rendre ce délai
   plus court. Notez aussi que vous ne pouvez combiner la streaming replication avec cette méthode.
  </para>

  <para>
   Les opérations qui se produisent sur le primaire et les serveurs de standby sont
   des opérations normales d'archivage et de recovery. Le seul point de
   contact entre les deux serveurs de bases de données est l'archive de fichiers WAL
   qu'ils partagent&nbsp;: le primaire écrivant dans l'archive, le standby
   lisant de l'archive. Des précautions doivent être prises pour s'assurer que les archives WAL de serveurs
   primaires différents ne soient pas mélangées ou confondues. L'archive n'a pas besoin
   d'être de grande taille si elle n'est utilisée que pour le fonctionnement de standby.
  </para>

  <para>
   La magie qui permet aux deux serveurs faiblement couplés de fonctionner ensemble est
   une simple <varname>restore_command</varname> utilisée sur le standby qui
   quand on lui demande le prochain fichier de WAL, attend que le primaire le mette
   à disposition. La récupération normale
   demanderait un fichier de l'archive WAL, en retournant un échec si le
   fichier n'était pas disponible. Pour un fonctionnement en standby, il est normal que
   le prochain fichier WAL ne soit pas disponible, ce qui entraîne que le standby doive attendre
   qu'il apparaisse. Pour les fichiers se terminant en
   <literal>.history</literal> il n'y a pas besoin d'attendre, et un code retour
   différent de zéro doit être retourné. Une <varname>restore_command</varname>  d'attente
   peut être écrite comme un script qui boucle après avoir scruté l'existence du prochain fichier de WAL.
   Il doit aussi y avoir un moyen de déclencher la bascule, qui devrait interrompre la
   <varname>restore_command</varname> , sortir le la boucle et retourner une erreur file-not-found
   au serveur de standby. Cela met fin à la récupération et le standby démarrera alors comme un serveur normal.
  </para>

  <para>
   Le pseudocode pour une <varname>restore_command</varname> appropriée est:
   <programlisting>
triggered = false;
while (!NextWALFileReady() &amp;&amp; !triggered)
{
    sleep(100000L);         /* wait for ~0.1 sec */
    if (CheckForExternalTrigger())
        triggered = true;
}
if (!triggered)
        CopyWALFileForRecovery();
   </programlisting>
  </para>

  <para>
   Un exemple fonctionnel de <varname>restore_command</varname> d'attente est fournie
   par le module <xref linkend="pgstandby"/>. Il
   devrait être utilisé en tant que référence, comme la bonne façon d'implémenter correctement la logique
   décrite ci-dessus. Il peut aussi être étendu pour supporter des configurations et des
   environnements spécifiques.
  </para>

  <para>
   La méthode pour déclencher une bascule est une composante importante de la
   planification et de la conception. Une possibilité est d'utiliser la
   commande <varname>restore_command</varname>. Elle est exécutée une fois
   pour chaque fichier WAL, mais le processus exécutant la <varname>restore_command</varname>
   est créé et meurt pour chaque fichier, il n'y a donc ni démon ni processus serveur, et
   on ne peut utiliser ni signaux ni gestionnaire de signaux.  Par conséquent, la
   <varname>restore_command</varname> n'est pas appropriée pour déclencher la bascule.
   Il est possible d'utiliser une simple fonctionnalité de timeout, particulièrement
   si utilisée en conjonction avec un paramètre <varname>archive_timeout</varname>
   sur le primaire. Toutefois, ceci est sujet à erreur, un problème réseau
   ou un serveur primaire chargé pouvant suffire à déclencher une bascule. Un système
   de notification comme la création explicite d'un fichier trigger est idéale, dans la
   mesure du possible.
  </para>

  <sect2 id="warm-standby-config">
   <title>Implémentation</title>

   <para>
    La procédure simplifié pour configurer un serveur de test en utilisant cette
    méthode alternative est la suivante. Pour tous les détails
    sur chaque étape, référez vous aux sections précédentes suivant les indications.
    <orderedlist>
     <listitem>
      <para>
       Paramétrez les systèmes primaire et standby de façon aussi identique que possible,
       y compris deux copies identiques de <productname>PostgreSQL</productname> au même niveau
       de version.
      </para>
     </listitem>
     <listitem>
      <para>
       Activez l'archivage en continu du primaire vers un répertoire d'archives WAL
       sur le serveur de standby. Assurez vous que
       <xref linkend="guc-archive-mode"/>,
       <xref linkend="guc-archive-command"/> et
       <xref linkend="guc-archive-timeout"/>
       sont positionnés correctement sur le primaire
       (voir <xref linkend="backup-archiving-wal"/>).
      </para>
     </listitem>
     <listitem>
      <para>
       Effectuez une sauvegarde de base du serveur primaire (voir <xref
       linkend="backup-base-backup"/>), et chargez ces données sur le standby.
      </para>
     </listitem>
     <listitem>
      <para>
       Commencez la récupération sur le serveur de standby à partir de
       l'archive WAL locale, en utilisant un paramètre
       <varname>restore_command</varname> qui attend comme décrit précédemment
       (voir <xref linkend="backup-pitr-recovery"/>).
      </para>
     </listitem>
    </orderedlist>
   </para>

   <para>
    Le récupération considère l'archive WAL comme étant en lecture seule, donc une fois qu'un fichier WAL
    a été copié sur le système de standby il peut être copié sur bande en même temps
    qu'il est lu par le serveur de bases de données de standby.
    Ainsi, on peut faire fonctionner un serveur de standby pour de la haute disponibilité
    en même temps que les fichiers sont stockés pour de la reprise après sinistre.
   </para>

   <para>
    À des fins de test, il est possible de faire fonctionner le serveur primaire et
    de standby sur le même système. Cela n'apporte rien en termes de robustesse du serveur,
    pas plus que cela ne pourrait être décrit comme de la haute disponibilité.
   </para>
  </sect2>

  <sect2 id="warm-standby-record">
   <title>Log Shipping par Enregistrements</title>

   <para>
    Il est aussi possible d'implémenter du log shipping par enregistrements en utilisant
    cette méthode alternative, bien qu'elle nécessite des développements spécifiques,
    et que les modifications ne seront toujours visibles aux requêtes de hot standby qu'après
    que le fichier complet de WAL ait été recopié.
   </para>

   <para>
    Un programme externe peut appeler la fonction <function>pg_walfile_name_offset()</function>
    (voir <xref linkend="functions-admin"/>) pour obtenir le nom de fichier et la position exacte
    en octets dans ce fichier de la fin actuelle du WAL. Il peut alors accéder au fichier WAL directement
    et copier les données de la fin précédente connue à la fin courante vers les serveurs de standby.
    Avec cette approche, la fenêtre de perte de données est la période de scrutation du programme de copie,
    qui peut être très petite, et il n'y a pas de bande passante gaspillée en forçant l'archivage
    de fichiers WAL partiellement remplis. Notez que les scripts <varname>restore_command</varname>
    des serveurs de standby ne peuvent traiter que des fichiers WAL complets, les données copiées
    de façon incrémentale ne sont donc d'ordinaire pas mises à  disposition des serveurs de standby.
    Elles ne sont utiles que si le serveur primaire tombe &mdash; alors le dernier fichier WAL partiel
    est fourni au standby avant de l'autoriser à s'activer. L'implémentation correcte de ce
    mécanisme requiert la coopération entre le script <varname>restore_command</varname> et
    le programme de recopie des données.
   </para>

   <para>
    À partir de <productname>PostgreSQL</productname> version 9.0, vous pouvez utiliser
    la streaming replication (voir <xref linkend="streaming-replication"/>) pour
    bénéficier des mêmes fonctionnalités avec moins d'efforts.
   </para>
  </sect2>
 </sect1>

 <sect1 id="hot-standby">
  <title>Hot Standby</title>

  <indexterm zone="high-availability">
   <primary>Hot Standby</primary>
  </indexterm>

  <para>
   Hot Standby est le terme utilisé pour décrire la possibilité de se
   connecter et d'exécuter des requêtes en lecture seule alors que le
   serveur est en récupération d'archive ou mode standby. C'est
   utile à la fois pour la réplication et pour restaurer
   une sauvegarde à un état désiré avec une grande précision.
   Le terme Hot Standby fait aussi référence à la capacité du serveur à passer
   de la récupération au fonctionnement normal tandis-que les utilisateurs
   continuent à exécuter des requêtes et/ou gardent leurs connexions ouvertes.
  </para>

  <para>
   Exécuter des requêtes en mode hot standby est similaire au fonctionnement
   normal des requêtes, bien qu'il y ait quelques différences d'utilisation
   et d'administration notées ci-dessous.
  </para>

  <sect2 id="hot-standby-users">
   <title>Aperçu pour l'utilisateur</title>

   <para>
    Quand le paramètre <xref linkend="guc-hot-standby"/> est configuré à true
    sur un serveur en attente, le serveur commencera à accepter les connexions
    une fois que la restauration est parvenue à un état cohérent. Toutes les
    connexions qui suivront seront des connexions en lecture seule&nbsp;; même
    les tables temporaires ne pourront pas être utilisées.
   </para>

   <para>
    Les données sur le standby mettent un certain temps pour arriver du serveur
    primaire, il y aura donc un délai mesurable entre primaire et standby. La même
    requête exécutée presque simultanément sur le primaire et le standby pourrait par
    conséquent retourner des résultats différents. On dit que la donnée est
    <firstterm>cohérente à terme</firstterm>  avec le primaire. Une fois que
    l'enregistrement de validation (COMMIT) d'une transaction est rejoué sur
    le serveur en attente, les modifications réalisées par cette transaction
    seront visibles par toutes les images de bases obtenues par les transactions
    en cours sur le serveur en attente. Ces images peuvent être prises au début
    de chaque requête ou de chaque transaction, suivant le niveau d'isolation
    des transactions utilisé à ce moment. Pour plus de détails, voir <xref
    linkend="transaction-iso"/>.
   </para>

   <para>
    Les transactions exécutées pendant la période de restauration sur un
    serveur en mode hot standby peuvent inclure les commandes suivantes&nbsp;:
    <itemizedlist>
     <listitem>
      <para>
       Accès par requête&nbsp;: <command>SELECT</command>, <command>COPY TO</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de curseur&nbsp;: <command>DECLARE</command>, <command>FETCH</command>, <command>CLOSE</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Paramètres&nbsp;: <command>SHOW</command>, <command>SET</command>, <command>RESET</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de gestion de transaction&nbsp;:
       <itemizedlist>
        <listitem>
         <para>
          <command>BEGIN</command>, <command>END</command>, <command>ABORT</command>, <command>START TRANSACTION</command>
         </para>
        </listitem>
        <listitem>
         <para>
          <command>SAVEPOINT</command>, <command>RELEASE</command>, <command>ROLLBACK TO SAVEPOINT</command>
         </para>
        </listitem>
        <listitem>
         <para>
          Blocs d'<command>EXCEPTION</command> et autres sous-transactions internes
         </para>
        </listitem>
       </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK TABLE</command>, mais seulement quand explicitement dans un de ces modes:
       <literal>ACCESS SHARE</literal>, <literal>ROW SHARE</literal> ou <literal>ROW EXCLUSIVE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Plans et ressources&nbsp;: <command>PREPARE</command>, <command>EXECUTE</command>,
       <command>DEALLOCATE</command>, <command>DISCARD</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Plugins et extensions&nbsp;: <command>LOAD</command>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>UNLISTEN</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Les transactions lancées pendant la restauration d'un serveur en hot standby
    ne se verront jamais affectées un identifiant de transactions et ne peuvent
    pas être écrites dans les journaux de transactions. Du coup, les actions
    suivantes produiront des messages d'erreur&nbsp;:

    <itemizedlist>
     <listitem>
      <para>
       Langage de Manipulation de Données (LMD ou DML)&nbsp;: <command>INSERT</command>,
       <command>UPDATE</command>, <command>DELETE</command>, <command>COPY FROM</command>,
       <command>TRUNCATE</command>.
       Notez qu'il n'y a pas d'action autorisée qui entraînerait l'exécution d'un
       trigger pendant la récupération. Cette restriction s'applique même pour
       les tables temporaires car les lignes de ces tables ne peuvent être
       lues et écrites s'il n'est pas possible d'affecter un identifiant de
       transactions, ce qui n'est actuellement pas possible dans un
       environnement Hot Standby.
      </para>
     </listitem>
     <listitem>
      <para>
       Langage de Définition de Données (LDD ou DDL)&nbsp;: <command>CREATE</command>,
       <command>DROP</command>, <command>ALTER</command>, <command>COMMENT</command>.
       Cette restriction s'applique aussi aux tables temporaires car, pour
       mener à bien ces opérations, cela nécessiterait de mettre à jour les
       catalogues systèmes.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>SELECT ... FOR SHARE | UPDATE</command>, car les verrous de
       lignes ne peuvent pas être pris sans mettre à jour les fichiers de
       données.
      </para>
     </listitem>
     <listitem>
      <para>
       Règles sur des ordres <command>SELECT</command> qui génèrent des commandes LMD.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> qui demandent explicitement un mode supérieur à <literal>ROW EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LOCK</command> dans sa forme courte par défaut, puisqu'il demande <literal>ACCESS EXCLUSIVE MODE</literal>.
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de gestion de transaction qui positionnent explicitement un état n'étant pas en lecture-seule&nbsp;:
       <itemizedlist>
        <listitem>
         <para>
          <command>BEGIN READ WRITE</command>,
          <command>START TRANSACTION READ WRITE</command>
         </para>
        </listitem>
        <listitem>
         <para>
          <command>SET TRANSACTION READ WRITE</command>,
          <command>SET SESSION CHARACTERISTICS AS TRANSACTION READ WRITE</command>
         </para>
        </listitem>
        <listitem>
         <para>
          <command>SET transaction_read_only = off</command>
         </para>
        </listitem>
       </itemizedlist>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de two-phase commit&nbsp;: <command>PREPARE TRANSACTION</command>,
       <command>COMMIT PREPARED</command>, <command>ROLLBACK PREPARED</command>
       parce que même les transactions en lecture seule ont besoin d'écrire dans le WAL
       durant la phase de préparation (la première des deux phases du two-phase commit).
      </para>
     </listitem>
     <listitem>
      <para>
       Mise à jour de séquence&nbsp;: <function>nextval()</function>, <function>setval()</function>
      </para>
     </listitem>
     <listitem>
      <para>
       <command>LISTEN</command>, <command>NOTIFY</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Dans le cadre normal, les transactions <quote>en lecture seule</quote>
    permettent l'utilisation des instructions
    <command>LISTEN</command> et
    <command>NOTIFY</command>, donc les sessions Hot Standby ont des
    restrictions légèrement inférieures à celles de sessions en lecture seule
    ordinaires. Il est possible que certaines des restrictions soient encore
    moins importantes dans une prochaine version.
   </para>

   <para>
    Lors du fonctionnement en serveur hot standby, le paramètre
    <varname>transaction_read_only</varname> est toujours à true et ne peut
    pas être modifié. Tant qu'il n'y a pas de tentative de modification sur
    la base de données, les connexions sur un serveur en hot standby se
    comportent de façon pratiquement identiques à celles sur un serveur normal.
    Quand une bascule (<foreignphrase>failover</foreignphrase> ou
    <foreignphrase>switchover</foreignphrase>) survient, la base de données
    bascule dans le mode de traitement normal. Les sessions resteront
    connectées pendant le changement de mode. Quand le mode hot standby est
    terminé, il sera possible de lancer des transactions en lecture/écriture,
    y compris pour les sessions connectées avant la bascule.
   </para>

   <para>
    Les utilisateurs pourront déterminer si leur session est en lecture seule en
    exécutant <command>SHOW transaction_read_only</command>. De plus, un jeu de
    fonctions (<xref linkend="functions-recovery-info-table"/>) permettent aux utilisateurs d'
    accéder à des informations à propos du serveur de standby. Ceci vous permet d'écrire
    des programmes qui sont conscients de l'état actuel de la base. Vous pouvez
    vous en servir pour superviser l'avancement de la récupération, ou pour écrire des
    programmes complexes qui restaurent la base dans des états particuliers.
   </para>
  </sect2>

  <sect2 id="hot-standby-conflict">
   <title>Gestion des conflits avec les requêtes</title>

   <para>
    Les nœuds primaire et standby sont faiblement couplés à bien des égards. Des
    actions sur le primaire auront un effet sur le standby. Par conséquent, il y a
    un risque d'interactions négatives ou de conflits entre eux. Le conflit le
    plus simple à comprendre est la performance : si un gros chargement de données a
    lieu sur le primaire, il générera un flux similaire d'enregistrements WAL sur le
    standby, et les requêtes du standby pourrait entrer en compétition pour les ressources
    systèmes, comme les entrées-sorties.
   </para>

   <para>
    Il y a aussi d'autres types de conflits qui peuvent se produire avec le
    Hot Standby. Ces conflits sont des <emphasis>conflits durs</emphasis> dans
    le sens où des requêtes pourraient devoir être annulées et, dans certains
    cas, des sessions déconnectées, pour les résoudre. L'utilisateur dispose
    de plusieurs moyens pour gérer ces conflits. Voici les différents cas de
    conflits possibles&nbsp;:

    <itemizedlist>
     <listitem>
      <para>
       Des verrous en accès exclusif pris sur le serveur primaire, incluant à
       la fois les commandes <command>LOCK</command> exclusives et quelques
       actions de type <acronym>DDL</acronym>, entrent en conflit avec les
       accès de table des requêtes en lecture seule.
      </para>
     </listitem>
     <listitem>
      <para>
       La suppression d'un tablespace sur le serveur primaire entre en conflit
       avec les requêtes sur le serveur standby qui utilisent ce tablespace
       pour les fichiers temporaires.
      </para>
     </listitem>
     <listitem>
      <para>
       La suppression d'une base de données sur le serveur primaire entre en
       conflit avec les sessions connectées sur cette base de données sur
       le serveur en attente.
      </para>
     </listitem>
     <listitem>
      <para>
       La copie d'un enregistrement nettoyé par un VACUUM entre en conflit
       avec les transactions sur le serveur en attente qui peuvent toujours
       <quote>voir</quote> au moins une des lignes à supprimer.
      </para>
     </listitem>
     <listitem>
      <para>
       La copie d'un enregistrement nettoyé par un VACUUM entre en conflit
       avec les requêtes accédant à la page cible sur le serveur en attente,
       qu'elles voient ou non les données à supprimer.
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Sur le serveur primaire, ces cas résultent en une attente
    supplémentaire&nbsp;; l'utilisateur peut choisir d'annuler une des actions
    en conflit. Néanmoins, sur le serveur en attente, il n'y a pas de choix
    possibles&nbsp;: l'action enregistrée dans les journaux de transactions
    est déjà survenue sur le serveur primaire et le serveur en standby doit
    absolument réussir à l'appliquer. De plus, permettre que l'enregistrement
    de l'action attende indéfiniment pourrait avoir des effets fortement non
    désirables car le serveur en attente sera de plus en plus en retard par
    rapport au primaire. Du coup, un mécanisme est fourni pour forcer
    l'annulation des requêtes sur le serveur en attente qui entreraient en
    conflit avec des enregistrements des journaux de transactions en attente.
   </para>

   <para>
    Voici un exemple de problème type&nbsp;: un administrateur exécute un
    <command>DROP TABLE</command> sur une table du serveur primaire qui est
    actuellement utilisé dans des requêtes du serveur en attente. Il est clair
    que la requête ne peut pas continuer à s'exécuter si l'enregistrement
    dans les journaux de transactions, correspondant au <command>DROP
     TABLE</command> est appliqué sur le serveur en attente. Si cette situation
    survient sur le serveur primaire, l'instruction <command>DROP TABLE</command>
    attendra jusqu'à ce que l'autre requête se termine. Par contre, quand le
    <command>DROP TABLE</command> est exécuté sur le serveur primaire, ce dernier
    ne sait pas les requêtes en cours d'exécution sur le serveur en attente,
    donc il n'attendra pas la fin de l'exécution des requêtes sur le serveur
    en attente. L'enregistrement de cette modification dans les journaux de
    transactions arrivera au serveur en attente alors que la requête sur le
    serveur en attente est toujours en cours d'exécution, causant un conflit.
    Le serveur en attente doit soit retarder l'application des enregistrements
    des journaux de transactions (et tous ceux qui sont après aussi) soit
    annuler la requête en conflit, pour appliquer l'instruction <command>DROP
     TABLE</command>.
   </para>

   <para>
    Quand une requête en conflit est courte, il est généralement préférable
    d'attendre un peu pour l'application du journal de transactions. Mais un
    délai plus long n'est généralement pas souhaitable. Donc, le mécanisme
    d'annulation dans l'application des enregistrements de journaux de
    transactions dispose de deux paramètres, <xref
    linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/>, qui définissent le délai
    maximum autorisé pour appliquer les enregistrements. Les requêtes en
    conflit seront annulées si l'application des enregistrements prend plus de
    temps que celui défini. Il existe deux paramètres pour que des délais
    différents puissent être observés suivant le cas&nbsp;: lecture des
    enregistrements à partir d'un journal archivé (par exemple lors de la
    restauration initiale à partir d'une sauvegarde ou lors d'un
    <quote>rattrapage</quote> si le serveur en attente accumulait du retard
    par rapport au primaire) et lecture des enregistrements à partir de la
    réplication en flux.
   </para>

   <para>
    Pour un serveur en attente dont le but principal est la haute-disponibilité,
    il est préférable de configurer des valeurs assez basses pour les
    paramètres de délai, de façon à ce que le serveur en attente ne soit pas
    trop en retard par rapport au serveur primaire à cause des délais suivis à
    cause des requêtes exécutées sur le serveur en attente. Par contre, si
    le serveur en attente doit exécuter des requêtes longues, alors une valeur
    haute, voire infinie, du délai pourrait être préférable. Néanmoins, gardez
    en tête qu'une requête mettant du temps à s'exécuter pourrait empêcher
    les autres requêtes de voir les modifications récentes sur le serveur
    primaire si elle retarde l'application des enregistrements de journaux
    de transactions.
   </para>

   <para>
    Une fois que le délai spécifié par
    <varname>max_standby_archive_delay</varname> ou
    <varname>max_standby_streaming_delay</varname> a été dépassé, toutes les
    requêtes en conflit seront annulées. Ceci résulte habituellement en une
    erreur d'annulation, bien que certains cas, comme un <command>DROP
     DATABASE</command>, peuvent occasionner l'arrêt complet de la connexion.
    De plus, si le conflit intervient sur un verrou détenu par une transaction
    en attente, la session en conflit sera terminée (ce comportement pourrait
    changer dans le futur).
   </para>

   <para>
    Les requêtes annulées peuvent être ré-exécutées immédiatement (après avoir
    commencé une nouvelle transaction, bien sûr). Comme l'annulation des
    requêtes dépend de la nature des enregistrements dans le journal de
    transactions, une requête annulée pourrait très bien réussir si elle est
    de nouveau exécutée.
   </para>

   <para>
    Gardez en tête que les paramètres de délai sont comparés au temps passé
    depuis que la donnée du journal de transactions a été reçue par le serveur
    en attente. Du coup, la période de grâce accordée aux requêtes n'est jamais
    supérieure au paramètre de délai, et peut être considérablement inférieure
    si le serveur en attente est déjà en retard suite à l'attente de la fin
    de l'exécution de requêtes précédentes ou suite à son impossibilité de
    conserver le rythme d'une grosse mise à jour.
   </para>

   <para>
    La raison la plus fréquente des conflits entre les requêtes en lecture
    seule et le rejeu des journaux de transactions est le <quote>nettoyage
     avancé</quote>. Habituellement, <productname>PostgreSQL</productname>
    permet le nettoyage des anciennes versions de lignes quand aucune
    transaction ne peut les voir pour s'assurer du respect des règles de MVCC.
    Néanmoins, cette règle peut seulement s'appliquer sur les transactions
    exécutées sur le serveur primaire. Donc il est possible que le nettoyage
    effectué sur le primaire supprime des versions de lignes toujours visibles
    sur une transaction exécutée sur le serveur en attente.
   </para>

   <para>
    Les utilisateurs expérimentés peuvent noter que le nettoyage des versions
    de ligne ainsi que le gel des versions de ligne peuvent potentiellement
    avoir un conflit avec les requêtes exécutées sur le serveur en attente.
    L'exécution d'un <command>VACUUM FREEZE</command> manuel a de grandes
    chances de causer des conflits, y compris sur les tables sans lignes mises
    à jour ou supprimées.
   </para>

   <para>
    Les utilisateurs doivent s'attendre à ce que les tables fréquemment mises
    à jour sur le serveur primaire seront aussi fréquemment la cause de
    requêtes annulées sur le serveur en attente. Dans un tel cas, le
    paramétrage d'une valeur finie pour
    <varname>max_standby_archive_delay</varname> ou
    <varname>max_standby_streaming_delay</varname> peut être considéré comme
    similaire à la configuration de <varname>statement_timeout</varname>.
   </para>

   <para>
    Si le nombre d'annulations de requêtes sur le serveur en attente est
    jugé inadmissible, quelques solutions existent. La première option est de définir la variable
    <varname>hot_standby_feedback</varname> qui permet d'empêcher les conflits liés au nettoyage
    opéré par la commande <command>VACUUM</command> en lui interdisant de nettoyer les lignes récemment supprimées. Si
    vous le faites, vous devez noter que cela retardera le nettoyage des
    versions de lignes mortes sur le serveur primaire, ce qui pourrait résulter
    en une fragmentation non désirée de la table. Néanmoins, cette situation
    ne sera pas meilleure si les requêtes du serveur en attente s'exécutaient
    directement sur le serveur primaire. Vous avez toujours le bénéfice de
    l'exécution sur un serveur distant. Si des serveurs en standby se
    connectent et se déconnectent fréquemment, vous pourriez vouloir faire des
    ajustements pour gérer la période durant laquelle <varname>hot_standby_feedback</varname>
    n'est pas renvoyé. Par exemple, vous pouvez considérer l'augmentation de
    <varname>max_standby_archive_delay</varname> pour que les requêtes ne
    soient pas annulées rapidement par des conflits avec le journal de transactions
    d'archive durant les périodes de déconnexion.  Vous pouvez également
    considérer l'augmentation de <varname>max_standby_streaming_delay</varname>
    pour éviter des annulations rapides par les nouvelles données de flux de
    transaction après la reconnexion.
   </para>

   <para>
    Une autre option revient à augmenter <xref
    linkend="guc-vacuum-defer-cleanup-age"/> sur le serveur primaire, pour que
    les lignes mortes ne soient pas nettoyées aussi rapidement que d'habitude.
    Cela donnera plus de temps aux requêtes pour s'exécuter avant d'être
    annulées sur le serveur en attente, sans voir à configurer une valeur
    importante de <varname>max_standby_streaming_delay</varname>. Néanmoins,
    il est difficile de garantir une fenêtre spécifique de temps d'exécution
    avec cette approche car <varname>vacuum_defer_cleanup_age</varname> est
    mesuré en nombre de transactions sur le serveur primaire.
   </para>

   <para>
    Le nombre de requêtes annulées et le motif de cette annulation peut être visualisé avec
    la vue système <structname>pg_stat_database_conflicts</structname> sur le serveur de
    standby. La vue système <structname>pg_stat_database</structname> contient aussi
    des informations synthétiques sur ce sujet.
   </para>
  </sect2>

  <sect2 id="hot-standby-admin">
   <title>Aperçu pour l'administrateur</title>

   <para>
    Si <varname>hot_standby</varname> est positionné à <literal>on</literal>
    dans <filename>postgresql.conf</filename> (valeur par défaut) et qu'un
    fichier <filename>recovery.signal</filename> est présent, le serveur
    fonctionnera en mode Hot Standby.
    Toutefois, il pourrait s'écouler du temps avant que les connections en
    Hot Standby soient autorisées, parce que le serveur n'acceptera pas de connexions tant
    que la récupération n'aura pas atteint un point garantissant un état cohérent permettant
    aux requêtes de s'exécuter. Pendant cette période, les clients qui tentent de se connecter
    seront rejetés avec un message d'erreur.
    Pour confirmer que le serveur a démarré, vous pouvez soit tenter de vous connecter en
    boucle, ou rechercher ces messages dans les journaux du serveur:

    <programlisting>
LOG:  entering standby mode

... puis, plus loin ...

LOG:  consistent recovery state reached
LOG:  database system is ready to accept read only connections
    </programlisting>

    L'information sur la cohérence est enregistrée une fois par checkpoint sur le primaire.
    Il n'est pas possible d'activer le hot standby si on lit des WAL générés durant
    une période pendant laquelle <varname>wal_level</varname> n'était pas positionné
    à <literal>replica</literal> ou <literal>logical</literal> sur le primaire. L'arrivée à un état cohérent
    peut aussi être retardée si ces deux conditions se présentent:

    <itemizedlist>
     <listitem>
      <para>
       Une transaction en écriture a plus de 64 sous-transactions
      </para>
     </listitem>
     <listitem>
      <para>
       Des transactions en écriture ont une durée très importante
      </para>
     </listitem>
    </itemizedlist>

    Si vous effectuez du log shipping par fichier ("warm standby"), vous pourriez
    devoir attendre jusqu'à l'arrivée du prochain fichier de WAL, ce qui pourrait
    être aussi long que le paramètre <varname>archive_timeout</varname> du primaire.
   </para>

   <para>
    Certains paramètres sur le standby vont devoir être revus si ils ont été
    modifiés sur le primaire. Pour ces paramètres, la valeur sur le standby
    devra être égale ou supérieure à celle du primaire. De ce fait, si vous
    voulez augmenter ces valeurs, vous devez le faire d'abord sur tous les
    serveurs standbys avant de le faire sur le serveur primaire. De la même
    façon, si vous voulez diminuer ces valeurs, vous devez tout d'abord le
    faire sur le serveur primaire, puis sur tous les serveurs secondaires. Si
    ces paramètres ne sont pas suffisamment élevés le standby refusera de
    démarrer. Il est tout à fait possible de fournir de nouvelles valeurs plus
    élevées et de redémarrer le serveur pour reprendre la récupération. Ces
    paramètres sont les suivants:

    <itemizedlist>
     <listitem>
      <para>
       <varname>max_connections</varname>
      </para>
     </listitem>
     <listitem>
      <para>
       <varname>max_prepared_transactions</varname>
      </para>
     </listitem>
     <listitem>
      <para>
       <varname>max_locks_per_transaction</varname>
      </para>
     </listitem>
     <listitem>
      <para>
       <varname>max_wal_senders</varname>
      </para>
     </listitem>
     <listitem>
      <para>
       <varname>max_worker_processes</varname>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Il est important que l'administrateur sélectionne le paramétrage approprié
    pour <xref linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/>. Le meilleur choix varie les
    priorités. Par exemple, si le serveur a comme tâche principale d'être un
    serveur de haute-disponibilité, alors il est préférable d'avoir une
    configuration assez basse, voire à zéro, de ces paramètres. Si le serveur
    en attente est utilisé comme serveur supplémentaire pour des requêtes du
    type décisionnel, il sera acceptable de mettre les paramètres de délai à
    des valeurs allant jusqu'à plusieurs heures, voire même -1 (cette valeur
    signifiant qu'il est possible d'attendre que les requêtes se terminent
    d'elles-même).
   </para>

   <para>
    Les "hint bits" (bits d'indices) écrits sur le primaire ne sont pas journalisés en WAL,
    il est donc probable que les hint bits soient réécrits sur le standby. Ainsi,
    le serveur de standby fera toujours des écritures disques même si tous les utilisateurs
    sont en lecture seule; aucun changement ne se produira sur les données elles mêmes.
    Les utilisateurs écriront toujours les fichiers temporaires pour les gros tris et
    re-génèreront les fichiers d'information relcache, il n'y a donc pas de morceau de la base
    qui soit réellement en lecture seule en mode hot standby.
    Notez aussi que les écritures dans des bases distantes en utilisant le module
    <application>dblink</application>, et d'autres opérations en dehors de la base s'appuyant sur
    des fonctions PL seront toujours possibles, même si la transaction est en lecture seule localement.
   </para>

   <para>
    Les types suivants de commandes administratives ne sont pas acceptées
    durant le mode de récupération:

    <itemizedlist>
     <listitem>
      <para>
       Langage de Définition de Données (LDD ou DDL)&nbsp;: comme <command>CREATE INDEX</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Droits et propriété&nbsp;: <command>GRANT</command>, <command>REVOKE</command>,
       <command>REASSIGN</command>
      </para>
     </listitem>
     <listitem>
      <para>
       Commandes de maintenance&nbsp;: <command>ANALYZE</command>, <command>VACUUM</command>,
       <command>CLUSTER</command>, <command>REINDEX</command>
      </para>
     </listitem>
    </itemizedlist>
   </para>

   <para>
    Notez encore une fois que certaines de ces commandes sont en fait
    autorisées durant les transactions en "lecture seule" sur le primaire.
   </para>

   <para>
    Par conséquent, vous ne pouvez pas créer d'index supplémentaires qui existeraient
    uniquement sur le standby, ni des statistiques qui n'existeraient que sur le standby.
    Si ces commandes administratives sont nécessaires, elles doivent être exécutées
    sur le primaire, et ces modifications se propageront à terme au standby.
   </para>

   <para>
    <function>pg_cancel_backend()</function> et
    <function>pg_terminate_backend()</function> fonctionneront sur les
    processus utilisateurs, mais pas sur les processus de démarrage, qui
    effectuent la récupération. <structname>pg_stat_activity</structname> ne
    montre pas les transactions de récupération comme actives. Ainsi,
    <structname>pg_prepared_xacts</structname> est toujours vide durant la
    récupération. Si vous voulez traiter des transactions préparées douteuses,
    interrogez <structname>pg_prepared_xacts</structname>  sur le primaire, et
    exécutez les commandes pour résoudre le problème à cet endroit ou
    résolvez-les après la fin de la restauration.
   </para>

   <para>
    <structname>pg_locks</structname> affichera les verrous possédés par les processus,
    comme en temps normal. <structname>pg_locks</structname> affiche aussi une transaction
    virtuelle gérée par le processus de démarrage qui possède tous les
    <literal>AccessExclusiveLocks</literal> possédés par les transactions rejouées par la récupération.
    Notez que le processus de démarrage n'acquiert pas de verrou pour effectuer les modifications à
    la base, et que par conséquent les verrous autre que <literal>AccessExclusiveLocks</literal>
    ne sont pas visibles dans <structname>pg_locks</structname> pour le processus de démarrage;
    ils sont simplement censés exister.
   </para>

   <para>
    Le plugin <productname>Nagios</productname> <productname>check_pgsql</productname> fonctionnera,
    parce que les informations simples qu'il vérifie existent.
    Le script de supervision <productname>check_postgres</productname> fonctionnera aussi,
    même si certaines valeurs retournées pourraient être différentes ou sujettes à confusion.
    Par exemple, la date de dernier vacuum ne sera pas mise à jour, puisqu'aucun vacuum ne se déclenche
    sur le standby. Les vacuums s'exécutant sur le primaire envoient toujours leurs modifications
    au standby.
   </para>

   <para>
    Les options de contrôle des fichiers de WAL ne fonctionneront pas durant la récupération,
    comme <function>pg_start_backup</function>, <function>pg_switch_wal</function>, etc...
   </para>

   <para>
    Les modules à chargement dynamique fonctionnent, comme <structname>pg_stat_statements</structname>.
   </para>

   <para>
    Les verrous consultatifs fonctionnent normalement durant la récupération,
    y compris en ce qui concerne la détection des verrous mortels (deadlocks).
    Notez que les verrous consultatifs ne sont jamais tracés dans les WAL, il est
    donc impossible pour un verrou consultatif sur le primaire ou le standby
    d'être en conflit avec la ré-application des WAL. Pas plus qu'il n'est
    possible d'acquérir un verrou consultatif sur le primaire et que celui-ci
    initie un verrou consultatif similaire sur le standby. Les verrous consultatifs
    n'ont de sens que sur le serveur sur lequel ils sont acquis.
   </para>

   <para>
    Les systèmes de réplications à base de triggers tels que <productname>Slony</productname>,
    <productname>Londiste</productname> et <productname>Bucardo</productname>
    ne fonctionneront pas sur le standby du tout, même s'ils fonctionneront sans problème
    sur le serveur primaire tant que les modifications ne sont pas envoyées sur le serveur standby
    pour y être appliquées. Le rejeu de WAL n'est pas à base de triggers, vous ne pouvez
    donc pas utiliser le standby comme relais vers un système qui aurait besoin d'écritures supplémentaires
    ou utilise des triggers.
   </para>

   <para>
    Il n'est pas possible d'assigner de nouveaux OID, bien que des générateurs d' <acronym>UUID</acronym>
    puissent tout de même fonctionner, tant qu'ils n'ont pas besoin d'écrire un nouveau statut dans
    la base.
   </para>

   <para>
    À l'heure actuelle, la création de table temporaire n'est pas autorisée durant les
    transactions en lecture seule, certains scripts existants pourraient donc
    ne pas fonctionner correctement. Cette restriction pourrait être levée dans une
    version ultérieure. Il s'agit à la fois d'un problème de respect des standards
    et un problème technique.
   </para>

   <para>
    <command>DROP TABLESPACE</command> ne peut réussir que si le tablespace est vide.
    Certains utilisateurs pourraient utiliser de façon active le tablespace via leur
    paramètre <varname>temp_tablespaces</varname>. S'il y a des fichiers temporaires
    dans le tablespace, toutes les requêtes actives sont annulées pour s'assurer que les
    fichiers temporaires sont supprimés, afin de supprimer le tablespace et de continuer
    l'application des WAL.
   </para>

   <para>
    Exécuter <command>DROP DATABASE</command> ou <command>ALTER DATABASE ...
     SET TABLESPACE</command> sur
    le serveur primaire générera un enregistrement dans les journaux de
    transactions qui causera la déconnexion de tous les utilisateurs
    actuellement connectés à cette base de données. Cette action survient
    immédiatement, quelque soit la valeur du paramètre
    <varname>max_standby_streaming_delay</varname>. Notez que
    <command>ALTER DATABASE ... RENAME</command> ne déconnecte pas les
    utilisateurs qui, dans la plupart des cas, ne s'en apercevront pas. Cela
    peut néanmoins confondre un programme qui dépendrait du nom de la base.
   </para>

   <para>
    En fonctionnement normal (pas en restauration), si vous exécutez
    <command>DROP USER</command> ou <command>DROP ROLE</command>
    pour un rôle ayant l'attribut LOGIN alors que cet utilisateur est toujours
    connecté alors rien ne se produit pour cet utilisateur connecté &mdash; il reste connecté. L'utilisateur
    ne peut toutefois pas se reconnecter. Ce comportement est le même en récupération, un
    <command>DROP USER</command> sur le primaire ne déconnecte donc pas cet utilisateur sur le standby.
   </para>

   <para>
    Le collecteur de statistiques est actif durant la récupération. Tous les parcours,
    lectures, utilisations de blocs et d'index, etc... seront enregistrés normalement
    sur le standby. Les actions rejouées ne dupliqueront pas leur effets sur le primaire,
    l'application d'insertions n'incrémentera pas la colonne Inserts de pg_stat_user_tables.
    Le fichier de statistiques est effacé au démarrage de la récupération, les statistiques
    du primaire et du standby différeront donc; c'est vu comme une fonctionnalité, pas un bug.
   </para>

   <para>
    Autovacuum n'est pas actif durant la récupération, il démarrera normalement
    à la fin de la récupération.
   </para>

   <para>
    Les processus d'écriture en arrière plan (checkpointer et background
    writer) sont actifs durant la restauration. Le processus checkpointer
    process effectuera les restartpoints (similaires aux checkpoints sur le
    primaire) et le processus background writer réalisera les activités
    normales de nettoyage de blocs. Ceci peut inclure la mise à jour des
    information de hint bit des données du serveur de standby. La commande
    <command>CHECKPOINT</command>  est acceptée pendant la récupération, bien
    qu'elle déclenche un restartpoint et non un checkpoint.
   </para>
  </sect2>

  <sect2 id="hot-standby-parameters">
   <title>Référence des paramètres de Hot Standby</title>

   <para>
    De nombreux paramètres ont été mentionnés ci-dessus dans
    <xref linkend="hot-standby-conflict"/>
    et <xref linkend="hot-standby-admin"/>.
   </para>

   <para>
    Sur le primaire, les paramètres <xref linkend="guc-wal-level"/> et
    <xref linkend="guc-vacuum-defer-cleanup-age"/> peuvent être utilisés.
    <xref linkend="guc-max-standby-archive-delay"/> et <xref
    linkend="guc-max-standby-streaming-delay"/> n'ont aucun effet sur le primaire.
   </para>

   <para>
    Sur le serveur en attente, les paramètres <xref linkend="guc-hot-standby"/>,
    <xref linkend="guc-max-standby-archive-delay"/> et
    <xref linkend="guc-max-standby-streaming-delay"/> peuvent être utilisés.
    <xref linkend="guc-vacuum-defer-cleanup-age"/> n'a pas d'effet tant que
    le serveur reste dans le mode standby, mais deviendra important quand le
    serveur en attente deviendra un serveur primaire.
   </para>
  </sect2>

  <sect2 id="hot-standby-caveats">
   <title>Avertissements</title>

   <para>
    Il y a plusieurs limitations au Hot Standby.
    Elles peuvent et seront probablement résolues dans des versions ultérieures:

    <itemizedlist>
     <listitem>
      <para>
       Une connaissance complète des transactions en cours d'exécution est nécessaire
       avant de pouvoir déclencher des instantanés. Des transactions utilisant un
       grand nombre de sous-transactions (à l'heure actuelle plus de 64) retarderont
       le démarrage des connexions en lecture seule jusqu'à complétion de la plus
       longue transaction en écriture. Si cette situation se produit, des messages
       explicatifs seront envoyés dans la trace du serveur.
      </para>
     </listitem>
     <listitem>
      <para>
       Des points de démarrage valides pour les requêtes de standby sont générés
       à chaque checkpoint sur le primaire. Si le standby est éteint alors
       que le primaire est déjà éteint, il est tout à fait possible ne pas pouvoir
       repasser en Hot Standby tant que le primaire n'aura pas été redémarré, afin
       qu'il génère de nouveaux points de démarrage dans les journaux WAL. Cette situation
       n'est pas un problème dans la plupart des situations où cela pourrait se produire.
       Généralement, si le primaire est éteint et plus disponible, c'est probablement
       en raison d'un problème sérieux qui va de toutes façons forcer la conversion
       du standby en primaire. Et dans des situations où le primaire est éteint
       intentionnellement, la procédure standard est de promouvoir le primaire.
      </para>
     </listitem>
     <listitem>
      <para>
       À la fin de la récupération, les <literal>AccessExclusiveLocks</literal> possédés
       par des transactions préparées nécessiteront deux fois le nombre d'entrées normal dans la
       table de verrous. Si vous pensez soit exécuter un grand nombre de transactions préparées
       prenant des <literal>AccessExclusiveLocks</literal>, ou une grosse transaction prenant
       beaucoup de <literal>AccessExclusiveLocks</literal>, il est conseillé d'augmenter la valeur
       de <varname>max_locks_per_transaction</varname>, peut-être jusqu'à une valeur double
       de celle du serveur primaire. Vous n'avez pas besoin de prendre ceci en compte
       si votre paramètre <varname>max_prepared_transactions</varname> est 0.
      </para>
     </listitem>
     <listitem>
      <para>
       Il n'est pas encore possible de passer une transaction en mode d'isolation sérialisable
       tout en supportant le hot standby (voir <xref linkend="xact-serializable"/> et
       <xref linkend="serializable-consistency"/> pour plus de détails).
       Une tentative de modification du niveau d'isolation d'une transaction à sérialisable
       en hot standby générera une erreur.
      </para>
     </listitem>
    </itemizedlist>
   </para>
  </sect2>

 </sect1>

</chapter>
