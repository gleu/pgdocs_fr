<?xml version="1.0" encoding="UTF-8"?>
<chapter id="wal">
 <title>Fiabilité et journaux de transaction</title>

 <para>
  Ce chapitre explique comment les journaux de transaction sont utilisés pour
  obtenir des traitements efficaces et fiables.
 </para>

 <sect1 id="wal-reliability">
  <title>Fiabilité</title>

  <para>
   La fiabilité est une propriété importante de tout système de base de
   données sérieux. <productname>PostgreSQL</productname> fait tout
   son possible pour garantir un fonctionnement fiable. Un des aspects
   de la fiabilité est de stocker toutes les données validées par une
   transaction dans un espace non volatile,
   insensible aux coupures de courant, aux bogues du système d'exploitation
   et aux problèmes matériels (sauf en cas de problème sur l'espace non
   volatile, bien sûr).
   Pour cela, il est habituellement suffisant d'écrire avec succès
   les données sur le stockage permanent de l'ordinateur
   (disque dur ou un équivalent).
   En fait, même si un ordinateur est irrémédiablement hors d'usage,
   si le disque dur survit, ces données peuvent être déplacées vers un autre
   ordinateur au matériel similaire et toutes les transactions validées
   resteront intactes.
  </para>

  <para>
   Bien que forcer l'enregistrement des données périodiquement sur le disque
   semble être une opération simple, ce n'est pas le cas. Comme les disques
   durs sont beaucoup plus lents que la mémoire principale et les
   processeurs, plusieurs niveaux de cache existent entre la mémoire principale
   de l'ordinateur et les disques.
   Tout d'abord, il y a le tampon cache du
   système d'exploitation, qui met en cache les blocs disque fréquemment utilisés
   et combine les écritures.
   Heureusement, tous les systèmes
   d'exploitation fournissent aux applications un moyen
   de forcer les écritures du cache vers
   le disque et <productname>PostgreSQL</productname> utilise ces fonctions
   (voir le paramètre <xref linkend="guc-wal-sync-method"/>
   pour en ajuster le fonctionnement).
  </para>

  <para>
   Ensuite, il peut y avoir un cache dans le contrôleur du disque dur&nbsp;;
   ceci est assez courant sur les cartes contrôleur <acronym>RAID</acronym>. Certains
   de ces caches sont <firstterm>write-through</firstterm>, ce qui signifie
   que les écritures sont envoyées au disque dès qu'elles arrivent.
   D'autres sont  <firstterm>write-back</firstterm>, ce qui veut dire
   que les données sont envoyées au lecteur un peu plus tard.
   De tels caches peuvent être un danger car
   la mémoire cache du contrôleur du disque est volatile&nbsp;
   elle perdra donc son contenu à la prochaine coupure de courant.
   Des cartes contrôleur de meilleure qualité ont des
   caches <firstterm>avec batterie</firstterm> (<acronym>BBU</acronym>),
   c'est-à-dire que la carte
   dispose d'une batterie qui maintient l'alimentation du cache
   en cas de coupure de courant.
   Une fois celui-ci revenu, les données seront écrites sur les
   disques durs.
  </para>

  <para>
   Et enfin, la plupart des disques durs ont des caches. Certains sont
   <foreignphrase>write-through</foreignphrase> alors que d'autres sont
   <foreignphrase>write-back</foreignphrase>, et les mêmes problèmes se
   posent pour les caches <foreignphrase>write-back</foreignphrase> des
   disques que pour ceux de contrôleurs.
   Les disques IDE et SATA grand public en particulier sont susceptibles
   d'avoir des caches <quote>write-back</quote> qui ne survivront pas
   à une perte de courant. De nombreux SSD sont aussi dotés de caches
   <quote>write-back</quote> volatiles.
  </para>

  <para>
   Ces caches peuvent généralement être désactivés. Néanmoins, la méthode pour
   le faire dépend du système d'exploitation et du type de disque&nbsp;:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Sur <productname>Linux</productname>, on peut interroger les disques
     IDE et SATA avec la commande <command>hdparm -I</command>&nbsp;; le
     cache en écriture est activé si une étoile (<literal>*</literal>) se
     trouve derrière le texte <literal>Write cache</literal>.
     <command>hdparm -W 0</command> peut être utilisé pour désactiver le
     cache en écriture. Les disques SCSI peuvent être vérifiés en utilisant
     <ulink url="http://sg.danny.cz/sg/sdparm.html"><application>sdparm</application></ulink>.
     Utilisez <command>sdparm --get=WCE</command> pour voir si le
     cache en écriture est activé et <command>sdparm --clear=WCE</command>
     pour le désactiver.
    </para>
   </listitem>

   <listitem>
    <para>
     Sur <productname>FreeBSD</productname>, les disques IDE peuvent être
     interrogés avec <command>atacontrol</command> et le cache en écriture
     désactivé avec <literal>hw.ata.wc=0</literal> dans le fichier de
     configuration <filename>/boot/loader.conf</filename>&nbsp;; les disques
     SCSI peuvent être interrogés avec <command>camcontrol
      identify</command>, et le cache en écriture peut être consulté et
     modifié en utilisant <command>sdparm</command> quand cette
     commande est disponible.
    </para>
   </listitem>

   <listitem>
    <para>
     Sur <productname>Solaris</productname>, le cache disque en écriture
     est contrôlé par <command>format -e</command>.
     (Le système de fichiers Solaris <acronym>ZFS</acronym> est sûr
     même quand le cache disque en écriture est activé car il exécute
     ses propres commandes de vidage du cache.)
    </para>
   </listitem>

   <listitem>
    <para>
     Sur <productname>Windows</productname>, si
     <varname>wal_sync_method</varname> vaut <literal>open_datasync</literal>
     (la valeur par défaut), le cache en écriture peut être désactivé
     en décochant <literal>My Computer\Open\<replaceable>disk
       drive</replaceable>\Properties\Hardware\Properties\Policies\Enable
      write caching on the disk</literal>.
     Ou bien configurez <varname>wal_sync_method</varname> à
     <literal>fsync</literal> ou <literal>fsync_writethrough</literal> pour
     désactiver le cache en écriture.
    </para>
   </listitem>

   <listitem>
    <para>
     Sur <productname>macOS</productname>, le cache en écriture peut
     être évité en configurant <varname>wal_sync_method</varname> à
     <literal>fsync_writethrough</literal>.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Les disques SATA récents (ceux compatibles <acronym>ATAPI-6</acronym> ou
   supérieurs) proposent une commande pour vider le cache sur le disque
   (<command>FLUSH CACHE EXT</command>) alors que les disques SCSI proposent
   depuis longtemps une commande similaire, <command>SYNCHRONIZE CACHE</command>.
   Ces commandes ne sont pas directement accessibles à
   <productname>PostgreSQL</productname>, mais certains systèmes de fichiers
   (comme <acronym>ZFS</acronym>, <acronym>ext4</acronym>) peuvent les utiliser pour vider
   les données sur disque pour les disques au cache en écriture activé.
   Malheureusement,
   ces systèmes de fichiers se comportent de façon non optimale avec des
   contrôleurs disque équipés de batterie (<acronym>BBU</acronym>, acronyme de
   <foreignphrase>Battery-Backed Unit</foreignphrase>). Dans ce type de
   configuration, la commande de synchronisation force l'écriture de toutes
   les données comprises dans le cache sur les disques, éliminant ainsi tout
   l'intérêt d'un cache protégé par une batterie. Vous pouvez lancer l'outil
   <xref linkend="pgtestfsync"/>, disponible dans le code source de
   PostgreSQL, pour vérifier si vous êtes affecté. Si vous l'êtes, les
   améliorations de performance du cache BBU peuvent être de nouveaux obtenues
   en désactivant les barrières d'écriture dans la configuration du système de
   fichiers ou en reconfigurant le contrôleur de disque, si cela est possible.
   Si les barrières d'écriture sont désactivées, assurez-vous que la batterie
   reste active. Une batterie défectueuse peut être une cause de perte de
   données. Il reste à espérer que les concepteurs de systèmes de fichiers et
   de contrôleurs disque finissent par corriger ce comportement gênant.
  </para>

  <para>
   Quand le système d'exploitation envoie une demande d'écriture au système de
   stockage,
   il ne peut pas faire grand chose pour s'assurer que les données sont
   arrivées dans un espace de stockage réellement non volatile.
   C'est plutôt la responsabilité de l'administrateur de
   s'assurer que tous les composants de
   stockage garantissent l'intégrité des données et des métadonnées du système de
   fichier. Évitez les contrôleurs disques
   ne disposant pas de caches protégés par batterie. Au niveau du disque,
   désactivez le cache <quote>write-back</quote> si le disque ne peut garantir
   que les données seront écrites avant un arrêt. Si vous utilisez des disques
   SSD, sachez que beaucoup n'honorent pas les commandes de vidage de cache
   par défaut. Vous pouvez tester la fiabilité du comportement du système
   disque en utilisant <ulink
   url="https://brad.livejournal.com/2116715.html"><filename>diskchecker.pl</filename></ulink>.
  </para>

  <para>
   Un autre risque de perte de données est dû aux opérations
   d'écriture sur les plateaux du disque. Les plateaux sont divisés en
   secteurs de 512 octets généralement. Chaque opération de lecture ou écriture
   physique traite un secteur entier. Quand une demande d'écriture arrive au
   lecteur, sa taille peut être un multiple de 512 octets
   (<productname>PostgreSQL</productname> écrit généralement 8192 octets, soit
   16 secteurs, à la fois) et le processus d'écriture peut échouer à cause
   d'une coupure de courant à tout moment, ce qui implique que certains octets
   peuvent être écrits et les autres perdus. Pour se prévenir contre ce
   type d'échec, <productname>PostgreSQL</productname> écrit périodiquement
   des pages complètes dans le stockage permanent des journaux de
   transactions <emphasis>avant</emphasis> de modifier la page réelle sur
   disque. Ainsi, lors d'une récupération après un arrêt brutal,
   <productname>PostgreSQL</productname> peut restaurer les pages écrites
   partiellement à partir des journaux de transactions. Si vous avez un système
   de fichiers qui vous protège contre les écritures de pages incomplètes (par
   exemple ZFS), vous pouvez désactiver la création des images de page en
   utilisant le paramètre <xref linkend="guc-full-page-writes"/>. Les
   contrôleurs disques disposant d'une batterie (<acronym>BBU</acronym> pour
   <foreignphrase>Battery-Backed Unit</foreignphrase>) n'empêchent pas les
   écritures de pages partielles sauf s'ils garantissent que les données
   sont écrites par pages complètes de 8&nbsp;ko.
  </para>

  <para>
   <productname>PostgreSQL</productname> protège aussi de certaines corruptions
   de données au niveau du support de stockage qui peuvent se produire suite à des
   erreurs au niveau matériel ou des problèmes d'usure au fil du temps,
   comme la lecture ou l'écriture de données invalides.
   <itemizedlist>
    <listitem>
     <para>
      Dans un journal de transaction, chaque enregistrement est protégé
      par une somme de contrôle CRC-32 (32 bits) qui nous dit si le
      contenu de l'enregistrement est correct. La valeur CRC est définie à
      l'écriture de chaque enregistrement dans le journal et
      vérifiée durant une récupération après arrêt brutal, la récupération d'une
      archive et la réplication.
     </para>
    </listitem>
    <listitem>
     <para>
      Par défaut, les pages de données ne disposent pas de sommes de contrôle, mais
      les images des pages complètes stockées dans les enregistrements des
      journaux de transactions seront protégées. Voir <link
      linkend="app-initdb-data-checksums"><application>initdb</application></link>
      pour les détails sur l'activation des sommes de contrôle sur les pages de
      données.
     </para>
    </listitem>
    <listitem>
     <para>
      Les structures de données internes comme
      <filename>pg_xact</filename>, <filename>pg_subtrans</filename>,
      <filename>pg_multixact</filename>, <filename>pg_serial</filename>,
      <filename>pg_notify</filename>, <filename>pg_stat</filename>,
      <filename>pg_snapshots</filename>
      n'ont pas de sommes de contrôle, et leurs pages de données ne sont pas
      protégées par les écritures de pages complètes.
      Cependant, lorsque de telles structures
      de données sont persistantes, les enregistrements des journaux de
      transactions sont écrits de manière à ce que les modifications récentes
      puissent être rapidement reconstruites lors d'une restauration après
      incident, et ces enregistrements sont protégés comme décrit plus haut.
     </para>
    </listitem>
    <listitem>
     <para>
      Les fichiers d'état de <filename>pg_twophase</filename> sont
      protégés chacun par une somme de contrôle CRC-32.
     </para>
    </listitem>
    <listitem>
     <para>
      Les fichiers de données temporaires utilisés lors de grosses requêtes SQL
      pour le tri, la matérialisation ou des résultats intermédiaires ne font
      pas actuellement l'objet d'une somme de contrôle, et la
      modification de ces fichiers n'est pas non plus consignée dans les
      enregistrements des journaux de transactions.
     </para>
    </listitem>
   </itemizedlist>
  </para>
  <para>
   <productname>PostgreSQL</productname> ne protége pas contre les erreurs
   mémoires et il est supposé que vous travaillez avec de la
   RAM avec correction d'erreur (ECC) aux standards de l'industrie, ou une meilleure protection.
  </para>
 </sect1>

 <sect1 id="wal-intro">
  <title>Write-Ahead Logging (<acronym>WAL</acronym>)</title>

  <indexterm zone="wal">
   <primary>WAL</primary>
  </indexterm>

  <indexterm>
   <primary>log transaction</primary>
   <see>WAL</see>
  </indexterm>

  <para>
   <firstterm>Write-Ahead Logging</firstterm> (ou <acronym>WAL</acronym>,
   pour «&nbsp;écriture de journaux en amont&nbsp;»)
   est une méthode courante pour garantir l'intégrité des
   données. Une description détaillée peut être trouvée dans la plupart
   des livres sur le traitement transactionnel, sinon tous.
   En résumé, le concept central
   du <acronym>WAL</acronym> est de ne modifier les fichiers de
   données (donc les tables et les index) qu'après que les changements
   ont été journalisés,
   c'est-à-dire qu'après que les enregistrements décrivant ces changements
   ont été écrits sur du stockage permanent.
   Si nous suivons cette procédure, nous n'avons
   pas besoin d'écrire les pages de données vers le disque à chaque
   validation de transaction car nous savons que, dans l'éventualité
   d'une défaillance, nous serons capables de récupérer la base de
   données en utilisant le journal&nbsp;: chaque changement qui n'a pas été
   appliqué aux pages de données peut être ré-exécuté depuis les
   enregistrements du journal. (Ceci est une récupération
   <foreignphrase>roll-forward</foreignphrase>,
   aussi connue sous le nom de REDO).
  </para>

  <tip>
   <para>
    Comme les journaux de transaction permettent de restaurer le contenu des
    fichiers de base de données après un arrêt brutal, un système de
    fichiers journalisé n'est pas nécessaire pour stocker avec fiabilité
    les fichiers de données ou les journaux de transactions.
    En fait, le
    surcroît de travail lié à la journalisation peut réduire les performances,
    tout spécialement si la journalisation cause l'écriture des
    <emphasis>données</emphasis> sur disque.
    Heureusement, l'écriture des données lors de la journalisation
    peut souvent être désactivée avec une option de montage du système de
    fichiers, par exemple <literal>data=writeback</literal> sur un système de
    fichiers Linux ext3. Par contre, les systèmes de fichiers journalisés
    accélèrent le redémarrage après un arrêt brutal.
   </para>
  </tip>

  <para>
   Utiliser les journaux de transaction permet de réduire de façon
   significative le nombre d'écritures sur le disque&nbsp;: seul le journal
   a besoin d'être écrit sur le disque pour garantir qu'une
   transaction a été validée, il n'y a pas besoin d'écrire dans chaque fichier de
   données modifié par la transaction. Ce journal est écrit séquentiellement,
   ainsi le coût de vidage sur disque du journal est largement moindre
   que le coût d'écriture des pages de données.
   Ceci est tout spécialement vrai pour
   les serveurs gérant beaucoup de petites transactions touchant différentes
   parties du stockage de données. De plus, quand le serveur traite beaucoup de
   petites transactions en parallèle, un <function>fsync</function> du
   journal des transactions peut suffire pour enregistrer plusieurs
   transactions.
  </para>

  <para>
   Les journaux de transaction rendent possible le support de sauvegarde
   en ligne et de récupération à un moment donné dans le temps,
   comme décrit dans la <xref linkend="continuous-archiving"/>.
   En archivant les journaux de transaction,
   nous permettons un retour à tout instant couvert par les données
   disponibles dans les journaux de transaction&nbsp;: nous installons
   simplement une ancienne sauvegarde physique de la base de données et nous
   rejouons les journaux de transaction jusqu'au moment désiré. Qui plus est,
   la sauvegarde physique n'a pas besoin d'être une image instantanée de
   l'état de la base de données &mdash; si elle a été faite pendant une
   longue période de temps, alors rejouer les journaux de transaction pour
   cette période corrigera toute incohérence interne.
  </para>
 </sect1>

 <sect1 id="wal-async-commit">
  <title>Validation asynchrone (Asynchronous Commit)</title>

  <indexterm>
   <primary>validation synchrone</primary>
  </indexterm>

  <indexterm>
   <primary>validation asynchrone</primary>
  </indexterm>

  <para>
   La <firstterm>validation asynchrone</firstterm> est une option qui
   permet aux transactions de se terminer plus rapidement. Le risque encouru
   est de perdre les transactions les plus récentes dans le cas où le serveur
   s'arrête brutalement. Dans beaucoup d'applications, le compromis est
   acceptable.
  </para>

  <para>
   Comme le décrit la section précédente, la validation d'une
   transaction est habituellement <firstterm>synchrone</firstterm>&nbsp;:
   le serveur attend que les enregistrements des journaux de transaction
   soient bien sauvegardés sur le stockage permanent
   avant d'informer le client du succès de l'opération.
   Le client a donc la garantie qu'une
   transaction validée est stockée de façon sûre, donc même
   en cas d'arrêt brutal immédiatement après. Néanmoins, pour les petites
   transactions, ce délai est une partie importante de la durée totale
   d'exécution de la transaction. Sélectionner le mode de validation
   asynchrone signifie que le serveur annonce le succès de l'opération dès
   que la transaction est terminée logiquement, donc avant que les
   enregistrements du journal de transaction que cette transaction a
   générés ne soient réellement stockés sur disque. Ceci peut apporter
   une accélération importante pour les petites transactions.
  </para>

  <para>
   La validation asynchrone introduit le risque des pertes de données.
   Il existe un petit délai entre le moment où le rapport de la fin d'une
   transaction est envoyé au client et celui où la transaction est réellement
   enregistrée (c'est-à-dire le moment où le résultat de cette transaction
   ne pourra pas être perdu même en cas d'arrêt brutal du serveur). Du coup,
   la validation asynchrone ne devrait pas être utilisée si le client
   se base sur le fait que la transaction est enregistrée de façon sûre.
   Par exemple, une banque ne devra pas utiliser la validation asynchrone
   pour une transaction enregistrant la sortie d'argent
   d'un distributeur bancaire.
   Dans de nombreux autres scénarios, comme la trace
   d'événements, il n'y a pas besoin de garantie forte de ce type.
  </para>

  <para>
   Le risque pris avec l'utilisation de la validation asynchrone concerne
   la perte de données, pas la corruption de données. Si le serveur s'arrête
   brutalement, il redémarrera en rejouant les journaux de transaction
   jusqu'au dernier enregistrement vidé sur disque.
   La base sera donc restaurée dans un état cohérent mais toutes les transactions
   qui n'auront pas été enregistrées sur disque n'y apparaîtront pas.
   L'effet immédiat est donc la perte des toutes dernières transactions.
   Comme les
   transactions sont rejouées dans l'ordre de validation, aucune incohérence
   ne sera introduite &mdash; par exemple, si la transaction B fait des
   modifications sur les effets d'une précédente transaction A, il n'est pas
   possible que les effets de A soient perdus alors que les effets de B
   sont préservés.
  </para>

  <para>
   L'utilisateur peut sélectionner le mode de validation de chaque
   transaction&nbsp;; il est donc possible d'avoir en même temps des transactions
   validées en synchrone et en asynchrone.
   Cela offre une grande flexibilité dans le choix entre la performance
   et la certitude de la durabilité des transactions.
   Le mode de validation est contrôlé par le paramètre utilisateur
   <xref linkend="guc-synchronous-commit"/>, qui peut être modifié comme
   tout autre paramètre utilisateur. Le mode utilisé pour une
   transaction dépend de la valeur de <varname>synchronous_commit</varname>
   quand démarre la validation de la transaction.
  </para>

  <para>
   Certaines commandes, par exemple <command>DROP TABLE</command>, sont
   forcées en mode synchrone quelle que soit la valeur du paramètre
   <varname>synchronous_commit</varname>. Ceci a pour but de s'assurer de
   la cohérence entre le système de fichiers du serveur et l'état logique
   de la base de données. Les commandes gérant la validation en deux phases
   (<foreignphrase>two-phase commit</foreignphrase>),
   comme <command>PREPARE TRANSACTION</command>, sont aussi toujours
   synchrones.
  </para>

  <para>
   Si la base de données s'arrête brutalement dans la fenêtre de
   vulnérabilité entre une validation asynchrone
   et l'écriture des enregistrements dans le journal
   des transactions, les modifications réalisées lors de cette transaction
   <emphasis>seront</emphasis> perdues. Cette fenêtre est limitée
   car un processus en tâche de fond (le <quote>wal writer</quote>)
   écrit sur le disque
   les enregistrements non écrits des journaux de transaction
   toutes les <xref linkend="guc-wal-writer-delay"/> millisecondes.
   La fenêtre de vulnérabilité s'étend en réalité à trois fois
   <varname>wal_writer_delay</varname> car le processus d'écriture des
   journaux de transaction est conçu pour favoriser l'écriture de pages
   complètes lors des périodes de grosses activités.
  </para>

  <caution>
   <para>
    Un arrêt en mode immédiat est équivalent à un arrêt brutal et causera
    du coup la perte des validations asynchrones.
   </para>
  </caution>

  <para>
   La validation asynchrone fournit un comportement différent de la simple
   désactivation de <xref linkend="guc-fsync"/>. <varname>fsync</varname> est
   un paramètre pour le serveur entier qui modifie le comportement de toutes
   les transactions.
   Il désactive dans <productname>PostgreSQL</productname>
   toute la logique qui tente de synchroniser les
   écritures dans les différentes parties de la base,
   et donc un arrêt brutal (plus précisément
   un arrêt brutal du matériel ou du système d'exploitation, pas un plantage
   de <productname>PostgreSQL</productname> lui-même) pourrait résulter
   en une corruption arbitraire de l'état de la base de données. Dans de
   nombreux scénarios, la validation asynchrone fournit la majorité des
   améliorations de performances obtenues par la désactivation de
   <varname>fsync</varname>, mais sans le risque de la corruption de données.
  </para>

  <para>
   <xref linkend="guc-commit-delay"/> semble aussi très similaire à la
   validation asynchrone mais il s'agit en fait d'une méthode de validation
   synchrone (en fait, <varname>commit_delay</varname> est ignoré lors d'une
   validation asynchrone). <varname>commit_delay</varname>
   provoque un délai avant la mise à jour sur disque
   du <acronym>WAL</acronym> d'une transaction,
   dans l'espoir que l'opération profite à d'autres transactions
   commitées à peu près au même moment. Ce paramètre peut
   être vu comme le moyen d'augmenter la fenêtre de temps durant laquelle
   chaque transaction peut participer à un même vidage sur disque, pour amortir
   son coût sur plusieurs transactions.
  </para>

 </sect1>

 <sect1 id="wal-configuration">
  <title>Configuration des journaux de transaction</title>

  <para>
   Il y a plusieurs paramètres de configuration associés aux
   journaux de transaction qui affectent les performances de la base.
   Cette section explique leur utilisation. Consultez
   <xref linkend="runtime-config"/> pour des détails sur la
   mise en place de ces paramètres.
  </para>

  <para>
   Dans la séquence des transactions, les
   <foreignphrase>checkpoints</foreignphrase>
   (ou <firstterm>points de contrôle</firstterm>)
   sont des points qui garantissent que les fichiers de données
   des table et des index ont été mis à
   jour avec toutes les informations enregistrées dans le journal avant le
   checkpoint. Au moment du checkpoint, toutes les
   pages de données modifiées (<foreignphrase>dirty</foreignphrase>)
   sont écrites sur le disque et une
   entrée spéciale, pour le checkpoint, est écrite dans le
   journal. (Les modifications étaient déjà envoyées dans les journaux de
   transactions.)
   En cas de défaillance, la procédure de récupération recherche le
   dernier enregistrement d'un checkpoint
   (enregistrement connus sous le nom de <quote>redo log</quote>)
   pour déterminer le point des journaux
   à partir duquel il devra lancer l'opération de REDO.
   Toute modification effectuée sur les fichiers de données avant ce point
   a la garantie d'avoir été enregistrée sur disque. Du coup, après un
   checkpoint, tous les segments représentant des journaux de
   transaction précédant celui
   contenant le <quote>redo record</quote> ne sont plus nécessaires et peuvent
   être soit recyclés, soit supprimés (quand l'archivage des journaux de
   transaction est activé, ces derniers doivent être archivés avant d'être
   recyclés ou supprimés).
  </para>

  <para>
   Un checkpoint doit écrire toutes les pages de données modifiées sur disque, ce
   qui peut causer une charge disque importante.
   Pour cette raison, l'activité d'un checkpoint
   est diluée de façon à ce que les entrées/sorties disque
   commencent au début du checkpoint et se terminent avant le démarrage du
   checkpoint suivant&nbsp;; ceci minimise la dégradation des performances
   lors des checkpoints.
  </para>

  <para>
   Le processus checkpointer lance automatiquement un checkpoint
   de temps en temps. Il démarre toutes les
   <xref linkend="guc-checkpoint-timeout"/> secondes
   ou si <xref linkend="guc-max-wal-size"/> risque d'être dépassé,
   suivant ce qui arrive en premier.
   La configuration par défaut de ces deux paramètres est,
   respectivement, 5&nbsp;minutes et 1&nbsp;Go.
   Si aucun enregistrement WAL n'a été écrit depuis le dernier checkpoint,
   il n'y en aura pas de nouveaux, même si la durée
   <varname>checkpoint_timeout</varname> est dépassée.
   (Si l'archivage des WAL est en place et que vous voulez définir une limite
   basse correspondant à la fréquence d'archivage des fichiers de
   manière à limiter la perte potentielle de données, vous devez ajuster le
   paramètre <xref linkend="guc-archive-timeout"/> plutôt que les paramètres
   affectant les checkpoints.)
   Il est aussi possible de forcer un checkpoint en utilisant la commande SQL
   <command>CHECKPOINT</command>.
  </para>

  <para>
   La réduction de <varname>checkpoint_timeout</varname> et/ou
   <varname>max_wal_size</varname> provoque
   des checkpoints plus fréquents. Cela permet une récupération
   plus rapide après arrêt brutal puisqu'il y aura moins d'écritures à
   refaire. Cependant, il faut équilibrer cela avec
   le coût d'écritures plus fréquentes des pages modifiées.
   Si <xref linkend="guc-full-page-writes"/> est configuré (ce qui est le
   défaut), il reste un autre facteur à considérer. Pour s'assurer de la
   cohérence des pages de données, la première modification d'une page de
   données après chaque checkpoint résulte dans la journalisation du
   contenu de la page en entier. Dans ce cas, un intervalle de checkpoints
   plus petit augmentera le volume d'écriture des journaux de transaction,
   annulant en partie l'intérêt d'utiliser cet intervalle plus petit et
   générant de toute façon plus d'entrées/sorties au niveau disque.
  </para>

  <para>
   Les checkpoints sont assez coûteux, tout d'abord parce qu'ils
   écrivent tous les tampons modifiés jusqu'à ce moment,
   et ensuite parce qu'ils génèrent un
   trafic supplémentaire dans les journaux de transaction, comme indiqué
   ci-dessus. Du coup, il est conseillé
   de configurer les paramètres des checkpoints assez haut pour qu'ils
   ne surviennent pas trop fréquemment. Pour une vérification
   rapide de l'adéquation de vos paramètres, vous pouvez configurer le
   paramètre <xref linkend="guc-checkpoint-warning"/>.
   Si les checkpoints se déclenchent à moins de <varname>checkpoint_warning</varname> secondes d'intervalle, un message
   est affiché dans les journaux applicatifs du serveur recommandant d'accroître
   <varname>max_wal_size</varname>. L'apparition occasionnelle d'un tel
   message ne doit pas vous alarmer mais, s'il apparaît souvent, alors les
   paramètres de contrôle devraient être augmentés. Les opérations en masse,
   comme les transferts importants de données via <command>COPY</command>,
   peuvent faire apparaître nombre de ces messages d'avertissement si
   vous n'avez pas configuré <varname>max_wal_size</varname> avec une valeur
   suffisamment haute.
  </para>

  <para>
   Pour éviter de saturer les entrées/sorties avec de très nombreuses
   écritures de pages modifiées, leur écriture pendant le checkpoint
   est étalée sur une période de temps. Elle est
   contrôlée par <xref linkend="guc-checkpoint-completion-target"/>,
   donné comme une fraction de l'intervalle des checkpoints.
   Le taux d'entrées/sorties est ajusté pour que le checkpoint
   se termine quand la fraction indiquée de
   <varname>checkpoint_timeout</varname> secondes s'est écoulée,
   ou avant que <varname>max_wal_size</varname> soit dépassé,
   selon ce qui arrivera en premier. Avec la valeur par
   défaut de 0,5, on peut s'attendre à ce que
   <productname>PostgreSQL</productname> termine chaque checkpoint
   dans la moitié du temps avant le démarrage du suivant.
   Sur un système
   très proche de son débit maximal d'entrées/sorties en fonctionnement
   normal, vous pouvez augmenter
   <varname>checkpoint_completion_target</varname> pour réduire la charge
   due aux checkpoints. L'inconvénient
   de prolonger les checkpoints est d'impacter le temps de
   récupération, car il faudra conserver plus de journaux de transaction
   si une récupération est nécessaire. Bien que
   <varname>checkpoint_completion_target</varname> puisse monter à 1.0, il
   le mieux est de la configurer à une valeur plus basse (au
   plus 0,9), car les checkpoints incluent d'autres
   activités en dehors de l'écriture des pages modifiées. Une valeur de 1,0
   peut résulter en checkpoints qui ne se terminent
   pas à temps, ce qui entraînerait des baisses de performance à
   cause de variations inattendues dans le nombre de journaux nécessaires.
  </para>

  <para>
   Sur les plateformes Linux et POSIX, <xref linkend="guc-checkpoint-flush-after"/>
   permet de forcer le système d'exploitation à vider sur disque
   les pages écrites par un checkpoint après qu'un nombre configurable
   d'octets soit écrit. Sinon ces pages pourraient rester dans
   le cache disque du système d'exploitation, provoquant
   un blocage quand <literal>fsync</literal> est exécuté à la fin d'un
   checkpoint. Cette configuration aide souvent à réduire la latence des
   transactions mais il peut aussi avoir un effet inverse sur les performances,
   particulièrement pour des charges supérieures à
   <xref linkend="guc-shared-buffers"/> mais
   plus petites que le cache disque du système d'exploitation.
  </para>

  <para>
   Le nombre de fichiers de segments WAL dans le répertoire
   <filename>pg_wal</filename> dépend des paramètres
   <varname>min_wal_size</varname>, <varname>max_wal_size</varname> et de la
   quantité de WAL générée lors des cycles de checkpoints précédents. Quand les
   anciens fichiers de segments ne sont plus nécessaires, ils sont supprimés
   ou recyclés (c'est-à-dire renommés pour devenir les segments suivants selon
   les numéros de la séquence). Si, à cause d'un bref pic du débit
   des WAL, <varname>max_wal_size</varname> est dépassé, les fichiers inutiles
   seront supprimés jusqu'à ce que le système revienne sous cette
   limite. En-dessous de cette limite, le système recycle suffisamment de
   fichiers WAL pour couvrir le besoin estimé jusqu'au checkpoint suivant, et
   supprime le reste. L'estimation est basée sur une moyenne glissante du
   nombre de fichiers WAL utilisés dans les cycles de checkpoint précédents.
   Elle est augmentée immédiatement si l'utilisation en cours
   dépasse l'estimation, pour correspondre aux pics d'utilisation
   plutôt qu'à l'utilisation moyenne, jusqu'à un certain point.
   <varname>min_wal_size</varname> définit un nombre minimum de fichiers
   WAL recyclés pour une utilisation future, même si le système est inutilisé
   et que l'estimation suggère que peu
   de WAL sont nécessaires.
  </para>

  <para>
   Indépendamment de <varname>max_wal_size</varname>, les
   <xref linkend="guc-wal-keep-size"/> méga-octets les plus récents des fichiers
   WAL et un fichier WAL supplémentaire sont conservés en
   permanence. De plus, si l'archivage est activé, les anciens segments ne
   sont ni supprimés ni recyclés jusqu'à la réussite de leur archivage. Si
   l'archivage des WAL n'est pas assez rapide pour tenir le rythme de
   la génération des WAL, ou si la commande indiquée par
   <varname>archive_command</varname> échoue de manière répétée, les anciens
   fichiers WAL s'accumuleront dans le répertoire <filename>pg_wal</filename>
   jusqu'à ce que ce problème soit résolu. Un serveur standby lent ou en échec,
   et qui utilise un slot de réplication, aura le même effet (voir <xref
   linkend="streaming-replication-slots"/>).
  </para>

  <para>
   En mode de restauration d'archive et en mode standby, le serveur
   réalise périodiquement des <firstterm>restartpoints</firstterm>
   <indexterm><primary>restartpoint</primary></indexterm> (points de
   redémarrage), qui sont similaire aux checkpoints lors du fonctionnement
   normal&nbsp;: le serveur force l'écriture de son état sur disque, met à
   jour le fichier <filename>pg_control</filename> pour indiquer que les
   données déjà traitées des journaux de transactions n'ont plus besoin d'être
   parcourues de nouveau, puis recycle les anciens journaux de transactions
   trouvés dans le répertoire <filename>pg_wal</filename>. Les restartpoints
   ne peuvent être réalisés plus fréquemment que les checkpoints du maître car
   les restartpoints peuvent seulement être réalisés aux enregistrements de
   checkpoint.
   Un restartpoint est déclenché lorsqu'un enregistement de checkpoint est
   atteint si un minimum de <varname>checkpoint_timeout</varname> secondes se
   sont écoulées depuis le dernier restartpoint, ou si la taille totale des
   journaux de transactions va dépasser <varname>max_wal_size</varname>.
   Néanmoins, à cause de ces limitations sur quand un restartpoint peut
   être effectué, <varname>max_wal_size</varname> est souvent dépassé lors
   d'une restauration jusqu'à au plus un cycle de checkpoint de journaux
   (<varname>max_wal_size</varname> n'est de toute façon jamais une limite en
   dur, vous devriez donc toujours laisser plein d'espace pour éviter de
   manquer d'espace disque).
  </para>

  <para>
   Il existe deux fonctions <acronym>WAL</acronym> internes couramment
   utilisées&nbsp;:
   <function>XLogInsertRecord</function> et <function>XLogFlush</function>.
   <function>XLogInsertRecord</function> est utilisée pour placer une
   nouvelle entrée à l'intérieur des tampons <acronym>WAL</acronym> en mémoire
   partagée. S'il n'y a plus
   d'espace pour la nouvelle entrée, <function>XLogInsertRecord</function>
   devra écrire (autrement dit, déplacer dans le cache du noyau) quelques
   tampons <acronym>WAL</acronym> remplis. Ceci n'est pas souhaitable parce que
   <function>XLogInsertRecord</function> est utilisée à chaque
   modification bas niveau de la base (par exemple, lors de l'insertion d'une
   ligne) quand un verrou exclusif est posé sur les pages de données
   affectées, et l'opération doit donc être aussi rapide que possible.
   Pire encore, écrire des tampons <acronym>WAL</acronym>
   peut aussi forcer la création d'un nouveau journal, ce qui
   prend encore plus de temps. Normalement, les tampons
   <acronym>WAL</acronym> doivent être écrits et vidés par un appel
   à <function>XLogFlush</function> fait, la plupart du
   temps, au moment de la validation d'une transaction pour assurer
   que les entrées de la transaction sont écrites vers un stockage
   permanent. Sur les systèmes avec une importante écriture de journaux,
   les requêtes de <function>XLogFlush</function> peuvent ne pas
   arriver assez souvent pour empêcher <function>XLogInsert</function> d'avoir
   à écrire lui-même. Sur de tels systèmes, on devrait augmenter le
   nombre de tampons
   <acronym>WAL</acronym> en modifiant le paramètre <xref
   linkend="guc-wal-buffers"/>. Quand <xref linkend="guc-full-page-writes"/> est configuré
   et que le système est très occupé, configurer <varname>wal_buffers</varname> avec une valeur
   plus importante aide à lisser les temps de réponse
   dans la période suivant immédiatement chaque checkpoint.
  </para>

  <para>
   Le paramètre <xref linkend="guc-commit-delay"/> définit combien de
   micro-secondes un processus maître d'un groupe de commit
   va s'endormir après avoir obtenu un verrou avec <function>XLogFlush</function>,
   pendant que les autres processus du groupe vont s'ajouter à la queue
   derrière le maître. Ce délai permet aux autres processus serveur
   d'ajouter leurs enregistrements de commit aux buffers WAL, pour
   qu'ils soient tous écrits par un éventuel vidage sur disque du
   maître. Il n'y aura pas d'endormissement si <xref linkend="guc-fsync"/> n'est
   pas activé, ou si moins de <xref linkend="guc-commit-siblings"/> autres
   sessions sont actuellement dans une transaction active&nbsp;; ce
   mécanisme évite l'endormissement quand il est improbable que d'autres sessions
   valident bientôt leur transactions. Il est à noter que, sur
   certaines plateformes, la résolution d'une requête d'endormissement est de dix
   millisecondes, ce qui implique que toute valeur comprise entre 1 et 10000 pour
   le paramètre <varname>commit_delay</varname> aura le même effet. Notez aussi
   que, sur certaines plateformes, les opérations d'endormissement
   peuvent être légèrement plus longues que ce qui a été demandé par le paramètre.
  </para>

  <para>
   Comme l'objet de <varname>commit_delay</varname> est de permettre d'amortir
   le coût de chaque opération de vidage sur disque sur plusieurs transactions concurrentes
   (potentiellement au prix de la latence des transactions), il est nécessaire de
   quantifier ce coût pour choisir intelligemment la valeur de ce paramètre. Plus le
   coût est élevé, plus <varname>commit_delay</varname> sera efficace
   au sein d'un débit de transactions croissant,
   jusqu'à un certain point. Le programme <xref linkend="pgtestfsync"/> peut être
   utilisé pour mesurer le temps moyen en microsecondes que prend une seule
   opération de vidage de WAL.
   La moitié du temps moyen rapporté par ce
   programme pour une mise à jour d'une simple opération d'écriture de 8&nbsp;ko
   est la valeur la plus souvent recommandée comme point de départ de
   l'optimisation d'une
   charge particulière. Bien que l'ajustement de la valeur de
   <varname>commit_delay</varname> soit particulièrement utile lorsque les
   journaux WAL sont stockés sur des disques à latence élevée, le gain peut
   aussi être significatif sur les supports de stockage avec des temps de
   synchronisation très rapides, comme les SSD ou les grappes RAID
   avec des caches en écriture dotés de batterie&nbsp;; mais dans tous les cas,
   cela doit être testé avec une charge représentative de la réalité. Des
   valeurs plus élevées de <varname>commit_siblings</varname> peuvent être
   utilisées dans ce cas, alors que de petites valeurs de
   <varname>commit_siblings</varname> sont souvent utiles sur des supports de
   grande latence. À noter qu'il est possible qu'une valeur trop élevée de
   <varname>commit_delay</varname> augmente la latence des transactions
   à un tel point que le débit des transactions en souffre.
  </para>

  <para>
   Lorsque <varname>commit_delay</varname> est défini à zéro (il s'agit de la
   valeur par défaut), il est toujours possible qu'un regroupement de commits se
   produise, mais chaque groupe ne consistera qu'en sessions atteignant
   le moment de l'enregistrement de commit pendant le laps de temps
   où la précédente opération de vidage (s'il y en a) opère.
   Avec un grand nombre de clients, un <quote>effet tunnel</quote>
   (<foreignphrase>gangway effect</foreignphrase>) a tendance à se produire,
   et ainsi les effets du regroupement de commits deviennent significatifs même
   lorsque <varname>commit_delay</varname> est à zéro, et dans ce cas
   <varname>commit_delay</varname> devient inutile. Définir
   <varname>commit_delay</varname> n'est utile que quand
   (1)&nbsp;il existe des transactions concurrentes, et
   (2)&nbsp;le débit est limité dans une certaine mesure
   par la vitesse de commit&nbsp;; mais, dans le cas d'un temps de latence
   du disque élevé, ce paramètre peut augmenter efficacement le flux de transaction
   avec seulement deux clients
   (c'est-à-dire un unique client qui valide, et une transaction sœur).
  </para>

  <para>
   Le paramètre <xref linkend="guc-wal-sync-method"/> détermine comment
   <productname>PostgreSQL</productname> demande au noyau de forcer les mises
   à jour des journaux de transaction sur le disque. Toutes les différentes
   options devraient être identiques en terme de fiabilité, à l'exception de
   <literal>fsync_writethrough</literal>, qui peut parfois forcer une écriture
   du cache disque même quand d'autres options ne le font pas. Néanmoins,
   connaître l'option la plus rapide est assez dépendant de la plateforme.
   Vous pouvez tester les vitesses des différentes
   options en utilisant le programme <xref
   linkend="pgtestfsync"/>. Notez que ce paramètre est ignoré si
   <varname>fsync</varname> a été désactivé.
  </para>

  <para>
   Activer le paramètre de configuration <xref linkend="guc-wal-debug"/> (à
   supposer que <productname>PostgreSQL</productname> ait été compilé avec le
   support de ce paramètre) permet d'enregistrer chaque appel
   <acronym>WAL</acronym> à <function>XLogInsertRecord</function> et
   <function>XLogFlush</function> dans les journaux applicatifs du serveur.
   Cette option
   pourrait être remplacée par un mécanisme plus général dans le futur.
  </para>
 </sect1>

 <sect1 id="wal-internals">
  <title>Vue interne des journaux de transaction</title>

  <indexterm zone="wal-internals">
   <primary>LSN</primary>
  </indexterm>

  <para>
   Le mécanisme <acronym>WAL</acronym> est automatiquement activé&nbsp;;
   aucune action n'est requise de la part de l'administrateur, sauf
   s'assurer que l'espace disque requis par les journaux de transaction
   est présent et que tous les réglages nécessaires sont faits (voir
   la <xref linkend="wal-configuration"/>).
  </para>

  <para>
   Les enregistrements <acronym>WAL</acronym> sont ajoutés aux journaux
   <acronym>WAL</acronym>, enregistrement après enregistrement. La position
   d'insertion est donnée par le
   <foreignphrase>Log Sequence Number</foreignphrase> (<acronym>LSN</acronym>,
   pour numéro de séquence de journal)
   qui est un décalage d'octets (<foreignphrase>offset</foreignphrase>)
   au sein des journaux de transactions,
   qui s'incrémente de manière monotone à chaque enregistrement. Les valeurs du
   <acronym>LSN</acronym> sont renvoyées en tant que type de données <link
   linkend="datatype-pg-lsn"><type>pg_lsn</type></link>. Les valeurs peuvent
   être comparées pour calculer le volume de données <acronym>WAL</acronym>
   les séparant, permettant ainsi de mesurer l'avancement de la réplication et
   de la restauration.
  </para>

  <para>
   Les journaux de transaction sont un ensemble de fichiers stockés dans le répertoire
   <filename>pg_wal</filename> sous celui des données,
   chacun d'une taille de 16&nbsp;Mo normalement (cette
   taille pouvant être modifiée en modifiant l'option
   <option>--wal-segsize</option> d'initdb). Chaque fichier est divisé en
   pages de généralement 8&nbsp;ko (cette taille pouvant être modifiée
   avec l'option <option>--with-wal-blocksize</option> de
   configure). Les en-têtes d'une entrée de journal sont décrites dans
   <filename>access/xlogrecord.h</filename>&nbsp;; le contenu d'une entrée
   dépend du type de l'événement qui est enregistré. Les fichiers sont nommés
   suivant des nombres continûment incrémentés, commençant par
   <filename>000000010000000000000001</filename>. Les nombres ne bouclent
   pas, mais cela prendra beaucoup, beaucoup de temps pour épuiser le stock de nombres
   disponibles.
  </para>

  <para>
   Il est avantageux que les journaux soient situés sur un autre disque que
   celui des fichiers principaux de la base de données.  Cela peut
   se faire en déplaçant le répertoire
   <filename>pg_wal</filename> vers un autre emplacement
   (serveur arrêté, bien sûr) et en créant dans le répertoire principal de
   données un lien symbolique de l'emplacement original vers le nouveau.
  </para>

  <para>
   Le but de <acronym>WAL</acronym> est de s'assurer que le journal est écrit
   avant de modifier les enregistrements de la base,
   mais cela peut être mis en échec par
   des disques<indexterm><primary>disques durs</primary></indexterm> qui
   rapportent une écriture
   réussie au noyau quand, en fait, ils ont seulement mis en cache
   les données et ne les ont pas encore stockées sur le disque. Une
   coupure de courant dans ce genre de situation peut mener à
   une corruption irrécupérable des données.  Les administrateurs
   devraient s'assurer que les disques contenant les journaux de
   transaction de <productname>PostgreSQL</productname> ne
   produisent pas ce genre de faux rapports.
   (Voir <xref linkend="wal-reliability"/>.)
  </para>

  <para>
   Après qu'un checkpoint a été fait et le journal vidé sur disque,
   la position du checkpoint est sauvegardée dans le
   fichier <filename>pg_control</filename>.  Donc, au début de la
   récupération, le serveur lit en premier
   <filename>pg_control</filename> et ensuite l'entrée du
   checkpoint&nbsp;; ensuite, il opère le REDO en progressant
   à partir de la position du journal indiquée dans l'entrée du
   checkpoint. Parce que l'ensemble du contenu des pages de
   données est sauvegardé dans le journal à la première modification de
   page après un checkpoint (en supposant que <xref
   linkend="guc-full-page-writes"/> n'est pas désactivé), toutes les pages
   modifiées depuis le checkpoint seront restaurées dans un état cohérent.
  </para>

  <para>
   Pour gérer le cas où <filename>pg_control</filename> est corrompu, nous
   devrions permettre le parcours des segments de journaux
   existants en ordre inverse &mdash; du plus récent au plus ancien &mdash; pour
   trouver le dernier checkpoint. Ceci n'a pas encore été implémenté.
   <filename>pg_control</filename> est assez petit (moins d'une page disque)
   pour ne pas être sujet aux problèmes d'écriture partielle et, au moment où
   ceci est écrit, il n'y a eu aucun rapport de défaillance d'une base
   due uniquement à l'incapacité à lire <filename>pg_control</filename>.
   Donc, bien qu'étant théoriquement un point faible,
   <filename>pg_control</filename> ne semble pas être un problème en pratique.
  </para>
 </sect1>
</chapter>
